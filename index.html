<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="肩膀有点痒，可能在长小翅膀">
<meta property="og:type" content="website">
<meta property="og:title" content="一只鱼的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="一只鱼的博客">
<meta property="og:description" content="肩膀有点痒，可能在长小翅膀">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="一只鱼的博客">
<meta name="twitter:description" content="肩膀有点痒，可能在长小翅膀">
  <link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>一只鱼的博客</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">一只鱼的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">七秒钟的记忆多一秒</p>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-question-circle"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    
      
      
        
      
        
      
        
          
        
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-question-circle"></i>标签<span class="badge">41</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    
      
      
        
      
        
          
        
      
        
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-question-circle"></i>分类<span class="badge">10</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    
      
      
        
          
        
      
        
      
        
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-question-circle"></i>归档<span class="badge">49</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-schedule">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>日程表</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-sitemap">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-commonweal">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/11/kafka内部原理和机制(一)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/02/11/kafka内部原理和机制(一)/" class="post-title-link" itemprop="url">kafka内部原理和机制</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-02-11 15:51:43" itemprop="dateCreated datePublished" datetime="2020-02-11T15:51:43+08:00">2020-02-11</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/11/kafka基础入门/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/02/11/kafka基础入门/" class="post-title-link" itemprop="url">kafka基础入门</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-02-11 12:01:29 / 修改时间：16:13:36" itemprop="dateCreated datePublished" datetime="2020-02-11T12:01:29+08:00">2020-02-11</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1. Kafka概述"></a>1. Kafka概述</h3><h4 id="1-1-为什么有消息系统"><a href="#1-1-为什么有消息系统" class="headerlink" title="1.1 为什么有消息系统"></a>1.1 为什么有消息系统</h4><p><strong>解耦</strong><br>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2020/02/11/kafka基础入门/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/10/Hadoop之MapReduce以及Yarn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/10/Hadoop之MapReduce以及Yarn/" class="post-title-link" itemprop="url">Hadoop之MapReduce以及Yarn</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-12-10 20:53:14 / 修改时间：21:30:18" itemprop="dateCreated datePublished" datetime="2019-12-10T20:53:14+08:00">2019-12-10</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-mapreduce编程"><a href="#1-mapreduce编程" class="headerlink" title="1 mapreduce编程"></a>1 mapreduce编程</h1><h2 id="1-1-mapreduce的定义"><a href="#1-1-mapreduce的定义" class="headerlink" title="1.1 mapreduce的定义"></a>1.1 mapreduce的定义</h2><p>MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。</p>
<p>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。</p>
<h2 id="1-2-mapreduce的核心思想"><a href="#1-2-mapreduce的核心思想" class="headerlink" title="1.2 mapreduce的核心思想"></a>1.2 mapreduce的核心思想</h2><p>MapReduce思想在生活中处处可见。或多或少都曾接触过这种思想。MapReduce的思想核心是“<strong>分而治之</strong>”，适用于大量复杂的任务处理场景（大规模数据处理场景）。即使是发布过论文实现分布式计算的谷歌也只是实现了这种思想，而不是自己原创。</p>
<p>Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。</p>
<p>Reduce负责“合”，即对map阶段的结果进行全局汇总。</p>
<p>这两个阶段合起来正是MapReduce思想的体现。</p>
<p>还有一个比较形象的语言解释MapReduce：　　</p>
<p>我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“<strong>Map</strong>”。我们人越多，数书就越快。</p>
<p>现在我们到一起，把所有人的统计数加在一起。这就是“<strong>Reduce</strong>”。</p>
<p>电影黑客帝国当中，特工”（Agents），Smith（史密斯）对付救世主Neo打不过怎么办？一个人打不过，就复制十个出来，十个不行就复制一百个</p>
<h2 id="1-3-mapreduce编程模型"><a href="#1-3-mapreduce编程模型" class="headerlink" title="1.3 mapreduce编程模型"></a>1.3 mapreduce编程模型</h2><p>MapReduce是采用一种<strong>分而治之</strong>的思想设计出来的分布式计算框架</p>
<p>那什么是分而治之呢？</p>
<p>比如一复杂、计算量大、耗时长的的任务，暂且称为“大任务”；</p>
<p>此时使用单台服务器无法计算或较短时间内计算出结果时，可将此大任务切分成一个个小的任务，小任务分别在不同的服务器上并行的执行；</p>
<p>最终再汇总每个小任务的结果</p>
<p>MapReduce由两个阶段组 成：</p>
<p>Map阶段（切分成一个个小的任务）</p>
<p>Reduce阶段（汇总小任务的结果）</p>
<h3 id="1-3-1-Map阶段"><a href="#1-3-1-Map阶段" class="headerlink" title="1.3.1 Map阶段"></a>1.3.1 Map阶段</h3><p>·         map阶段有一个关键的map()函数；</p>
<p>·         此函数的输入是键值对</p>
<p>·         输出是一系列键值对，输出写入本地磁盘。</p>
<h3 id="1-3-2-Reduce阶段"><a href="#1-3-2-Reduce阶段" class="headerlink" title="1.3.2 Reduce阶段"></a>1.3.2 Reduce阶段</h3><p>·         reduce阶段有一个关键的函数reduce()函数</p>
<p>·         此函数的输入也是键值对（即map的输出（kv对））</p>
<p>·         输出也是一系列键值对，结果最终写入HDFS</p>
<h3 id="1-3-3-Map-amp-Reduce"><a href="#1-3-3-Map-amp-Reduce" class="headerlink" title="1.3.3 Map&amp;Reduce"></a>1.3.3 Map&amp;Reduce</h3><p>   <img src="/2019/12/10/Hadoop之MapReduce以及Yarn/1.png" alt="1575982708937"></p>
<h2 id="1-4-mapreduce编程指导思想"><a href="#1-4-mapreduce编程指导思想" class="headerlink" title="1.4 mapreduce编程指导思想"></a>1.4 mapreduce编程指导思想</h2><p>mapReduce编程模型的总结：</p>
<p>MapReduce的开发一共有八个步骤其中map阶段分为2个步骤，shuffle阶段4个步骤，reduce阶段分为2个步骤</p>
<h3 id="Map阶段2个步骤"><a href="#Map阶段2个步骤" class="headerlink" title="Map阶段2个步骤"></a>Map阶段2个步骤</h3><p>第一步：设置inputFormat类，将我们的数据切分成key，value对，输入到第二步</p>
<p>第二步：自定义map逻辑，处理我们第一步的输入数据，然后转换成新的key，value对进行输出</p>
<h3 id="shuffle阶段4个步骤"><a href="#shuffle阶段4个步骤" class="headerlink" title="shuffle阶段4个步骤"></a>shuffle阶段4个步骤</h3><p>第三步：对输出的key，value对进行分区。相同key的数据发送到同一个reduce里面去，相同key合并，value形成一个集合</p>
<p>第四步：对不同分区的数据按照相同的key进行排序</p>
<p>第五步：对分组后的数据进行规约(combine操作)，降低数据的网络拷贝（可选步骤）</p>
<p>第六步：对排序后的额数据进行分组，分组的过程中，将相同key的value放到一个集合当中</p>
<h3 id="reduce阶段2个步骤"><a href="#reduce阶段2个步骤" class="headerlink" title="reduce阶段2个步骤"></a>reduce阶段2个步骤</h3><p>第七步：对多个map的任务进行合并，排序，写reduce函数自己的逻辑，对输入的key，value对进行处理，转换成新的key，value对进行输出</p>
<p>第八步：设置outputformat将输出的key，value对数据进行保存到文件中</p>
<h2 id="1-5-hadoop当中常用的数据类型"><a href="#1-5-hadoop当中常用的数据类型" class="headerlink" title="1.5 hadoop当中常用的数据类型"></a>1.5 hadoop当中常用的数据类型</h2><p>hadoop没有沿用java当中基本的数据类型，而是自己进行封装了一套数据类型，其自己封装的类型与java的类型对应如下</p>
<p>表5-1 常用的数据类型对应的Hadoop数据序列化类型</p>
<table>
<thead>
<tr>
<th>Java类型</th>
<th>Hadoop Writable类型</th>
</tr>
</thead>
<tbody><tr>
<td>Boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>Byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>Int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>Float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>Long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>Double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>Map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>Array</td>
<td>ArrayWritable</td>
</tr>
<tr>
<td>byte[]</td>
<td>BytesWritable</td>
</tr>
</tbody></table>
<h2 id="1-6-mapreduce编程入门之单词统计"><a href="#1-6-mapreduce编程入门之单词统计" class="headerlink" title="1.6 mapreduce编程入门之单词统计"></a>1.6 mapreduce编程入门之单词统计</h2><p>  <img src="/2019/12/10/Hadoop之MapReduce以及Yarn/2.png" alt="1575982854447"></p>
<h2 id="1-7-mapreduce编程入门案例之单词计数统计实现"><a href="#1-7-mapreduce编程入门案例之单词计数统计实现" class="headerlink" title="1.7 mapreduce编程入门案例之单词计数统计实现"></a>1.7 mapreduce编程入门案例之单词计数统计实现</h2><p>需求：现有数据格式如下，每一行数据之间都是使用逗号进行分割，求取每个单词出现的次数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hello,hello</span><br><span class="line"></span><br><span class="line">world,world</span><br><span class="line"></span><br><span class="line">hadoop,hadoop</span><br><span class="line"></span><br><span class="line">hello,world</span><br><span class="line"></span><br><span class="line">hello,flume</span><br><span class="line"></span><br><span class="line">hadoop,hive</span><br><span class="line"></span><br><span class="line">hive,kafka</span><br><span class="line"></span><br><span class="line">flume,storm</span><br><span class="line"></span><br><span class="line">hive,oozie</span><br></pre></td></tr></table></figure>

<h3 id="第一步：创建maven工程并导入以下jar包"><a href="#第一步：创建maven工程并导入以下jar包" class="headerlink" title="第一步：创建maven工程并导入以下jar包"></a>第一步：创建maven工程并导入以下jar包</h3><p>cdh版本的软件jar包下载参见以下这两个文档链接</p>
<p><a href="https://docs.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo.html</a></p>
<p><a href="https://docs.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.6.0-mr1-cdh5.14.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;4.11&lt;/version&gt;</span><br><span class="line">        &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.testng&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;testng&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;RELEASE&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.0&lt;/version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">                &lt;!--   &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.4.3&lt;/version&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                    &lt;/goals&gt;</span><br><span class="line">                    &lt;configuration&gt;</span><br><span class="line">                        &lt;minimizeJar&gt;true&lt;/minimizeJar&gt;</span><br><span class="line">                    &lt;/configuration&gt;</span><br><span class="line">                &lt;/execution&gt;</span><br><span class="line">            &lt;/executions&gt;</span><br><span class="line">        &lt;/plugin&gt;</span><br><span class="line">    &lt;/plugins&gt;</span><br><span class="line">&lt;/build&gt;</span><br></pre></td></tr></table></figure>

<h3 id="第二步：定义mapper类"><a href="#第二步：定义mapper类" class="headerlink" title="第二步：定义mapper类"></a>第二步：定义mapper类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.wordcountdemo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-05 14:29</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text k2;</span><br><span class="line">    <span class="keyword">private</span> IntWritable v2;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        k2 = <span class="keyword">new</span> Text();</span><br><span class="line">        v2 = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key  行偏移量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value 一行文本内容</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String[] split = value.toString().split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word:split) &#123;</span><br><span class="line">            k2.set(word);</span><br><span class="line">            v2.set(<span class="number">1</span>);</span><br><span class="line">            context.write(k2,v2);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="第三步：定义reducer类"><a href="#第三步：定义reducer类" class="headerlink" title="第三步：定义reducer类"></a>第三步：定义reducer类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.wordcountdemo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-05 14:30</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key  单词</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values  迭代器，每个单词出现的次数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value:values) &#123;</span><br><span class="line">            result += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,<span class="keyword">new</span> IntWritable(result));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="第四步：组装main程序"><a href="#第四步：组装main程序" class="headerlink" title="第四步：组装main程序"></a>第四步：组装main程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.wordcountdemo;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-05 14:30</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span></span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在这里面组装job对象，构建mr的各个组件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> strings</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        <span class="comment">//获取job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf,<span class="string">"wordCount"</span>);</span><br><span class="line">        <span class="comment">//打包提交到集群上运行</span></span><br><span class="line">        job.setJarByClass(WordCountMain.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.读取文件解析成为k1，v1</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"hdfs://node01:8020/kk/dir1/wordcount_input"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.自定义map逻辑，接受k1v1，转换成为新的k2v2</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3-6 分区、排序、规约、分组</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"hdfs://node01:8020/kk/dir1/outputdebug"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> b?<span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 程序入口类</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(<span class="keyword">new</span> Configuration(),<span class="keyword">new</span> WordCountMain(),args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="1-8-mapreduce的运行模式"><a href="#1-8-mapreduce的运行模式" class="headerlink" title="1.8 mapreduce的运行模式"></a>1.8 mapreduce的运行模式</h2><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>（1）mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行</p>
<p>（2）而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上</p>
<p>（3）怎样实现本地运行？写一个程序，不要带集群的配置文件</p>
<p>本质是程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname=local参数</p>
<p>（4）本地模式非常便于进行业务逻辑的debug，只要在eclipse中打断点即可</p>
<p>本地模式运行代码设置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">configuration.set(<span class="string">"mapreduce.framework.name"</span>,<span class="string">"local"</span>);</span><br><span class="line">configuration.set(<span class="string">"yarn.resourcemanager.hostname"</span>,<span class="string">"local"</span>);</span><br><span class="line"> </span><br><span class="line">TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"file:///D:\\input"</span>));</span><br><span class="line">TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"file:///D:\\output"</span>));</span><br></pre></td></tr></table></figure>

<h3 id="集群运行模式"><a href="#集群运行模式" class="headerlink" title="集群运行模式"></a>集群运行模式</h3><p>（1）将mapreduce程序提交给yarn集群，分发到很多的节点上并发执行</p>
<p>（2）处理的数据和输出结果应该位于hdfs文件系统</p>
<p>（3）提交集群的实现步骤：</p>
<p>将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar com.kk.hdfs.demo1.JobMain</span><br></pre></td></tr></table></figure>

<h2 id="1-9-hadoop的序列化和反序列化"><a href="#1-9-hadoop的序列化和反序列化" class="headerlink" title="1.9 hadoop的序列化和反序列化"></a>1.9 hadoop的序列化和反序列化</h2><p>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 </p>
<p>反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</p>
<p>Java 的序列化（Serializable）是一个重量级序列化框架，一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系…），不便于在网络中高效传输；所以，hadoop 自己开发了一套序列化机制（Writable），精简，高效。不用像 java 对象类一样传输多层的父子关系，需要哪个属性就传输哪个属性值，大大的减少网络传输的开销。 </p>
<p>Writable是Hadoop的序列化格式，hadoop定义了这样一个Writable接口。 一个类要支持可序列化只需实现这个接口即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">另外Writable有一个子接口是WritableComparable，</span><br><span class="line">writableComparable是既可实现序列化，也可以对key进行比较，</span><br><span class="line">可以通过自定义key实现WritableComparable来实现排序功能</span><br></pre></td></tr></table></figure>

<p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</p>
<p>具体实现bean对象序列化步骤如下7步。</p>
<p>（1）必须实现Writable接口</p>
<p>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p>
<p>需求：现有数据参见资料当中，求取每个手机号的上行流量，下行流量之和，以及上行总流量，下行总流量之和</p>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><p>定义javaBean对象：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.phoneflowcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-05 19:32</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer upFlow;</span><br><span class="line">    <span class="keyword">private</span> Integer downFlow;</span><br><span class="line">    <span class="keyword">private</span> Integer upCountFlow;</span><br><span class="line">    <span class="keyword">private</span> Integer downCountFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeInt(upFlow);</span><br><span class="line">        dataOutput.writeInt(downFlow);</span><br><span class="line">        dataOutput.writeInt(upCountFlow);</span><br><span class="line">        dataOutput.writeInt(downCountFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//反序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readInt();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readInt();</span><br><span class="line">        <span class="keyword">this</span>.upCountFlow = dataInput.readInt();</span><br><span class="line">        <span class="keyword">this</span>.downCountFlow = dataInput.readInt();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(Integer upFlow, Integer downFlow, Integer upCountFlow, Integer downCountFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.upCountFlow = upCountFlow;</span><br><span class="line">        <span class="keyword">this</span>.downCountFlow = downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Integer upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Integer downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getUpCountFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpCountFlow</span><span class="params">(Integer upCountFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upCountFlow = upCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getDownCountFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownCountFlow</span><span class="params">(Integer downCountFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downCountFlow = downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"FlowBean&#123;"</span> +</span><br><span class="line">                <span class="string">"upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">", downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">", upCountFlow="</span> + upCountFlow +</span><br><span class="line">                <span class="string">", downCountFlow="</span> + downCountFlow +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>定义mapper类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.phoneflowcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-05 19:36</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoneFlowCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text k2;</span><br><span class="line">    <span class="keyword">private</span> FlowBean v2;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        k2 = <span class="keyword">new</span> Text();</span><br><span class="line">        v2 = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String[] splits = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        String phonenumber = splits[<span class="number">1</span>];</span><br><span class="line">        String upFlow = splits[<span class="number">6</span>];</span><br><span class="line">        String downFlow = splits[<span class="number">7</span>];</span><br><span class="line">        String upCountFlow = splits[<span class="number">8</span>];</span><br><span class="line">        String downCountFlow = splits[<span class="number">9</span>];</span><br><span class="line"></span><br><span class="line">        k2.set(phonenumber);</span><br><span class="line">        v2.setUpFlow(Integer.parseInt(upFlow));</span><br><span class="line">        v2.setDownFlow(Integer.parseInt(downFlow));</span><br><span class="line">        v2.setUpCountFlow(Integer.parseInt(upCountFlow));</span><br><span class="line">        v2.setDownCountFlow(Integer.parseInt(downCountFlow));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context.write(k2,v2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>定义reducer类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.phoneflowcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-05 19:36</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoneFlowCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key   手机号</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values  四个变量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> upFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> downFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> upCountFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> downCountFlow = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value:values) &#123;</span><br><span class="line">            upFlow += value.getUpFlow();</span><br><span class="line">            downFlow += value.getDownFlow();</span><br><span class="line">            upCountFlow += value.getUpCountFlow();</span><br><span class="line">            downCountFlow += value.getDownCountFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(key,<span class="keyword">new</span> Text(upFlow+<span class="string">"\t"</span>+downFlow+<span class="string">"\t"</span>+upCountFlow+<span class="string">"\t"</span>+downCountFlow));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>定义main方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.phoneflowcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.xiaoyu.wordcountdemo.WordCountMain;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-05 19:36</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoneFlowCountMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        <span class="comment">//获取job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf,<span class="string">"phoneflowcount"</span>);</span><br><span class="line">        <span class="comment">//打jar包</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.解析文件成为k1v1</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line"></span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"I:\\data_flow.dat"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自定义map逻辑，将k1v1转换为K2,V2</span></span><br><span class="line">        job.setMapperClass(PhoneFlowCountMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自定义reduce逻辑，将k2v2转换成为k3v3</span></span><br><span class="line">        job.setReducerClass(PhoneFlowCountReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"I:\\output"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b?<span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(<span class="keyword">new</span> Configuration(),<span class="keyword">new</span> PhoneFlowCountMain(),args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="1-10-mapreduce的输入解析InputFormat详解以及mapTask个数"><a href="#1-10-mapreduce的输入解析InputFormat详解以及mapTask个数" class="headerlink" title="1.10 mapreduce的输入解析InputFormat详解以及mapTask个数"></a>1.10 mapreduce的输入解析InputFormat详解以及mapTask个数</h2><h3 id="1-10-1-InputFormat详解"><a href="#1-10-1-InputFormat详解" class="headerlink" title="1.10.1 InputFormat详解"></a>1.10.1 InputFormat详解</h3><p>InputFormat是mapreduce当中用于处理数据输入的一个组件，是最顶级的一个抽象父类，主要用于解决各个地方的数据源的数据输入问题。其中InputFormat的UML类图可以通过idea进行查看</p>
<p><img src="/2019/12/10/Hadoop之MapReduce以及Yarn/3.png" alt="1575983560620"></p>
<p><img src="/2019/12/10/Hadoop之MapReduce以及Yarn/4.png" alt="1575983612928">   </p>
<h3 id="1-10-2-FileInputFormat常用类介绍"><a href="#1-10-2-FileInputFormat常用类介绍" class="headerlink" title="1.10.2 FileInputFormat常用类介绍"></a>1.10.2 FileInputFormat常用类介绍</h3><p>FileInputFormat类也是InputFormat的一个子类，如果需要操作hdfs上面的文件，基本上都是通过FileInputFormat类来实现的，我们可以通过FileInputFormat来实现各种格式的文件操作，FileInputFormat的子实现类的UML类图如下</p>
<p>   <img src="/2019/12/10/Hadoop之MapReduce以及Yarn/5.png" alt="1575983657953"></p>
<p>   <img src="/2019/12/10/Hadoop之MapReduce以及Yarn/6.png" alt="1575983683254"></p>
<table>
<thead>
<tr>
<th><strong>类名</strong></th>
<th><strong>主要作用</strong></th>
</tr>
</thead>
<tbody><tr>
<td>TextInputFormat</td>
<td>读取文本文件</td>
</tr>
<tr>
<td>CombineFileInputFormat</td>
<td>在MR当中用于合并小文件，将多个小文件合并之后只需要启动一个mapTask进行运行</td>
</tr>
<tr>
<td>SequenceFileInputFormat</td>
<td>处理SequenceFile这种格式的数据</td>
</tr>
<tr>
<td>KeyValueTextInputFormat</td>
<td>通过手动指定分隔符，将每一条数据解析成为key，value对类型</td>
</tr>
<tr>
<td>NLineInputFormat</td>
<td>指定数据的行数作为一个切片</td>
</tr>
<tr>
<td>FixedLengthInputFormat</td>
<td>从文件中读取固定宽度的二进制记录</td>
</tr>
</tbody></table>
<h3 id="1-10-3-MapTask的数量以及文件的输入切片机制"><a href="#1-10-3-MapTask的数量以及文件的输入切片机制" class="headerlink" title="1.10.3 MapTask的数量以及文件的输入切片机制"></a>1.10.3 MapTask的数量以及文件的输入切片机制</h3><h4 id="1-10-3-1-MapTask的个数决定"><a href="#1-10-3-1-MapTask的个数决定" class="headerlink" title="1.10.3.1 MapTask的个数决定"></a>1.10.3.1 MapTask的个数决定</h4><p>   <img src="/2019/12/10/Hadoop之MapReduce以及Yarn/7.png" alt="1575983728564"></p>
<p>在运行我们的MapReduce程序的时候，我们可以清洗的看到会有多个mapTask的运行，那么maptask的个数究竟与什么有关，是不是maptask越多越好，或者说是不是maptask的个数越少越好呢？？？我们可以通过MapReduce的源码进行查看mapTask的个数究竟是如何决定的</p>
<p>在MapReduce当中，每个mapTask处理一个切片split的数据量，注意切片与block块的概念很想，但是block块是HDFS当中存储数据的单位，切片split是MapReduce当中每个MapTask处理数据量的单位。</p>
<p>MapTask并行度决定机制</p>
<p>数据块：Block是HDFS物理上把数据分成一块一块。</p>
<p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。</p>
<p>查看FileInputFormat的源码，里面getSplits的方法便是获取所有的切片，其中有个方法便是获取切片大小</p>
<p><img src="/2019/12/10/Hadoop之MapReduce以及Yarn/8.png" alt="1575983806908"></p>
<p>切片大小的计算公式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Math.max(minSize, Math.min(maxSize, blockSize));   </span><br><span class="line">mapreduce.input.fileinputformat.split.minsize=1 默认值为1  </span><br><span class="line">mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue </span><br><span class="line">blockSize为128M</span><br></pre></td></tr></table></figure>

<p>由以上计算公式可以推算出split切片的大小刚好与block块相等</p>
<p>那么hdfs上面如果有以下两个文件，文件大小分别为300M和12M，那么会启动多少个MapTask？？？</p>
<p>1、输入文件两个</p>
<p>file1.txt    300M</p>
<p>file2.txt    10M</p>
<p>2、经过FileInputFormat的切片机制运算后，形成的切片信息如下：</p>
<p>file1.txt.split1–  0~128</p>
<p>file1.txt.split2–  128~256</p>
<p>file1.txt.split3–  256~300</p>
<p>file2.txt.split1–  0~10M</p>
<p>一共就会有四个切片，与我们block块的个数刚好相等</p>
<p>如果有1000个小文件，每个小文件是1kb-100MB之间，那么我们启动1000个MapTask是否合适，该如何合理的控制MapTask的个数？？？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">不合适，会出现分配资源的时间远大于数据处理的时间</span><br></pre></td></tr></table></figure>

<h4 id="1-10-3-2-如何控制mapTask的个数"><a href="#1-10-3-2-如何控制mapTask的个数" class="headerlink" title="1.10.3.2 如何控制mapTask的个数"></a>1.10.3.2 如何控制mapTask的个数</h4><p>如果需要控制maptask的个数，我们只需要调整maxSize和minsize这两个值，那么切片的大小就会改变，切片大小改变之后，mapTask的个数就会改变</p>
<p>maxsize（切片最大值）：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。</p>
<p>minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大。</p>
<h3 id="1-10-4-使用CombineTextInputFormat实现切片个数控制"><a href="#1-10-4-使用CombineTextInputFormat实现切片个数控制" class="headerlink" title="1.10.4 使用CombineTextInputFormat实现切片个数控制"></a>1.10.4 使用CombineTextInputFormat实现切片个数控制</h3><p>框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</p>
<p>1、应用场景：</p>
<p>CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</p>
<p>2、虚拟存储切片最大值设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4M</span><br></pre></td></tr></table></figure>

<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<p>3、切片机制</p>
<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>
<p>（1）虚拟存储过程：</p>
<p>将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。</p>
<p>例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。</p>
<p><img src="/2019/12/10/Hadoop之MapReduce以及Yarn/9.png" alt="1575984111220"></p>
<p>（2）切片过程：</p>
<p>（a）判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。</p>
<p>（b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</p>
<p>（c）测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M）</span><br><span class="line">最终会形成3个切片，大小分别为：</span><br><span class="line">（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</span><br></pre></td></tr></table></figure>

<h3 id="1-10-5-CombineTextInputFormat实战"><a href="#1-10-5-CombineTextInputFormat实战" class="headerlink" title="1.10.5 CombineTextInputFormat实战"></a>1.10.5 CombineTextInputFormat实战</h3><p>在我们前面的wordCount实战案例中，更改以下代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class); //设置我们的输入类型为CombineTextInputFormat</span><br><span class="line">//虚拟存储切片最大值设置4m，设置每个切片处理数据量为4M</span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);</span><br></pre></td></tr></table></figure>

<p>将我们的切片设置成为4M大小，然后重新打包运行，观察mapTask的个数</p>
<h3 id="1-10-6-KeyValueTextInputFormat使用示例"><a href="#1-10-6-KeyValueTextInputFormat使用示例" class="headerlink" title="1.10.6 KeyValueTextInputFormat使用示例"></a>1.10.6 KeyValueTextInputFormat使用示例</h3><p>KeyValueTextInputFormat允许我们自己来定义分隔符，通过分隔符来自定义我们的key和value，参见附件当中的数据，数据之间的分隔符为@zolen@   数据内容如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello@zolen@  input datas today </span><br><span class="line">count@zolen@  hadoop  spark</span><br><span class="line">hello@zolen@  input some datas  to test</span><br></pre></td></tr></table></figure>

<p>期望输出结果如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hello   2</span><br><span class="line">count   1</span><br></pre></td></tr></table></figure>

<p>定义Mapper程序代码实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.keyValue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-06 14:37</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyValueMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        IntWritable v2 = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">        context.write(key,v2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>定义Reducer程序代码实现</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.keyValue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-06 14:37</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyValueReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value:values) &#123;</span><br><span class="line">            result += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,<span class="keyword">new</span> IntWritable(result));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>定义Main程序代码实现</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiaoyu.keyValue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> xiaoyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2019-12-06 14:37</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyValueMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        conf.set(<span class="string">"key.value.separator.in.input.line"</span>,<span class="string">"@zolen@"</span>);</span><br><span class="line">        <span class="comment">//获取job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf,<span class="string">"keyvalue"</span>);</span><br><span class="line">        <span class="comment">//打包</span></span><br><span class="line">        job.setJarByClass(KeyValueMain.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//解析文件k1v1</span></span><br><span class="line">        job.setInputFormatClass(KeyValueTextInputFormat.class);</span><br><span class="line">        KeyValueTextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"I:\\数据"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自定义map</span></span><br><span class="line">        job.setMapperClass(KeyValueMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//reduce</span></span><br><span class="line">        job.setReducerClass(KeyValueReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"I:\\output"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b?<span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(<span class="keyword">new</span> Configuration(), <span class="keyword">new</span> KeyValueMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="1-10-7-NlineInputFormat使用示例"><a href="#1-10-7-NlineInputFormat使用示例" class="headerlink" title="1.10.7 NlineInputFormat使用示例"></a>1.10.7 NlineInputFormat使用示例</h3><p>NLineInputFormat允许我们自己定义输入的行数作为一个切片数据</p>
<p>需求：读取课件当中的数据，实现自己定义多少行数据作为一个切片，并统计单词出现的次数</p>
<h3 id="1-10-8-自定义InputFormat"><a href="#1-10-8-自定义InputFormat" class="headerlink" title="1.10.8 自定义InputFormat"></a>1.10.8 自定义InputFormat</h3><p>mapreduce框架当中已经给我们提供了很多的文件输入类，用于处理文件数据的输入，如果以上提供的文件数据类还不够用的话，我们也可以通过自定义InputFormat来实现文件数据的输入</p>
<p>需求：现在有大量的小文件，我们通过自定义InputFormat实现将小文件全部读取，然后输出成为一个SequenceFile格式的大文件，进行文件的合并</p>
<h4 id="第一步：自定义InputFormat"><a href="#第一步：自定义InputFormat" class="headerlink" title="第一步：自定义InputFormat"></a>第一步：自定义InputFormat</h4><p>public class MyInputFormat extends FileInputFormat&lt;NullWritable,BytesWritable&gt; {</p>
<p>​    @Override</p>
<p>​    public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {</p>
<p>​        MyRecordReader myRecordReader = new MyRecordReader();</p>
<p>​        myRecordReader.initialize(inputSplit,taskAttemptContext);</p>
<p>​        return myRecordReader;</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * 注意这个方法，决定我们的文件是否可以切分，如果不可切分，直接返回false</p>
<p>​     * 到时候读取数据的时候，一次性将文件内容全部都读取出来</p>
<p>​     * @param context</p>
<p>​     * @param filename</p>
<p>​     * @return</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    protected boolean isSplitable(JobContext context, Path filename) {</p>
<p>​        return false;</p>
<p>​    }</p>
<p>}</p>
<h4 id="第二步：自定义RecordReader读取数据"><a href="#第二步：自定义RecordReader读取数据" class="headerlink" title="第二步：自定义RecordReader读取数据"></a>第二步：自定义RecordReader读取数据</h4><p>import org.apache.hadoop.conf.Configuration;</p>
<p>import org.apache.hadoop.fs.FSDataInputStream;</p>
<p>import org.apache.hadoop.fs.FileSystem;</p>
<p>import org.apache.hadoop.fs.Path;</p>
<p>import org.apache.hadoop.io.BytesWritable;</p>
<p>import org.apache.hadoop.io.IOUtils;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.mapreduce.InputSplit;</p>
<p>import org.apache.hadoop.mapreduce.RecordReader;</p>
<p>import org.apache.hadoop.mapreduce.TaskAttemptContext;</p>
<p>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>import java.io.IOException;</p>
<p>public class MyRecordReader extends RecordReader&lt;NullWritable,BytesWritable&gt; {</p>
<p>​    private FileSplit fileSplit ;</p>
<p>​    private Configuration configuration;</p>
<p>​    private BytesWritable bytesWritable;</p>
<p>//读取文件的标识</p>
<p>​    private boolean  flag = false;</p>
<p>​    /**</p>
<p>​     * 初始化的方法  只在初始化的时候调用一次.只要拿到了文件的切片，就拿到了文件的内容</p>
<p>​     * @param inputSplit   输入的文件的切片</p>
<p>​     * @param taskAttemptContext</p>
<p>​     * @throws IOException</p>
<p>​     * @throws InterruptedException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {</p>
<p>​        this.fileSplit = (FileSplit) inputSplit;</p>
<p>​       this.configuration = taskAttemptContext.getConfiguration();</p>
<p>​       bytesWritable = new BytesWritable();</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * 读取数据</p>
<p>​     * 返回值boolean  类型，如果返回true，表示文件已经读取完成，不用再继续往下读取了</p>
<p>​     * 如果返回false，文件没有读取完成，继续读取下一行</p>
<p>​     * @return</p>
<p>​     * @throws IOException</p>
<p>​     * @throws InterruptedException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public boolean nextKeyValue() throws IOException, InterruptedException {</p>
<p>​        if(!flag){</p>
<p>​            long length = fileSplit.getLength();</p>
<p>​            byte[] bytes =new byte[(int)length];</p>
<p>​            //获取到了文件的切片之后，我们就需要将文件切片的内容获取出来</p>
<p>​            Path path = fileSplit.getPath();  //获取文件切片的路径   file:///  hdfs://</p>
<p>​            FileSystem fileSystem = path.getFileSystem(configuration);//获取文件系统</p>
<p>​            FSDataInputStream open = fileSystem.open(path);//打开文件的输入流</p>
<p>​            //已经获取到了文件的输入流，我们需要将流对象，封装到BytesWritable 里面去</p>
<p>​            //  inputStream   ==&gt;  byte[]   ==&gt;  BytesWritable</p>
<p>​            IOUtils.readFully(open,bytes,0,(int)length);</p>
<p>​            bytesWritable.set(bytes,0,(int)length);</p>
<p>​            flag = true;</p>
<p>​            return true;</p>
<p>​        }</p>
<p>​        return false;</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * 获取数据的key1</p>
<p>​     * @return</p>
<p>​     * @throws IOException</p>
<p>​     * @throws InterruptedException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public NullWritable getCurrentKey() throws IOException, InterruptedException {</p>
<p>​        return NullWritable.get();</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * 获取数据的value1</p>
<p>​     * @return</p>
<p>​     * @throws IOException</p>
<p>​     * @throws InterruptedException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public BytesWritable getCurrentValue() throws IOException, InterruptedException {</p>
<p>​        return bytesWritable;</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * 读取文件的进度，没什么用</p>
<p>​     * @return</p>
<p>​     * @throws IOException</p>
<p>​     * @throws InterruptedException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public float getProgress() throws IOException, InterruptedException {</p>
<p>​        return flag?1.0f:0.0f;</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * 关闭资源</p>
<p>​     * @throws IOException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public void close() throws IOException {</p>
<p>​    }</p>
<p>}</p>
<h4 id="第三步：自定义mapper类"><a href="#第三步：自定义mapper类" class="headerlink" title="第三步：自定义mapper类"></a>第三步：自定义mapper类</h4><p>import org.apache.hadoop.io.BytesWritable;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>import java.io.IOException;</p>
<p>public class MyInputFormatMapper extends Mapper&lt;NullWritable,BytesWritable,Text,BytesWritable&gt; {</p>
<p>​    @Override</p>
<p>​    protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException {</p>
<p>​        FileSplit inputSplit = (FileSplit) context.getInputSplit();</p>
<p>​        String name = inputSplit.getPath().getName();//获取文件的名称</p>
<p>​        context.write(new Text(name),value);</p>
<p>​    }</p>
<p>}</p>
<h4 id="第四步：定义main方法"><a href="#第四步：定义main方法" class="headerlink" title="第四步：定义main方法"></a>第四步：定义main方法</h4><p>import org.apache.hadoop.conf.Configuration;</p>
<p>import org.apache.hadoop.conf.Configured;</p>
<p>import org.apache.hadoop.fs.Path;</p>
<p>import org.apache.hadoop.io.BytesWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Job;</p>
<p>import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</p>
<p>import org.apache.hadoop.util.Tool;</p>
<p>import org.apache.hadoop.util.ToolRunner;</p>
<p>public class MyInputFormatMain extends Configured implements Tool {</p>
<p>​    @Override</p>
<p>​    public int run(String[] args) throws Exception {</p>
<p>​        Job job = Job.getInstance(super.getConf(), “mergeSmallFile”);</p>
<p>​        job.setInputFormatClass(MyInputFormat.class);</p>
<p>​        MyInputFormat.addInputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\4、自定义InputFormat实现小文件合并\文件数据”));</p>
<p>​        job.setMapperClass(MyInputFormatMapper.class);</p>
<p>​        job.setMapOutputKeyClass(Text.class);</p>
<p>​        job.setMapOutputValueClass(BytesWritable.class);</p>
<p>​        //没有reduce。但是要设置reduce的输出的k3   value3 的类型</p>
<p>​        job.setOutputKeyClass(Text.class);</p>
<p>​        job.setOutputValueClass(BytesWritable.class);</p>
<p>//将我们的文件输出成为sequenceFile这种格式</p>
<p>​        job.setOutputFormatClass(SequenceFileOutputFormat.class);</p>
<p>​        SequenceFileOutputFormat.setOutputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\4、自定义InputFormat实现小文件合并\文件数据\out_sequence”));</p>
<p>​        boolean b = job.waitForCompletion(true);</p>
<p>​        return b?0:1;</p>
<p>​    }</p>
<p>​    public static void main(String[] args) throws Exception {</p>
<p>​        int run = ToolRunner.run(new Configuration(), new MyInputFormatMain(), args);</p>
<p>​        System.exit(run);</p>
<p>​    }</p>
<p>}</p>
<h2 id="11、mapreduce的partitioner详解"><a href="#11、mapreduce的partitioner详解" class="headerlink" title="11、mapreduce的partitioner详解"></a>11、mapreduce的partitioner详解</h2><p>在mapreduce执行当中，有一个默认的步骤就是partition分区，分区主要的作用就是将相同的数据发送到同一个reduceTask里面去，在mapreduce当中有一个抽象类叫做Partitioner，默认使用的实现类是HashPartitioner，我们可以通过HashPartitioner的源码，查看到分区的逻辑如下</p>
<p>需求：基于hadoop当中的序列化的手机流量数据，实现将不同的手机号的数据划分到不同的文件里面去</p>
<p>135开头的手机号分到一个文件里面去，</p>
<p>136开头的手机号分到一个文件里面去，</p>
<p>137开头的手机号分到一个文件里面去，</p>
<p>138开头的手机号分到一个文件里面去，</p>
<p>139开头的手机号分到一个文件里面去，</p>
<p>其他开头的手机号分到一个文件里面去</p>
<p>添加自定义分区类：</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Partitioner;</p>
<p>public class PartitionOwn extends Partitioner&lt;Text, FlowBean&gt; {</p>
<p>​    @Override</p>
<p>​    public int getPartition(Text text, FlowBean flowBean, int numPartitions) {</p>
<p>​        String phoenNum = text.toString();</p>
<p>​        if(null != phoenNum &amp;&amp; !phoenNum.equals(“”)){</p>
<p>​            if(phoenNum.startsWith(“135”)){</p>
<p>​                return 0;</p>
<p>​            }else if(phoenNum.startsWith(“136”)){</p>
<p>​                return 1;</p>
<p>​            }else if(phoenNum.startsWith(“137”)){</p>
<p>​                return 2;</p>
<p>​            }else if(phoenNum.startsWith(“138”)){</p>
<p>​                return 3;</p>
<p>​            }else if(phoenNum.startsWith(“139”)){</p>
<p>​                return 4;</p>
<p>​            }else {</p>
<p>​                return 5;</p>
<p>​            }</p>
<p>​        }else{</p>
<p>​            return 5;</p>
<p>​        }</p>
<p>​    }</p>
<p>}</p>
<p>程序的main方法当中添加以下配置</p>
<p>job.setPartitionerClass(PartitionOwn.class);</p>
<p>job.setNumReduceTasks(6);</p>
<p>注意：对于我们自定义分区的案例，必须打成jar包上传到集群上面去运行，因为我们本地已经没法通过多线程模拟本地程序运行了，将我们的数据上传到hdfs上面去，然后通过 hadoop jar提交到集群上面去运行，观察我们分区的个数与reduceTask个数的关系</p>
<p>思考：如果手动指定6个分区，reduceTask个数设置为3个会出现什么情况</p>
<p>如果手动指定6个分区，reduceTask个数设置为9个会出现什么情况</p>
<h2 id="12、mapreduce当中的排序"><a href="#12、mapreduce当中的排序" class="headerlink" title="12、mapreduce当中的排序"></a>12、mapreduce当中的排序</h2><p>排序是MapReduce框架中最重要的操作之一。</p>
<p>MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。</p>
<p>默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。</p>
<p>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。</p>
<p>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</p>
<p>各种排序的分类：</p>
<p>1、部分排序</p>
<p>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序</p>
<p>2、全排序</p>
<p>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构</p>
<p>3、辅助排序</p>
<p>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</p>
<p>4、二次排序</p>
<p>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p>
<p>在前面的序列化当中在数据输出的时候，我们对上行流量，下行流量，上行总流量，下行总流量进行了汇总，在序列化汇总输出数据的基础上，我们需要对下行流量，以及上行总流量进行排序，如果下行流量相等就按照上行总流量进行排序</p>
<p>数据参见附件当中的hadoop的二次排序</p>
<p>代码实现：</p>
<p>定义javaBean对象，用于封装数据</p>
<p>import org.apache.hadoop.io.WritableComparable;</p>
<p>import java.io.DataInput;</p>
<p>import java.io.DataOutput;</p>
<p>import java.io.IOException;</p>
<p>public class FlowSortBean implements WritableComparable<flowsortbean> {</flowsortbean></p>
<p>​    private String phone;</p>
<p>​    private Integer  upFlow;</p>
<p>​    private Integer  downFlow;</p>
<p>​    private Integer  upCountFlow;</p>
<p>​    private Integer  downCountFlow;</p>
<p>​    @Override</p>
<p>​    public int compareTo(FlowSortBean o) {</p>
<p>​        int i = this.downFlow.compareTo(o.downFlow);</p>
<p>​        if(i == 0){</p>
<p>​            i = this.upCountFlow.compareTo(o.upCountFlow);</p>
<p>​        }</p>
<p>​        return i;</p>
<p>​    }</p>
<p>​    @Override</p>
<p>​    public void write(DataOutput out) throws IOException {</p>
<p>​        out.writeUTF(phone);</p>
<p>​        out.writeInt(upFlow);</p>
<p>​        out.writeInt(downFlow);</p>
<p>​        out.writeInt(upCountFlow);</p>
<p>​        out.writeInt(downCountFlow);</p>
<p>​    }</p>
<p>​    @Override</p>
<p>​    public void readFields(DataInput in) throws IOException {</p>
<p>​        this.phone = in.readUTF();</p>
<p>​        this.upFlow= in.readInt();</p>
<p>​        this.downFlow= in.readInt();</p>
<p>​        this.upCountFlow = in.readInt();</p>
<p>​        this.downCountFlow =  in.readInt();</p>
<p>​    }</p>
<p>​    @Override</p>
<p>​    public String toString() {</p>
<p>​        return  phone + “\t” + upFlow + “\t” +downFlow + “\t” + upCountFlow + “\t” + downCountFlow ;</p>
<p>​    }</p>
<p>​    public String getPhone() {</p>
<p>​        return phone;</p>
<p>​    }</p>
<p>​    public void setPhone(String phone) {</p>
<p>​        this.phone = phone;</p>
<p>​    }</p>
<p>​    public Integer getUpFlow() {</p>
<p>​        return upFlow;</p>
<p>​    }</p>
<p>​    public void setUpFlow(Integer upFlow) {</p>
<p>​        this.upFlow = upFlow;</p>
<p>​    }</p>
<p>​    public Integer getDownFlow() {</p>
<p>​        return downFlow;</p>
<p>​    }</p>
<p>​    public void setDownFlow(Integer downFlow) {</p>
<p>​        this.downFlow = downFlow;</p>
<p>​    }</p>
<p>​    public Integer getUpCountFlow() {</p>
<p>​        return upCountFlow;</p>
<p>​    }</p>
<p>​    public void setUpCountFlow(Integer upCountFlow) {</p>
<p>​        this.upCountFlow = upCountFlow;</p>
<p>​    }</p>
<p>​    public Integer getDownCountFlow() {</p>
<p>​        return downCountFlow;</p>
<p>​    }</p>
<p>​    public void setDownCountFlow(Integer downCountFlow) {</p>
<p>​        this.downCountFlow = downCountFlow;</p>
<p>​    }</p>
<p>}</p>
<p>定义mapper类</p>
<p>import org.apache.hadoop.io.LongWritable;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>import java.io.IOException;</p>
<p>public class FlowSortMapper extends Mapper&lt;LongWritable,Text,FlowSortBean, NullWritable&gt; {</p>
<p>​    private FlowSortBean flowSortBean;</p>
<p>​    @Override</p>
<p>​    protected void setup(Context context) throws IOException, InterruptedException {</p>
<p>​        flowSortBean = new FlowSortBean();</p>
<p>​    }</p>
<p>​    @Override</p>
<p>​    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {</p>
<p>​        String[] split = value.toString().split(“\t”);</p>
<p>​        flowSortBean.setPhone(split[0]);</p>
<p>​        flowSortBean.setUpFlow(Integer.parseInt(split[1]));</p>
<p>​        flowSortBean.setDownFlow(Integer.parseInt(split[2]));</p>
<p>​        flowSortBean.setUpCountFlow(Integer.parseInt(split[3]));</p>
<p>​        flowSortBean.setDownCountFlow(Integer.parseInt(split[4]));</p>
<p>​        context.write(flowSortBean,NullWritable.get());</p>
<p>​    }</p>
<p>}</p>
<p>定义reducer类</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>import java.io.IOException;</p>
<p>public class FlowSortReducer extends Reducer&lt;FlowSortBean, NullWritable,FlowSortBean,NullWritable&gt; {</p>
<p>​    @Override</p>
<p>​    protected void reduce(FlowSortBean key, Iterable<nullwritable> values, Context context) throws IOException, InterruptedException {</nullwritable></p>
<p>​       context.write(key, NullWritable.get());</p>
<p>​    }</p>
<p>}</p>
<p>定义main方法</p>
<p>import org.apache.hadoop.conf.Configuration;</p>
<p>import org.apache.hadoop.conf.Configured;</p>
<p>import org.apache.hadoop.fs.Path;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.mapreduce.Job;</p>
<p>import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</p>
<p>import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</p>
<p>import org.apache.hadoop.util.Tool;</p>
<p>import org.apache.hadoop.util.ToolRunner;</p>
<p>public class FlowSortMain extends Configured implements Tool {</p>
<p>​    @Override</p>
<p>​    public int run(String[] args) throws Exception {</p>
<p>​        //获取job对象</p>
<p>​        Job job = Job.getInstance(super.getConf(), “flowSort”);</p>
<p>​        //如果程序打包运行必须要设置这一句</p>
<p>​        job.setJarByClass(FlowSortMain.class);</p>
<p>​        job.setInputFormatClass(TextInputFormat.class);</p>
<p>​        TextInputFormat.addInputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\7、hadoop的二次排序\数据\input”));</p>
<p>​        job.setMapperClass(FlowSortMapper.class);</p>
<p>​        job.setMapOutputKeyClass(FlowSortBean.class);</p>
<p>​        job.setMapOutputValueClass(NullWritable.class);</p>
<p>​        job.setReducerClass(FlowSortReducer.class);</p>
<p>​        job.setOutputKeyClass(FlowSortBean.class);</p>
<p>​        job.setOutputValueClass(NullWritable.class);</p>
<p>​        job.setOutputFormatClass(TextOutputFormat.class);</p>
<p>​        TextOutputFormat.setOutputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\7、hadoop的二次排序\数据\input\out_sort”));</p>
<p>​        boolean b = job.waitForCompletion(true);</p>
<p>​        return b?0:1;</p>
<p>​    }</p>
<p>​    public static void main(String[] args) throws Exception {</p>
<p>​        Configuration configuration = new Configuration();</p>
<p>​        int run = ToolRunner.run(configuration, new FlowSortMain(), args);</p>
<p>​        System.exit(run);</p>
<p>​    }</p>
<p>}</p>
<h2 id="13、mapreduce当中的combiner"><a href="#13、mapreduce当中的combiner" class="headerlink" title="13、mapreduce当中的combiner"></a>13、mapreduce当中的combiner</h2><p>combiner基本介绍</p>
<p>以下这种情况求平均值的时候，就不适合使用combiner</p>
<p>Mapper</p>
<p>3 5 7 -&gt;(3+5+7)/3=5 </p>
<p>2 6 -&gt;(2+6)/2=4</p>
<p>Reducer</p>
<p>(3+5+7+2+6)/5=23/5    不等于    (5+4)/2=9/2</p>
<p>需求：对于我们前面的wordCount单词计数统计，我们加上Combiner过程，实现map端的数据进行汇总之后，再发送到reduce端，减少数据的网络拷贝</p>
<p>自定义combiner类</p>
<p>import org.apache.hadoop.io.IntWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>import java.io.IOException;</p>
<p>public class CombinerClass extends Reducer&lt;Text, IntWritable,Text,IntWritable&gt; {</p>
<p>​    @Override</p>
<p>​    protected void reduce(Text key, Iterable<intwritable> values, Context context) throws IOException, InterruptedException {</intwritable></p>
<p>​        int mapCombiner = 0;</p>
<p>​        for (IntWritable value : values) {</p>
<p>​            mapCombiner += value.get();</p>
<p>​        }</p>
<p>​        context.write(key,new IntWritable(mapCombiner));</p>
<p>​    }</p>
<p>}</p>
<p>main方法当中添加我们的combiner这个组件</p>
<p> job.setCombinerClass(CombinerClass.class);</p>
<p>运行程序，观察控制台有combiner和没有combiner的异同</p>
<h2 id="14、mapreduce当中的GroupingComparator分组详解"><a href="#14、mapreduce当中的GroupingComparator分组详解" class="headerlink" title="14、mapreduce当中的GroupingComparator分组详解"></a>14、mapreduce当中的GroupingComparator分组详解</h2><p>GroupingComparator是mapreduce当中reduce端的一个功能组件，主要的作用是决定哪些数据作为一组，调用一次reduce的逻辑，默认是每个不同的key，作为多个不同的组，每个组调用一次reduce逻辑，我们可以自定义GroupingComparator实现不同的key作为同一个组，调用一次reduce逻辑</p>
<p>分组排序步骤：</p>
<p>（1）自定义类继承WritableComparator</p>
<p>（2）重写compare()方法</p>
<p>@Override</p>
<p>public int compare(WritableComparable a, WritableComparable b) {</p>
<p>​        // 比较的业务逻辑</p>
<p>​        return result;</p>
<p>}</p>
<p>（3）创建一个构造将比较对象的类传给父类</p>
<p>protected OrderGroupingComparator() {</p>
<p>​        super(OrderBean.class, true);</p>
<p>}</p>
<p>需求：现在有订单数据如下</p>
<table>
<thead>
<tr>
<th><strong>订单**</strong>id**</th>
<th><strong>商品**</strong>id**</th>
<th><strong>成交金额</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Order_0000001</td>
<td>Pdt_01</td>
<td>222.8</td>
</tr>
<tr>
<td>Order_0000001</td>
<td>Pdt_05</td>
<td>25.8</td>
</tr>
<tr>
<td>Order_0000002</td>
<td>Pdt_03</td>
<td>522.8</td>
</tr>
<tr>
<td>Order_0000002</td>
<td>Pdt_04</td>
<td>122.4</td>
</tr>
<tr>
<td>Order_0000002</td>
<td>Pdt_05</td>
<td>722.4</td>
</tr>
<tr>
<td>Order_0000003</td>
<td>Pdt_01</td>
<td>222.8</td>
</tr>
</tbody></table>
<p>现在需要求取每个订单当中金额最大的商品</p>
<p>自定义OrderBean对象</p>
<p>import org.apache.hadoop.io.WritableComparable;</p>
<p>import java.io.DataInput;</p>
<p>import java.io.DataOutput;</p>
<p>import java.io.IOException;</p>
<p>public class OrderBean implements WritableComparable<orderbean> {</orderbean></p>
<p>​    private String orderId;</p>
<p>​    private Double price ;</p>
<p>​    @Override</p>
<p>​    public int compareTo(OrderBean o) {</p>
<p>​        //注意：如果是不同的订单之间，金额不需要排序，没有可比性</p>
<p>​        int orderIdCompare = this.orderId.compareTo(o.orderId);</p>
<p>​        if(orderIdCompare == 0){</p>
<p>​            //比较金额，按照金额进行排序</p>
<p>​            int priceCompare = this.price.compareTo(o.price);</p>
<p>​            return -priceCompare;</p>
<p>​        }else{</p>
<p>​            //如果订单号不同，没有可比性，直接返回订单号的排序即可</p>
<p>​            return orderIdCompare;</p>
<p>​        }</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * 序列化方法</p>
<p>​     * @param out</p>
<p>​     * @throws IOException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public void write(DataOutput out) throws IOException {</p>
<p>​        out.writeUTF(orderId);</p>
<p>​        out.writeDouble(price);</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * 反序列化方法</p>
<p>​     * @param in</p>
<p>​     * @throws IOException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public void readFields(DataInput in) throws IOException {</p>
<p>​        this.orderId = in.readUTF();</p>
<p>​        this.price = in.readDouble();</p>
<p>​    }</p>
<p>​    public String getOrderId() {</p>
<p>​        return orderId;</p>
<p>​    }</p>
<p>​    public void setOrderId(String orderId) {</p>
<p>​        this.orderId = orderId;</p>
<p>​    }</p>
<p>​    public Double getPrice() {</p>
<p>​        return price;</p>
<p>​    }</p>
<p>​    public void setPrice(Double price) {</p>
<p>​        this.price = price;</p>
<p>​    }</p>
<p>​    @Override</p>
<p>​    public String toString() {</p>
<p>​        return orderId + “\t” +  price;</p>
<p>​    }</p>
<p>}</p>
<p>自定义mapper类：</p>
<p>import org.apache.hadoop.io.LongWritable;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>import java.io.IOException;</p>
<p>public class GroupMapper extends Mapper&lt;LongWritable,Text,OrderBean,NullWritable&gt; {</p>
<p>​    /**</p>
<p>​     * Order_0000005    Pdt_01  222.8</p>
<p>​     Order_0000005  Pdt_05  25.8</p>
<p>​     Order_0000002  Pdt_03  322.8</p>
<p>​     Order_0000002  Pdt_04  522.4</p>
<p>​     Order_0000002  Pdt_05  822.4</p>
<p>​     Order_0000003  Pdt_01  222.8</p>
<p>​     * @param key</p>
<p>​     * @param value</p>
<p>​     * @param context</p>
<p>​     * @throws IOException</p>
<p>​     * @throws InterruptedException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {</p>
<p>​        String[] split = value.toString().split(“\t”);</p>
<p>​        OrderBean orderBean = new OrderBean();</p>
<p>​        orderBean.setOrderId(split[0]);</p>
<p>​        orderBean.setPrice(Double.valueOf(split[2]));</p>
<p>​        //输出orderBean</p>
<p>​        context.write(orderBean, NullWritable.get());</p>
<p>​    }</p>
<p>}</p>
<p>自定义分区类：</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.mapreduce.Partitioner;</p>
<p>public class GroupPartition extends Partitioner&lt;OrderBean,NullWritable&gt; {</p>
<p>​    @Override</p>
<p>​    public int getPartition(OrderBean orderBean, NullWritable nullWritable, int numReduceTasks) {</p>
<p>​        //(key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</p>
<p>​        //注意这里：使用orderId作为分区的条件，来进行判断，保证相同的orderId进入到同一个reduceTask里面去</p>
<p>​        return (orderBean.getOrderId().hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</p>
<p>​    }</p>
<p>}</p>
<p>自定义分组类：</p>
<p>import org.apache.hadoop.io.WritableComparable;</p>
<p>import org.apache.hadoop.io.WritableComparator;</p>
<p>/**</p>
<p> * 第六步：自定义分组逻辑</p>
<p> */</p>
<p>public class MyGroup extends WritableComparator {</p>
<p>​    /**</p>
<p>​     * 覆写默认构造器，通过反射，构造OrderBean对象</p>
<p>​     * 通过反射来构造OrderBean对象</p>
<p>​     * 接受到的key2  是orderBean类型，我们就需要告诉分组，以orderBean接受我们的参数</p>
<p>​     */</p>
<p>​    public MyGroup(){</p>
<p>​        super(OrderBean.class,true);</p>
<p>​    }</p>
<p>​    /**</p>
<p>​     * compare方法接受到两个参数，这两个参数其实就是我们前面传过来的OrderBean</p>
<p>​     * @param a</p>
<p>​     * @param b</p>
<p>​     * @return</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    public int compare(WritableComparable a, WritableComparable b) {</p>
<p>​        OrderBean first = (OrderBean) a;</p>
<p>​        OrderBean second = (OrderBean) b;</p>
<p>​        //以orderId作为比较条件，判断哪些orderid相同作为同一组</p>
<p>​        return first.getOrderId().compareTo(second.getOrderId());</p>
<p>​    }</p>
<p>}</p>
<p>自定义reduce类</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>import java.io.IOException;</p>
<p>public class GroupReducer extends Reducer&lt;OrderBean,NullWritable,OrderBean,NullWritable&gt; {</p>
<p>​    @Override</p>
<p>​    protected void reduce(OrderBean key, Iterable<nullwritable> values, Context context) throws IOException, InterruptedException {</nullwritable></p>
<p>​        context.write(key, NullWritable.get());</p>
<p>​    }</p>
<p>}</p>
<p>定义程序入口类</p>
<p>import org.apache.hadoop.conf.Configuration;</p>
<p>import org.apache.hadoop.conf.Configured;</p>
<p>import org.apache.hadoop.fs.Path;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.mapreduce.Job;</p>
<p>import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</p>
<p>import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</p>
<p>import org.apache.hadoop.util.Tool;</p>
<p>import org.apache.hadoop.util.ToolRunner;</p>
<p>public class GroupMain extends Configured implements Tool {</p>
<p>​    @Override</p>
<p>​    public int run(String[] args) throws Exception {</p>
<p>​        //获取job对象</p>
<p>​        Job job = Job.getInstance(super.getConf(), “group”);</p>
<p>​        //第一步：读取文件，解析成为key，value对</p>
<p>​        job.setInputFormatClass(TextInputFormat.class);</p>
<p>​        TextInputFormat.addInputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\9、mapreduce当中的分组求topN\数据”));</p>
<p>​        //第二步：自定义map逻辑</p>
<p>​        job.setMapperClass(GroupMapper.class);</p>
<p>​        job.setMapOutputKeyClass(OrderBean.class);</p>
<p>​        job.setMapOutputValueClass(NullWritable.class);</p>
<p>​        //第三步：分区</p>
<p>​        job.setPartitionerClass(GroupPartition.class);</p>
<p>​        //第四步：排序  已经做了</p>
<p>​        //第五步：规约  combiner  省掉</p>
<p>​        //第六步：分组   自定义分组逻辑</p>
<p>​        job.setGroupingComparatorClass(MyGroup.class);</p>
<p>​        //第七步：设置reduce逻辑</p>
<p>​        job.setReducerClass(GroupReducer.class);</p>
<p>​        job.setOutputKeyClass(OrderBean.class);</p>
<p>​        job.setOutputValueClass(NullWritable.class);</p>
<p>​        //第八步：设置输出路径</p>
<p>​        job.setOutputFormatClass(TextOutputFormat.class);</p>
<p>​        TextOutputFormat.setOutputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\9、mapreduce当中的分组求topN\数据\out_top1”));</p>
<p>​        boolean b = job.waitForCompletion(true);</p>
<p>​        return b?0:1;</p>
<p>​    }</p>
<p>​    public static void main(String[] args) throws Exception {</p>
<p>​        int run = ToolRunner.run(new Configuration(), new GroupMain(), args);</p>
<p>​        System.exit(run);</p>
<p>​    }</p>
<p>}</p>
<p>拓展：如何求每个组当中的top2的订单金额数据？？？</p>
<h2 id="15、自定义outputFormat"><a href="#15、自定义outputFormat" class="headerlink" title="15、自定义outputFormat"></a>15、自定义outputFormat</h2><h3 id="1-需求"><a href="#1-需求" class="headerlink" title="1 需求"></a>1 需求</h3><p>现在有一些订单的评论数据，需求，将订单的好评与差评进行区分开来，将最终的数据分开到不同的文件夹下面去，数据内容参见资料文件夹，其中数据第九个字段表示好评，中评，差评。0：好评，1：中评，2：差评</p>
<h3 id="2-分析"><a href="#2-分析" class="headerlink" title="2 分析"></a>2 分析</h3><p>程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现</p>
<h3 id="3-实现"><a href="#3-实现" class="headerlink" title="3 实现"></a>3 实现</h3><p>实现要点：</p>
<p>1、              在mapreduce中访问外部资源</p>
<p>2、              自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write()</p>
<h4 id="第一步：自定义一个outputformat"><a href="#第一步：自定义一个outputformat" class="headerlink" title="第一步：自定义一个outputformat"></a>第一步：自定义一个outputformat</h4><p><strong>import</strong> org.apache.hadoop.fs.FSDataOutputStream;<br> <strong>import</strong> org.apache.hadoop.fs.FileSystem;<br> <strong>import</strong> org.apache.hadoop.fs.Path;<br> <strong>import</strong> org.apache.hadoop.io.NullWritable;<br> <strong>import</strong> org.apache.hadoop.io.Text;<br> <strong>import</strong> org.apache.hadoop.mapreduce.RecordWriter;<br> <strong>import</strong> org.apache.hadoop.mapreduce.TaskAttemptContext;<br> <strong>import</strong> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p> <strong>import</strong> java.io.IOException;</p>
<p> <strong>public class</strong> MyOutPutFormat <strong>extends</strong> FileOutputFormat&lt;Text,NullWritable&gt; {<br>     @Override<br>     <strong>public</strong> RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext context) <strong>throws</strong> IOException, InterruptedException {<br>         FileSystem fs = FileSystem.<em>get</em>(context.getConfiguration());<br>         Path goodComment = <strong>new</strong> Path(<strong>“file:///D:\**</strong>开课吧课程资料\1<strong><strong>、Hadoop&amp;ZooKeeper</strong></strong>课件\2<strong><strong>、hadoop</strong></strong>课件资料\3<strong><strong>、第三次课MapReduce</strong></strong>入门\10<strong><strong>、自定义outputFormat\</strong></strong>数据\goodComment\1.txt”<strong>);<br>         Path badComment = **new</strong> Path(<strong>“file:///D:\**</strong>开课吧课程资料\1<strong><strong>、Hadoop&amp;ZooKeeper</strong></strong>课件\2<strong><strong>、hadoop</strong></strong>课件资料\3<strong><strong>、第三次课MapReduce</strong></strong>入门\10<strong><strong>、自定义outputFormat\</strong></strong>数据\badcomment\2.txt”<strong>);<br>         FSDataOutputStream goodOutputStream = fs.create(goodComment);<br>         FSDataOutputStream badOutputStream = fs.create(badComment);<br>         **return new</strong> MyRecordWriter(goodOutputStream,badOutputStream);<br>     }<br>     <strong>static class</strong> MyRecordWriter <strong>extends</strong> RecordWriter&lt;Text, NullWritable&gt;{</p>
<pre><code>    FSDataOutputStream **goodStream** = **null**;
    FSDataOutputStream **badStream** = **null**;

    **public** MyRecordWriter(FSDataOutputStream goodStream, FSDataOutputStream badStream) {
        **this**.**goodStream** = goodStream;
        **this**.**badStream** = badStream;
    }

    @Override
    **public void** write(Text key, NullWritable value) **throws** IOException, InterruptedException {
        **if** (key.toString().split(**&quot;\t&quot;**)[9].equals(**&quot;0&quot;**)){
            **goodStream**.write(key.toString().getBytes());
            **goodStream**.write(**&quot;\r\n&quot;**.getBytes());
        }**else**{
            **badStream**.write(key.toString().getBytes());
            **badStream**.write(**&quot;\r\n&quot;**.getBytes());
        }
    }
    @Override
    **public void** close(TaskAttemptContext context) **throws** IOException, InterruptedException {
        **if**(**badStream** !=**null**){
            **badStream**.close();
        }
        **if**(**goodStream** !=**null**){
            **goodStream**.close();
        }
    }
}</code></pre><p> }</p>
<h4 id="第二步：开发mapreduce处理流程"><a href="#第二步：开发mapreduce处理流程" class="headerlink" title="第二步：开发mapreduce处理流程"></a>第二步：开发mapreduce处理流程</h4><p><strong>import</strong> org.apache.hadoop.conf.Configuration;<br> <strong>import</strong> org.apache.hadoop.conf.Configured;<br> <strong>import</strong> org.apache.hadoop.fs.Path;<br> <strong>import</strong> org.apache.hadoop.io.LongWritable;<br> <strong>import</strong> org.apache.hadoop.io.NullWritable;<br> <strong>import</strong> org.apache.hadoop.io.Text;<br> <strong>import</strong> org.apache.hadoop.mapreduce.Job;<br> <strong>import</strong> org.apache.hadoop.mapreduce.Mapper;<br> <strong>import</strong> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br> <strong>import</strong> org.apache.hadoop.util.Tool;<br> <strong>import</strong> org.apache.hadoop.util.ToolRunner;</p>
<p> <strong>import</strong> java.io.IOException;</p>
<p> <strong>public class</strong> MyOwnOutputFormatMain <strong>extends</strong> Configured <strong>implements</strong> Tool {<br>     @Override<br>     <strong>public int</strong> run(String[] args) <strong>throws</strong> Exception {<br>         Configuration conf = <strong>super</strong>.getConf();<br>         Job job = Job.<em>getInstance</em>(conf, MyOwnOutputFormatMain.<strong>class</strong>.getSimpleName());<br>         job.setJarByClass(MyOwnOutputFormatMain.<strong>class</strong>);<br>         job.setInputFormatClass(TextInputFormat.<strong>class</strong>);<br>         TextInputFormat.<em>addInputPath</em>(job,<strong>new</strong> Path(<strong>“file:///D:\**</strong>开课吧课程资料\1<strong><strong>、Hadoop&amp;ZooKeeper</strong></strong>课件\2<strong><strong>、hadoop</strong></strong>课件资料\3<strong><strong>、第三次课MapReduce</strong></strong>入门\10<strong><strong>、自定义outputFormat\</strong></strong>数据\input”<strong>));<br>         job.setMapperClass(MyOwnMapper.</strong>class<strong>);<br>         job.setMapOutputKeyClass(Text.</strong>class<strong>);<br>         job.setOutputValueClass(NullWritable.</strong>class<strong>);<br>         job.setOutputFormatClass(MyOutPutFormat.</strong>class<strong>);<br>         *//</strong>设置一个输出目录，这个目录会输出一个success<strong>的成功标志的文件*         MyOutPutFormat.<em>setOutputPath</em>(job,</strong>new<em>* Path(<strong>“file:///D:\**</strong>开课吧课程资料\1<strong><strong>、Hadoop&amp;ZooKeeper</strong></strong>课件\2<strong><strong>、hadoop</strong></strong>课件资料\3<strong><strong>、第三次课MapReduce</strong></strong>入门\10<strong><strong>、自定义outputFormat\</strong></strong>数据\outputResult”<strong>));<br>         job.setOutputKeyClass(Text.</strong>class<strong>);<br>         job.setOutputValueClass(NullWritable.</strong>class<strong>);<br>         **boolean</strong> b = job.waitForCompletion(<strong>true</strong>);<br>         <strong>return</strong> b?0:1;<br>     }<br>     <strong>public static class</strong> MyOwnMapper <strong>extends</strong> Mapper&lt;LongWritable, Text,Text,NullWritable&gt; {<br>         @Override<br>         <strong>protected void</strong> map(LongWritable key, Text value, Context context) <strong>throws</strong> IOException, InterruptedException {<br>             String[] split = value.toString().split(<strong>“\t”</strong>);<br>             String commentStatus = split[9];<br>             context.write(value,NullWritable.*get</em>());<br>         }<br>     }<br>     <strong>public static void</strong> main(String[] args) <strong>throws</strong> Exception {<br>         Configuration configuration = <strong>new</strong> Configuration();<br>         ToolRunner.<em>run</em>(configuration,<strong>new</strong> MyOwnOutputFormatMain(),args);<br>     }<br> }</p>
<h2 id="16、mapTask工作机制"><a href="#16、mapTask工作机制" class="headerlink" title="16、mapTask工作机制"></a>16、mapTask工作机制</h2><p>（1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p>
<p>（2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p>
<p>（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p>
<p>（4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>溢写阶段详情：</p>
<p>步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p>
<p>步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p>
<p>（5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。</p>
<p>在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<h2 id="17、ReduceTask工作机制"><a href="#17、ReduceTask工作机制" class="headerlink" title="17、ReduceTask工作机制"></a>17、ReduceTask工作机制</h2><p>（1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>（2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p>
<p>（3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p>
<p>（4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>
<p>2．设置ReduceTask并行度（个数）</p>
<p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置：</p>
<p>// 默认值是1，手动设置为4</p>
<p>job.setNumReduceTasks(4);</p>
<p>3．实验：测试ReduceTask多少合适</p>
<p>（1）实验环境：1个Master节点，16个Slave节点：CPU:8GHZ，内存: 2G</p>
<p>（2）实验结论：</p>
<p>表4-3 改变ReduceTask （数据量为1GB）</p>
<table>
<thead>
<tr>
<th><strong>MapTask    =16</strong></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>ReduceTask</td>
<td>1</td>
<td>5</td>
<td>10</td>
<td>15</td>
<td>16</td>
<td>20</td>
<td>25</td>
<td>30</td>
<td>45</td>
<td>60</td>
</tr>
<tr>
<td>总时间</td>
<td>892</td>
<td>146</td>
<td>110</td>
<td>92</td>
<td>88</td>
<td>100</td>
<td>128</td>
<td>101</td>
<td>145</td>
<td>104</td>
</tr>
</tbody></table>
<h2 id="18、mapreduce完整流程"><a href="#18、mapreduce完整流程" class="headerlink" title="18、mapreduce完整流程"></a>18、mapreduce完整流程</h2><p>第一步：读取文件，解析成为key，value对</p>
<p>第二步：自定义map逻辑接受k1,v1，转换成为新的k2,v2输出</p>
<p>第三步：分区Partition。将相同key的数据发送到同一个reduce里面去</p>
<p>第四步：排序，map阶段分区内的数据进行排序</p>
<p>第五步：combiner。调优过程，对数据进行map阶段的合并</p>
<p>第六步：将环形缓冲区的数据进行溢写到本地磁盘小文件</p>
<p>第七步：归并排序，对本地磁盘溢写小文件进行归并排序</p>
<p>第八步：等待reduceTask启动线程来进行拉取数据</p>
<p>第九步：reduceTask启动线程拉取属于自己分区的数据</p>
<p>第十步：从mapTask拉取回来的数据继续进行归并排序</p>
<p>第十一步：进行groupingComparator分组操作</p>
<p>第十二步：调用reduce逻辑，写出数据</p>
<p>第十三步：通过outputFormat进行数据输出，写到文件，一个reduceTask对应一个文件</p>
<h2 id="19、shuffle当中的数据压缩"><a href="#19、shuffle当中的数据压缩" class="headerlink" title="19、shuffle当中的数据压缩"></a>19、shuffle当中的数据压缩</h2><p>在shuffle阶段，可以看到数据通过大量的拷贝，从map阶段输出的数据，都要通过网络拷贝，发送到reduce阶段，这一过程中，涉及到大量的网络IO，如果数据能够进行压缩，那么数据的发送量就会少得多，那么如何配置hadoop的文件压缩呢，以及hadoop当中的文件压缩支持哪些压缩算法呢？？我们接下来一一细</p>
<p>MapReduce的执行流程</p>
<p>为什么要配置压缩：</p>
<p>MapReduce</p>
<p>input</p>
<p>mapper</p>
<p>shuffle</p>
<p>partitioner、sort、combiner、【compress】、group</p>
<p>reducer</p>
<p>output</p>
<p>1、hadoop当中支持的压缩算法</p>
<p>文件压缩有两大好处，节约磁盘空间，加速数据在网络和磁盘上的传输</p>
<p>前面我们的hadoop的版本经过我们重新编译之后，我们可以看到我们的hadoop已经支持所有的压缩格式了，剩下的问题就是我们该如何选择使用这些压缩格式来对我们的MapReduce程序进行压缩</p>
<p>我们可以使用bin/hadoop checknative 来查看我们编译之后的hadoop支持的各种压缩，如果出现openssl为false，那么就在线安装一下依赖包</p>
<p>bin/hadoop checknative</p>
<p>yum install openssl-devel</p>
<p>hadoop支持的压缩算法</p>
<table>
<thead>
<tr>
<th><strong>压缩格式</strong></th>
<th><strong>工具</strong></th>
<th><strong>算法</strong></th>
<th><strong>文件扩展名</strong></th>
<th><strong>是否可切分</strong></th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>无</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>否</td>
</tr>
<tr>
<td>LZ4</td>
<td>无</td>
<td>LZ4</td>
<td>.lz4</td>
<td>否</td>
</tr>
<tr>
<td>Snappy</td>
<td>无</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
<p>各种压缩算法对应使用的java类</p>
<table>
<thead>
<tr>
<th><strong>压缩格式</strong></th>
<th><strong>对应使用的java类</strong></th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DeFaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GZipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>LZ4</td>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>常见的压缩速率比较</p>
<table>
<thead>
<tr>
<th><strong>压缩算法</strong></th>
<th><strong>原始文件大小</strong></th>
<th><strong>压缩后的文件大小</strong></th>
<th><strong>压缩速度</strong></th>
<th><strong>解压缩速度</strong></th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO-bset</td>
<td>8.3GB</td>
<td>2GB</td>
<td>4MB/s</td>
<td>60.6MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>135 MB/s</td>
<td>410 MB/s</td>
</tr>
<tr>
<td>snappy</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>172MB/s</td>
<td>409MB/s</td>
</tr>
</tbody></table>
<p>常用的压缩算法主要有LZO和snappy等</p>
<p>如何开启我们的压缩：</p>
<h3 id="方式一：在代码中进行设置压缩"><a href="#方式一：在代码中进行设置压缩" class="headerlink" title="方式一：在代码中进行设置压缩"></a>方式一：在代码中进行设置压缩</h3><p>设置我们的map阶段的压缩</p>
<p>Configuration configuration = new Configuration();</p>
<p> configuration.set(“mapreduce.map.output.compress”,”true”);</p>
<p> configuration.set(“mapreduce.map.output.compress.codec”,”org.apache.hadoop.io.compress.SnappyCodec”);</p>
<p>设置我们的reduce阶段的压缩</p>
<p>configuration.set(“mapreduce.output.fileoutputformat.compress”,”true”);</p>
<p>configuration.set(“mapreduce.output.fileoutputformat.compress.type”<strong>,</strong>“RECORD”);</p>
<p>configuration.set(“mapreduce.output.fileoutputformat.compress.codec”,”org.apache.hadoop.io.compress.SnappyCodec”);</p>
<h3 id="方式二：修改mapred-site-xml进行MapReduce压缩"><a href="#方式二：修改mapred-site-xml进行MapReduce压缩" class="headerlink" title="方式二：修改mapred-site.xml进行MapReduce压缩"></a>方式二：修改mapred-site.xml进行MapReduce压缩</h3><p>我们可以修改mapred-site.xml配置文件，然后重启集群，以便对所有的mapreduce任务进行压缩</p>
<p>map输出数据进行压缩</p>
<property>

<p>​          <name>mapreduce.map.output.compress</name></p>
<p>​          <value>true</value></p>
</property>

<property>

<p>​         <name>mapreduce.map.output.compress.codec</name></p>
<p>​         <value>org.apache.hadoop.io.compress.SnappyCodec</value></p>
</property>



<p>reduce输出数据进行压缩</p>
<property>       

<p><name>mapreduce.output.fileoutputformat.compress</name></p>
<p> <value>true</value></p>
</property>

<property>        

<p><name>mapreduce.output.fileoutputformat.compress.type</name></p>
<p><value>RECORD</value></p>
</property>

 <property>        

<p> <name>mapreduce.output.fileoutputformat.compress.codec</name></p>
<p><value>org.apache.hadoop.io.compress.SnappyCodec</value> </p>
</property>



<p>所有节点都要修改mapred-site.xml，修改完成之后记得重启集群</p>
<h3 id="使用hadoop的snappy压缩来对我们的数据进行压缩"><a href="#使用hadoop的snappy压缩来对我们的数据进行压缩" class="headerlink" title="使用hadoop的snappy压缩来对我们的数据进行压缩"></a>使用hadoop的snappy压缩来对我们的数据进行压缩</h3><h4 id="第一步：代码中添加配置"><a href="#第一步：代码中添加配置" class="headerlink" title="第一步：代码中添加配置"></a>第一步：代码中添加配置</h4><p>这里我们通过修改代码的方式来实现数据的压缩</p>
<p>map阶段输出压缩配置</p>
<p>Configuration configuration = new Configuration();</p>
<p> configuration.set(“mapreduce.map.output.compress”,”true”);</p>
<p>configuration.set(“mapreduce.map.output.compress.codec”,”org.apache.hadoop.io.compress.SnappyCodec”);</p>
<p>reduce阶段输出压缩配置</p>
<p>configuration.set(“mapreduce.output.fileoutputformat.compress”,”true”);</p>
<p> configuration.set(“mapreduce.output.fileoutputformat.compress.type”,”RECORD”);</p>
<p> configuration.set(“mapreduce.output.fileoutputformat.compress.codec”,”org.apache.hadoop.io.compress.SnappyCodec”);</p>
<h4 id="第二步：重新打包测试mr程序"><a href="#第二步：重新打包测试mr程序" class="headerlink" title="第二步：重新打包测试mr程序"></a>第二步：重新打包测试mr程序</h4><p>会发现我们的MR运行之后的输出文件都变成了以.snappy的压缩文件</p>
<h2 id="20、MapReduce当中的计数器-累加器"><a href="#20、MapReduce当中的计数器-累加器" class="headerlink" title="20、MapReduce当中的计数器/累加器"></a>20、MapReduce当中的计数器/累加器</h2><p>计数器是收集作业统计信息的有效手段之一，用于质量控制或应用级统计。计数器还可辅助诊断系统故障。如果需要将日志信息传输到map 或reduce 任务， 更好的方法通常是看能否用一个计数器值来记录某一特定事件的发生。对于大型分布式作业而言，使用计数器更为方便。除了因为获取计数器值比输出日志更方便，还有根据计数器值统计特定事件的发生次数要比分析一堆日志文件容易得多。</p>
<p>hadoop内置计数器列表</p>
<table>
<thead>
<tr>
<th>MapReduce任务计数器</th>
<th>org.apache.hadoop.mapreduce.TaskCounter</th>
</tr>
</thead>
<tbody><tr>
<td>文件系统计数器</td>
<td>org.apache.hadoop.mapreduce.FileSystemCounter</td>
</tr>
<tr>
<td>FileInputFormat计数器</td>
<td>org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter</td>
</tr>
<tr>
<td>FileOutputFormat计数器</td>
<td>org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter</td>
</tr>
<tr>
<td>作业计数器</td>
<td>org.apache.hadoop.mapreduce.JobCounter</td>
</tr>
</tbody></table>
<p>每次mapreduce执行完成之后，我们都会看到一些日志记录出来，其中最重要的一些日志记录如下截图</p>
<p>所有的这些都是MapReduce的计数器的功能，既然MapReduce当中有计数器的功能，我们如何实现自己的计数器？？？</p>
<p>需求：以上面排序以及序列化为案例，统计map接收到的数据记录条数</p>
<p>第一种方式定义计数器，通过context上下文对象可以获取我们的计数器，进行记录</p>
<p>通过context上下文对象，在map端使用计数器进行统计</p>
<p><strong>public class</strong> SortMapper <strong>extends</strong> Mapper&lt;LongWritable,Text,PairWritable,IntWritable&gt; {</p>
<pre><code>**private** PairWritable **mapOutKey** = **new** PairWritable();
**private** IntWritable **mapOutValue** = **new** IntWritable();

@Override
**public  void** map(LongWritable key, Text value, Context context) **throws** IOException, InterruptedException {</code></pre><p> //自定义我们的计数器，这里实现了统计map数据数据的条数<br>         Counter counter = context.getCounter(<strong>“MR_COUNT”</strong>, <strong>“MapRecordCounter”</strong>);<br>         counter.increment(1L);</p>
<pre><code>    String lineValue = value.toString();
    String[] strs = lineValue.split(**&quot;\t&quot;**);

    *//**设置组合key**和value ==&gt; &lt;(key,value),value&gt;*         **mapOutKey**.set(strs[0], Integer.*valueOf*(strs[1]));
    **mapOutValue**.set(Integer.*valueOf*(strs[1]));
    context.write(**mapOutKey**, **mapOutValue**);
}</code></pre><p> }</p>
<p>运行程序之后就可以看到我们自定义的计数器在map阶段读取了七条数据</p>
<p>第二种方式定义计数器</p>
<p>通过enum枚举类型来定义计数器</p>
<p>统计reduce端数据的输入的key有多少个，对应的value有多少个</p>
<p><strong>public class</strong> SortReducer <strong>extends</strong> Reducer&lt;PairWritable,IntWritable,Text,IntWritable&gt; {</p>
<pre><code>**private** Text **outPutKey** = **new** Text();
**public static enum** Counter{
    **REDUCE_INPUT_RECORDS**, **REDUCE_INPUT_VAL_NUMS,**     }
@Override
**public void** reduce(PairWritable key, Iterable&lt;IntWritable&gt; values, Context context) **throws** IOException, InterruptedException {
    context.getCounter(Counter.**REDUCE_INPUT_RECORDS**).increment(1L);
    *//**迭代输出*         **for**(IntWritable value : values) {
        context.getCounter(Counter.**REDUCE_INPUT_VAL_NUMS**).increment(1L);
        **outPutKey**.set(key.getFirst());
        context.write(**outPutKey**, value);
    }
}</code></pre><p> }</p>
<h2 id="21、mapreduce当中的join操作"><a href="#21、mapreduce当中的join操作" class="headerlink" title="21、mapreduce当中的join操作"></a>21、mapreduce当中的join操作</h2><h3 id="1、reduce端的join操作"><a href="#1、reduce端的join操作" class="headerlink" title="1、reduce端的join操作"></a>1、reduce端的join操作</h3><p>1、需求：</p>
<p>订单数据表t_order：</p>
<table>
<thead>
<tr>
<th><strong>id</strong></th>
<th><strong>date</strong></th>
<th><strong>pid</strong></th>
<th><strong>amount</strong></th>
</tr>
</thead>
<tbody><tr>
<td>1001</td>
<td>20150710</td>
<td>P0001</td>
<td>2</td>
</tr>
<tr>
<td>1002</td>
<td>20150710</td>
<td>P0002</td>
<td>3</td>
</tr>
<tr>
<td>1002</td>
<td>20150710</td>
<td>P0003</td>
<td>3</td>
</tr>
</tbody></table>
<p>商品信息表t_product</p>
<table>
<thead>
<tr>
<th><strong>id</strong></th>
<th><strong>pname</strong></th>
<th><strong>category_id</strong></th>
<th><strong>price</strong></th>
</tr>
</thead>
<tbody><tr>
<td>P0001</td>
<td>小米5</td>
<td>1000</td>
<td>2000</td>
</tr>
<tr>
<td>P0002</td>
<td>锤子T1</td>
<td>1000</td>
<td>3000</td>
</tr>
</tbody></table>
<p>假如数据量巨大，两表的数据是以文件的形式存储在HDFS中，需要用mapreduce程序来实现一下SQL查询运算：</p>
<p>select    a.id,a.date,b.name,b.category_id,b.price from t_order a join t_product   b on a.pid = b.id   </p>
<p>2、实现机制：</p>
<p>通过将关联的条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce task，在reduce中进行数据的串联</p>
<p>定义Mapper：</p>
<p>import org.apache.hadoop.io.LongWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>import java.io.IOException;</p>
<p>public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; {</p>
<p>​    //现在我们读取了两个文件，如何确定当前处理的这一行数据是来自哪一个文件里面的</p>
<p>​    @Override</p>
<p>​    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {</p>
<p>​     /*   //通过文件名判断.获取文件的切片</p>
<p>​        FileSplit inputSplit = (FileSplit) context.getInputSplit();//获取我们输入的文件的切片</p>
<p>//获取文件名称</p>
<p>​        String name = inputSplit.getPath().getName();</p>
<p>​        if(name.equals(“orders.txt”)){</p>
<p>​            //订单表数据</p>
<p>​        }else{</p>
<p>​            //商品表数据</p>
<p>​        }*/</p>
<p>​        String[] split = value.toString().split(“,”);</p>
<p>​        if( value.toString().startsWith(“p”)){</p>
<p>​        //以商品id作为key2,相同商品的数据都会到一起去</p>
<p>​            context.write(new Text(split[0]),value);</p>
<p>​        }else{</p>
<p>​            context.write(new Text(split[2]),value);</p>
<p>​        }</p>
<p>​    }</p>
<p>}</p>
<p>定义Reducer：</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>import java.io.IOException;</p>
<p>public class ReduceJoinReducer extends Reducer&lt;Text,Text,Text,NullWritable&gt; {</p>
<p>​    @Override</p>
<p>​    protected void reduce(Text key, Iterable<text> values, Context context) throws IOException, InterruptedException {</text></p>
<p>​        String firstPart = “”;</p>
<p>​        String secondPart = “”;</p>
<p>​        for (Text value : values) {</p>
<p>​           if( value.toString().startsWith(“p”)){</p>
<p>​               secondPart = value.toString();</p>
<p>​           }else{</p>
<p>​               firstPart = value.toString();</p>
<p>​           }</p>
<p>​        }</p>
<p>​        context.write(new Text(firstPart + “\t” +  secondPart), NullWritable.get());</p>
<p>​    }</p>
<p>}</p>
<p>定义main方法</p>
<p>import org.apache.hadoop.conf.Configuration;</p>
<p>import org.apache.hadoop.conf.Configured;</p>
<p>import org.apache.hadoop.fs.Path;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Job;</p>
<p>import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</p>
<p>import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</p>
<p>import org.apache.hadoop.util.Tool;</p>
<p>import org.apache.hadoop.util.ToolRunner;</p>
<p>public class ReduceJoinMain  extends Configured implements Tool {</p>
<p>​    @Override</p>
<p>​    public int run(String[] args) throws Exception {</p>
<p>​        //获取job对象</p>
<p>​        Job job = Job.getInstance(super.getConf(), “reduceJoin”);</p>
<p>​        //第一步：读取文件</p>
<p>​        job.setInputFormatClass(TextInputFormat.class);</p>
<p>​        TextInputFormat.addInputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\11、join操作\数据\reduce端join\input”));</p>
<p>​        //第二步：设置自定义mapper逻辑</p>
<p>​        job.setMapperClass(ReduceJoinMapper.class);</p>
<p>​        job.setMapOutputKeyClass(Text.class);</p>
<p>​        job.setMapOutputValueClass(Text.class);</p>
<p>​        //分区，排序，规约，分组 省略</p>
<p>​        //第七步：设置reduce逻辑</p>
<p>​        job.setReducerClass(ReduceJoinReducer.class);</p>
<p>​        job.setOutputKeyClass(Text.class);</p>
<p>​        job.setOutputValueClass(NullWritable.class);</p>
<p>​        //第八步：设置输出数据路径</p>
<p>​        job.setOutputFormatClass(TextOutputFormat.class);</p>
<p>​        TextOutputFormat.setOutputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\11、join操作\数据\reduce端join\output”));</p>
<p>​        boolean b = job.waitForCompletion(true);</p>
<p>​        return b?0:1;</p>
<p>​    }</p>
<p>​    public static void main(String[] args) throws Exception {</p>
<p>​        int run = ToolRunner.run(new Configuration(), new ReduceJoinMain(), args);</p>
<p>​        System.exit(run);</p>
<p>​    }</p>
<p>}</p>
<p><strong>java.lang.Exception: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable</strong>   </p>
<p><strong>setMapOutputKeyClass(Text.class)</strong></p>
<h3 id="2、map端join操作"><a href="#2、map端join操作" class="headerlink" title="2、map端join操作"></a>2、map端join操作</h3><p>1、原理阐述</p>
<p>适用于关联表中有小表的情形；</p>
<p>可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果，可以大大提高join操作的并发度，加快处理速度</p>
<p>2、实现示例</p>
<p>–先在mapper类中预先定义好小表，进行join</p>
<p>–引入实际场景中的解决方案：一次加载数据库或者用</p>
<p>定义mapper类：</p>
<p>import org.apache.hadoop.conf.Configuration;</p>
<p>import org.apache.hadoop.filecache.DistributedCache;</p>
<p>import org.apache.hadoop.fs.FSDataInputStream;</p>
<p>import org.apache.hadoop.fs.FileSystem;</p>
<p>import org.apache.hadoop.fs.Path;</p>
<p>import org.apache.hadoop.io.LongWritable;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>import java.io.BufferedReader;</p>
<p>import java.io.IOException;</p>
<p>import java.io.InputStreamReader;</p>
<p>import java.net.URI;</p>
<p>import java.util.HashMap;</p>
<p>import java.util.Map;</p>
<p>public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; {</p>
<p>​    private Map&lt;String,String&gt;  pdtsMap ;</p>
<p>​    /**</p>
<p>​     * 初始化方法，只在程序启动调用一次</p>
<p>​     * @param context</p>
<p>​     * @throws IOException</p>
<p>​     * @throws InterruptedException</p>
<p>​     */</p>
<p>​    @Override</p>
<p>​    protected void setup(Context context) throws IOException, InterruptedException {</p>
<p>​        pdtsMap = new HashMap&lt;String, String&gt;();</p>
<p>​        Configuration configuration = context.getConfiguration();</p>
<p>​        //获取到所有的缓存文件，但是现在只有一个缓存文件</p>
<p>​        URI[] cacheFiles = DistributedCache.getCacheFiles(configuration);</p>
<p>​        //获取到 了我们放进去的缓存文件</p>
<p>​        URI cacheFile = cacheFiles[0];</p>
<p>​        //获取FileSystem</p>
<p>​        FileSystem fileSystem = FileSystem.get(cacheFile, configuration);</p>
<p>​        //读取文件，获取到输入流。这里面装的都是商品表的数据</p>
<p>​        FSDataInputStream fsDataInputStream = fileSystem.open(new Path(cacheFile));</p>
<p>​        /**</p>
<p>​         * p0001,xiaomi,1000,2</p>
<p>​         p0002,appale,1000,3</p>
<p>​         p0003,samsung,1000,4</p>
<p>​         */</p>
<p>​        //获取到BufferedReader 之后就可以一行一行的读取数据</p>
<p>​        BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fsDataInputStream));</p>
<p>​        String line =null;</p>
<p>​        while((line = bufferedReader.readLine()) != null){</p>
<p>​            String[] split = line.split(“,”);</p>
<p>​            pdtsMap.put(split[0],line);</p>
<p>​        }</p>
<p>​    }</p>
<p>​    @Override</p>
<p>​    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {</p>
<p>​        String[] split = value.toString().split(“,”);</p>
<p>​        //获取订单表的商品id</p>
<p>​        String pid = split[2];</p>
<p>​        //获取商品表的数据</p>
<p>​        String pdtsLine = pdtsMap.get(pid);</p>
<p>​        context.write(new Text(value.toString()+”\t” +  pdtsLine), NullWritable.get());</p>
<p>​    }</p>
<p>}</p>
<p>定义main方法：</p>
<p>import org.apache.hadoop.conf.Configuration;</p>
<p>import org.apache.hadoop.conf.Configured;</p>
<p>import org.apache.hadoop.filecache.DistributedCache;</p>
<p>import org.apache.hadoop.fs.Path;</p>
<p>import org.apache.hadoop.io.NullWritable;</p>
<p>import org.apache.hadoop.io.Text;</p>
<p>import org.apache.hadoop.mapreduce.Job;</p>
<p>import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</p>
<p>import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</p>
<p>import org.apache.hadoop.util.Tool;</p>
<p>import org.apache.hadoop.util.ToolRunner;</p>
<p>import java.net.URI;</p>
<p>public class MapJoinMain extends Configured implements Tool {</p>
<p>​    @Override</p>
<p>​    public int run(String[] args) throws Exception {</p>
<p>​        URI uri = new URI(“hdfs://node01:8020/cache/pdts.txt”);</p>
<p>​        Configuration conf = super.getConf();</p>
<p>​        //添加缓存文件</p>
<p>​        DistributedCache.addCacheFile(uri,conf);</p>
<p>​        //获取job对象</p>
<p>​        Job job = Job.getInstance(conf, “mapJoin”);</p>
<p>​        //读取文件，解析成为key，value对</p>
<p>​        job.setInputFormatClass(TextInputFormat.class);</p>
<p>​        TextInputFormat.addInputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\11、join操作\数据\map端join\map_join_iput”));</p>
<p>​        job.setMapperClass(MapJoinMapper.class);</p>
<p>​        job.setMapOutputKeyClass(Text.class);</p>
<p>​        job.setMapOutputValueClass(NullWritable.class);</p>
<p>​        //没有reducer逻辑，不用设置了</p>
<p>​        job.setOutputFormatClass(TextOutputFormat.class);</p>
<p>​        TextOutputFormat.setOutputPath(job,new Path(“file:///D:\开课吧课程资料\Hadoop&amp;ZooKeeper课件\最新版本课件\hadoop与zookeeper课件资料\3、第三天\11、join操作\数据\map端join\output_result”));</p>
<p>​        boolean b = job.waitForCompletion(true);</p>
<p>​        return b?0:1;</p>
<p>​    }</p>
<p>​    public static void main(String[] args) throws Exception {</p>
<p>​        int run = ToolRunner.run(new Configuration(), new MapJoinMain(), args);</p>
<p>​        System.exit(run);</p>
<p>​    }</p>
<p>}</p>
<h1 id="10、yarn资源调度管理平台"><a href="#10、yarn资源调度管理平台" class="headerlink" title="10、yarn资源调度管理平台"></a>10、yarn资源调度管理平台</h1><h2 id="1、yarn的介绍"><a href="#1、yarn的介绍" class="headerlink" title="1、yarn的介绍"></a>1、yarn的介绍</h2><p>·         Apache Hadoop YARN(Yet Another Resource Negotiator)是Hadoop的子项目，为分离Hadoop2.0资源管理和计算组件而引入</p>
<p>·         YRAN具有足够的通用性，可以支持其它的分布式计算模式</p>
<h2 id="2、yarn的架构"><a href="#2、yarn的架构" class="headerlink" title="2、yarn的架构"></a>2、yarn的架构</h2><p>类似HDFS，YARN也是经典的<strong>主从（**</strong>master/slave<strong>**）架构</strong></p>
<p>·         YARN服务由一个ResourceManager（RM）和多个NodeManager（NM）构成</p>
<p>·         ResourceManager为主节点（master）</p>
<p>·         NodeManager为从节点（slave）</p>
<p>ApplicationMaster可以在容器内运行任何类型的任务。例如，MapReduce ApplicationMaster请求容器启动map或reduce任务，而Giraph ApplicationMaster请求容器运行Giraph任务。</p>
<h3 id="2-1-ResourceManager"><a href="#2-1-ResourceManager" class="headerlink" title="2.1 ResourceManager"></a>2.1 ResourceManager</h3><p>·         ResourceManager是YARN中主的角色</p>
<p>·         RM是一个全局的资源管理器，集群只有一个对外提供服务</p>
<p>o    负责整个系统的资源管理和分配</p>
<p>o    包括处理客户端请求</p>
<p>o    启动/监控 ApplicationMaster</p>
<p>o    监控 NodeManager、资源的分配与调度</p>
<p>·         它主要由两个组件构成：</p>
<p>o    调度器（Scheduler）</p>
<p>o    应用程序管理器（Applications Manager，ASM）</p>
<p>·         调度器Scheduler</p>
<p>o    调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。</p>
<p>o    需要注意的是，该调度器是一个“纯调度器”</p>
<p>§  它不从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。</p>
<p>§  调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。</p>
<p>·         应用程序管理器Applications Manager，ASM</p>
<p>o    应用程序管理器主要负责管理整个系统中所有应用程序</p>
<p>o    接收job的提交请求</p>
<p>o    为应用分配第一个 Container 来运行 ApplicationMaster</p>
<p>§  包括应用程序提交</p>
<p>§  与调度器协商资源以启动 ApplicationMaster</p>
<p>§  监控 ApplicationMaster 运行状态并在失败时重新启动它等</p>
<h3 id="2-2-NodeManager"><a href="#2-2-NodeManager" class="headerlink" title="2.2 NodeManager"></a>2.2 NodeManager</h3><p>·         NodeManager 是YARN中的 slave角色</p>
<p>·         NodeManager ：</p>
<p>o    每个计算节点，运行一个NodeManager进程；通过心跳（每秒 yarn.resourcemanager.nodemanagers.heartbeat-interval-ms ）上报节点的资源状态(磁盘，内存，cpu等使用信息)</p>
<p>o    它负责接收 ResourceManager 的资源分配请求，分配具体的 Container 给应用。</p>
<p>·         负责监控并报告 Container 使用信息给 ResourceManager。</p>
<p>·         功能：</p>
<p>o    NodeManager 监控本节点上的资源使用情况和各个 Container 的运行状态（cpu和内存等资源）</p>
<p>o    接收及处理来自 ResourceManager 的命令请求，分配 Container 给应用的某个任务；</p>
<p>o    定时地向RM汇报以确保整个集群平稳运行，RM 通过收集每个 NodeManager 的报告信息来追踪整个集群健康状态的，而 NodeManager 负责监控自身的健康状态；</p>
<p>o    处理来自 ApplicationMaster 的请求；</p>
<p>o    管理着所在节点每个 Container 的生命周期；</p>
<p>o    管理每个节点上的日志；</p>
<p>o    当一个节点启动时，它会向 ResourceManager 进行注册并告知 ResourceManager 自己有多少资源可用。</p>
<p>o    在运行期，通过 NodeManager 和 ResourceManager 协同工作，这些信息会不断被更新并保障整个集群发挥出最佳状态。</p>
<p>o    NodeManager 只负责管理自身的 Container，它并不知道运行在它上面应用的信息。负责管理应用信息的组件是 ApplicationMaster</p>
<h3 id="2-3-Container"><a href="#2-3-Container" class="headerlink" title="2.3 Container"></a>2.3 Container</h3><p>·         Container 是 YARN 中的资源抽象</p>
<p>o    YARN以Container为单位分配资源</p>
<p>o    它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等</p>
<p>o    当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的</p>
<p>·         YARN 会为每个任务分配一个 Container，且该任务只能使用该 Container 中描述的资源。</p>
<p>·         Container 和集群NodeManager节点的关系是：</p>
<p>o    一个NodeManager节点可运行多个 Container</p>
<p>o    但一个 Container 不会跨节点。</p>
<p>o    任何一个 job 或 application 必须运行在一个或多个 Container 中</p>
<p>o    在 Yarn 框架中，ResourceManager 只负责告诉 ApplicationMaster 哪些 Containers 可以用</p>
<p>o    ApplicationMaster 还需要去找 NodeManager 请求分配具体的 Container。</p>
<p>·         需要注意的是</p>
<p>o    Container 是一个动态资源划分单位，是根据应用程序的需求动态生成的</p>
<p>o    目前为止，YARN 仅支持 CPU 和内存两种资源，且使用了轻量级资源隔离机制 Cgroups 进行资源隔离。</p>
<p>·         功能：</p>
<p>o    对task环境的抽象；</p>
<p>o    描述一系列信息；</p>
<p>o    任务运行资源的集合（cpu、内存、io等）；</p>
<p>o    任务运行环境</p>
<h3 id="2-4-ApplicationMaster"><a href="#2-4-ApplicationMaster" class="headerlink" title="2.4 ApplicationMaster"></a>2.4 ApplicationMaster</h3><p>·         功能：</p>
<p>o    获得数据分片；</p>
<p>o    为应用程序申请资源并进一步分配给内部任务（TASK）；</p>
<p>o    任务监控与容错；</p>
<p>o    负责协调来自ResourceManager的资源，并通过NodeManager监视容器的执行和资源使用情况。</p>
<p>·         ApplicationMaster 与 ResourceManager 之间的通信</p>
<p>o    是整个 Yarn 应用从提交到运行的最核心部分，是 Yarn 对整个集群进行动态资源管理的根本步骤</p>
<p>o    Yarn 的动态性，就是来源于多个Application 的 ApplicationMaster 动态地和 ResourceManager 进行沟通，不断地申请、释放、再申请、再释放资源的过程。</p>
<h3 id="2-5-JobHistoryServer"><a href="#2-5-JobHistoryServer" class="headerlink" title="2.5 JobHistoryServer"></a>2.5 JobHistoryServer</h3><p>作业历史服务</p>
<p>记录在yarn中调度的作业历史运行情况情况，可以通过历史任务日志服务器来查看hadoop的历史任务，出现错误都应该第一时间来查看日志日志</p>
<p>配置历史日志服务</p>
<h4 id="第一步：修改mapred-site-xml"><a href="#第一步：修改mapred-site-xml" class="headerlink" title="第一步：修改mapred-site.xml"></a>第一步：修改mapred-site.xml</h4><p>node01执行以下命令修改mapred-site.xml</p>
<p>cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop</p>
<p>vim mapred-site.xml</p>
<property>

<p>​    <name>mapreduce.jobhistory.address</name></p>
<p>​    <value>node01:10020</value></p>
</property>



<property>

<p>​    <name>mapreduce.jobhistory.webapp.address</name></p>
<p>​    <value>node01:19888</value></p>
</property>

<p>注意：如果已经存在以上两项配置，那么就不需要再进行配置了</p>
<h4 id="第二步：修改yan-site-xml"><a href="#第二步：修改yan-site-xml" class="headerlink" title="第二步：修改yan-site.xml"></a>第二步：修改yan-site.xml</h4><p>cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop</p>
<p>vim yarn-site.xml</p>
<property>

<p>​    <name>yarn.log-aggregation-enable</name></p>
<p>​    <value>true</value></p>
</property>

<!-- 多长时间聚合删除一次日志 此处 -->

<property>

<p>​    <name>yarn.log-aggregation.retain-seconds</name></p>
<p>​    <value>2592000</value><!--30 day--></p>
</property>

<!-- 时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的 -->

<property>

<p>​    <name>yarn.nodemanager.log.retain-seconds</name></p>
<p>​    <value>604800</value><!-- 7 day --></p>
</property>

<!-- 指定文件压缩类型用于压缩汇总日志 -->

<property>

<p>​    <name>yarn.nodemanager.log-aggregation.compression-type</name></p>
<p>​    <value>gz</value></p>
</property>

<!--  nodemanager本地文件存储目录  -->

<property>

<p>​    <name>yarn.nodemanager.local-dirs</name></p>
<p>​    <value>/kkb/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/yarn/local</value></p>
</property>

<!--  resourceManager  保存最大的任务完成个数  -->

<property>

<p>​    <name>yarn.resourcemanager.max-completed-applications</name></p>
<p>​    <value>1000</value></p>
</property>











<h4 id="第三步：将修改后的文件同步到其他机器上面去"><a href="#第三步：将修改后的文件同步到其他机器上面去" class="headerlink" title="第三步：将修改后的文件同步到其他机器上面去"></a>第三步：将修改后的文件同步到其他机器上面去</h4><p>node01服务器执行以下命令，将修改后的文件同步发送到其他服务器上面去</p>
<p>cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop</p>
<p>scp mapred-site.xml  yarn-site.xml  node02:$PWD</p>
<p>scp mapred-site.xml  yarn-site.xml  node03:$PWD</p>
<h4 id="第四步：重启yarn以及jobhistory服务"><a href="#第四步：重启yarn以及jobhistory服务" class="headerlink" title="第四步：重启yarn以及jobhistory服务"></a>第四步：重启yarn以及jobhistory服务</h4><p>node01执行以下命令重启yarn以及jobhistory服务</p>
<p>cd /kkb/install/hadoop-2.6.0-cdh5.14.2</p>
<p>sbin/start-yarn.sh</p>
<p>sbin/mr-jobhistory-daemon.sh start historyserver</p>
<h3 id="2-7-Timeline-Server"><a href="#2-7-Timeline-Server" class="headerlink" title="2.7 Timeline Server"></a>2.7 Timeline Server</h3><p>用来写日志服务数据 , 一般来写与第三方结合的日志服务数据(比如spark等)</p>
<p>它是对jobhistoryserver功能的有效补充，jobhistoryserver只能对mapreduce类型的作业信息进行记录</p>
<p>它记录除了jobhistoryserver能够对作业运行过程中信息进行记录之外</p>
<p>还记录更细粒度的信息，比如任务在哪个队列中运行，运行任务时设置的用户是哪个用户。</p>
<p>根据官网的解释jobhistoryserver只能记录mapreduce应用程序的记录，timelineserver功能更强大,但不是替代jobhistory两者是功能间的互补关系</p>
<h2 id="3、yarn的工作机制以及任务提交流程"><a href="#3、yarn的工作机制以及任务提交流程" class="headerlink" title="3、yarn的工作机制以及任务提交流程"></a>3、yarn的工作机制以及任务提交流程</h2><p>2．工作机制详解</p>
<p>​       （1）MR程序提交到客户端所在的节点。</p>
<p>​       （2）YarnRunner向ResourceManager申请一个Application。</p>
<p>​       （3）RM将该应用程序的资源路径返回给YarnRunner。</p>
<p>​       （4）该程序将运行所需资源提交到HDFS上。</p>
<p>​       （5）程序资源提交完毕后，申请运行mrAppMaster。</p>
<p>​       （6）RM将用户的请求初始化成一个Task。</p>
<p>​       （7）其中一个NodeManager领取到Task任务。</p>
<p>​       （8）该NodeManager创建容器Container，并产生MRAppmaster。</p>
<p>​       （9）Container从HDFS上拷贝资源到本地。</p>
<p>​       （10）MRAppmaster向RM 申请运行MapTask资源。</p>
<p>​       （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<p>​       （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p>
<p>（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</p>
<p>​       （14）ReduceTask向MapTask获取相应分区的数据。</p>
<p>​       （15）程序运行完毕后，MR会向RM申请注销自己。</p>
<h2 id="4、yarn的任务调度器"><a href="#4、yarn的任务调度器" class="headerlink" title="4、yarn的任务调度器"></a>4、yarn的任务调度器</h2><h3 id="1、-资源调度器的职能"><a href="#1、-资源调度器的职能" class="headerlink" title="1、 资源调度器的职能"></a>1、 资源调度器的职能</h3><p>资源调度器是YARN最核心的组件之一，是一个插拔式的服务组件，负责整个集群资源的管理和分配。YARN提供了三种可用的资源调度器：FIFO、Capacity Scheduler、Fair Scheduler。</p>
<h3 id="2、三种调度器的介绍"><a href="#2、三种调度器的介绍" class="headerlink" title="2、三种调度器的介绍"></a>2、三种调度器的介绍</h3><p>1．先进先出调度器（FIFO），如图4-27所示</p>
<p>有两个任务，第一个任务是一个大的任务，需要执行3个小时，第二个任务是一个小任务，需要执行3分钟，使用先进先出调度器第二个任务需要等第一个任务执行结束才可以执行。</p>
<p>2．容量调度器（Capacity Scheduler），如图4-28所示</p>
<p>3．公平调度器（Fair Scheduler），如图4-29所示</p>
<h3 id="3、自定义队列，实现任务提交不同队列"><a href="#3、自定义队列，实现任务提交不同队列" class="headerlink" title="3、自定义队列，实现任务提交不同队列"></a>3、自定义队列，实现任务提交不同队列</h3><p>前面我们看到了hadoop当中有各种资源调度形式，当前hadoop的任务提交，默认提交到default队列里面去了，将所有的任务都提交到default队列，我们在实际工作当中，可以通过划分队列的形式，对不同的用户，划分不同的资源，让不同的用户的任务，提交到不同的队列里面去，实现资源的隔离</p>
<p>资源隔离目前有2种，静态隔离和动态隔离。</p>
<p>所谓静态隔离是以服务隔离，是通过cgroups（LINUX control groups) 功能来支持的。比如HADOOP服务包含HDFS, HBASE, YARN等等，那么我们固定的设置比例，HDFS:20%, HBASE:40%, YARN：40%， 系统会帮我们根据整个集群的CPU，内存，IO数量来分割资源，先提一下，IO是无法分割的，所以只能说当遇到IO问题时根据比例由谁先拿到资源，CPU和内存是预先分配好的。</p>
<p>上面这种按照比例固定分割就是静态分割了，仔细想想，这种做法弊端太多，假设我按照一定的比例预先分割好了，但是如果我晚上主要跑mapreduce, 白天主要是HBASE工作，这种情况怎么办？ 静态分割无法很好的支持，缺陷太大</p>
<p>动态隔离只要是针对 YARN以及impala, 所谓动态只是相对静态来说，其实也不是动态。 先说YARN， 在HADOOP整个环境，主要服务有哪些？ mapreduce（这里再提一下，mapreduce是应用，YARN是框架，搞清楚这个概念），HBASE, HIVE，SPARK，HDFS，IMPALA， 实际上主要的大概这些，很多人估计会表示不赞同，oozie, ES, storm , kylin，flink等等这些和YARN离的太远了，不依赖YARN的资源服务，而且这些服务都是单独部署就OK，关联性不大。 所以主要和YARN有关也就是HIVE, SPARK，Mapreduce。这几个服务也正式目前用的最多的（HBASE用的也很多，但是和YARN没啥关系）。</p>
<p>根据上面的描述，大家应该能理解为什么所谓的动态隔离主要是针对YARN。好了，既然YARN占的比重这么多，那么如果能很好的对YARN进行资源隔离，那也是不错的。如果我有3个部分都需要使用HADOOP，那么我希望能根据不同部门设置资源的优先级别，实际上也是根据比例来设置，建立3个queue name, 开发部们30%，数据分析部分50%，运营部门20%。 </p>
<p>设置了比例之后，再提交JOB的时候设置mapreduce.queue.name，那么JOB就会进入指定的队列里面。 非常可惜的是，如果你指定了一个不存在的队列，JOB仍然可以执行，这个是目前无解的，默认提交JOB到YARN的时候，规则是root.users.username ， 队列不存在，会自动以这种格式生成队列名称。 队列设置好之后，再通过ACL来控制谁能提交或者KIll job。</p>
<p>从上面2种资源隔离来看，没有哪一种做的很好，如果非要选一种，我会选取后者，隔离YARN资源， 第一种固定分割服务的方式实在支持不了现在的业务</p>
<p>需求：现在一个集群当中，可能有多个用户都需要使用，例如开发人员需要提交任务，测试人员需要提交任务，以及其他部门工作同事也需要提交任务到集群上面去，对于我们多个用户同时提交任务，我们可以通过配置yarn的多用户资源隔离来进行实现</p>
<h4 id="第一步：node01编辑yarn-site-xml"><a href="#第一步：node01编辑yarn-site-xml" class="headerlink" title="第一步：node01编辑yarn-site.xml"></a>第一步：node01编辑yarn-site.xml</h4><p>node01修改yarn-site.xml添加以下配置</p>
<p>cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop</p>
<p>vim yarn-site.xml</p>
<!--  指定我们的任务调度使用fairScheduler的调度方式  -->

<property>

<p>​      <name>yarn.resourcemanager.scheduler.class</name></p>
<p> <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value></p>
</property>



<!--  指定我们的任务调度的配置文件路径  -->

<property>

<p>​      <name>yarn.scheduler.fair.allocation.file</name></p>
<p>​      <value>/kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/fair-scheduler.xml</value></p>
</property>



<!-- 是否启用资源抢占，如果启用，那么当该队列资源使用

yarn.scheduler.fair.preemption.cluster-utilization-threshold 这么多比例的时候，就从其他空闲队列抢占资源

  -->

<property>

<p>​      <name>yarn.scheduler.fair.preemption</name></p>
<p>​      <value>true</value></p>
</property>

<property>

<p>​      <name>yarn.scheduler.fair.preemption.cluster-utilization-threshold</name></p>
<p>​      <value>0.8f</value></p>
</property>





<!-- 默认提交到default队列  -->

<property>

<p>​      <name>yarn.scheduler.fair.user-as-default-queue</name></p>
<p>​      <value>true</value></p>
<p>​      <description>default is True</description></p>
</property>



<!-- 如果提交一个任务没有到任何的队列，是否允许创建一个新的队列，设置false不允许  -->

<property>

<p>​      <name>yarn.scheduler.fair.allow-undeclared-pools</name></p>
<p>​      <value>false</value></p>
<p>​      <description>default is True</description></p>
</property>

















<h4 id="第二步：node01添加fair-scheduler-xml配置文件"><a href="#第二步：node01添加fair-scheduler-xml配置文件" class="headerlink" title="第二步：node01添加fair-scheduler.xml配置文件"></a>第二步：node01添加fair-scheduler.xml配置文件</h4><p>node01执行以下命令，添加faie-scheduler.xml的配置文件</p>
<p>cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop</p>
<p>vim fair-scheduler.xml</p>
<?xml version="1.0"?>

<allocations>

<!-- users max running apps  -->

<p><usermaxappsdefault>30</usermaxappsdefault></p>
<!-- 定义我们的队列  -->

<queue name="root">

<p>​      <minresources>512mb,4vcores</minresources></p>
<p>​      <maxresources>102400mb,100vcores</maxresources></p>
<p>​      <maxrunningapps>100</maxrunningapps></p>
<p>​      <weight>1.0</weight></p>
<p>​      <schedulingmode>fair</schedulingmode></p>
<p>​      <aclsubmitapps> </aclsubmitapps></p>
<p>​      <acladministerapps> </acladministerapps></p>
<p>​      <queue name="default"></queue></p>
<p>​           <minresources>512mb,4vcores</minresources></p>
<p>​           <maxresources>30720mb,30vcores</maxresources></p>
<p>​           <maxrunningapps>100</maxrunningapps></p>
<p>​           <schedulingmode>fair</schedulingmode></p>
<p>​           <weight>1.0</weight></p>
<p>​           <!--  所有的任务如果不指定任务队列，都提交到default队列里面来 --></p>
<p>​           <aclsubmitapps>*</aclsubmitapps></p>
<p>​      </p></queue><p></p>
<!-- 



weight

资源池权重



aclSubmitApps

允许提交任务的用户名和组；

格式为： 用户名 用户组



当有多个用户时候，格式为：用户名1,用户名2 用户名1所属组,用户名2所属组



aclAdministerApps

允许管理任务的用户名和组；



格式同上。

 -->

<p>​      <queue name="hadoop"></queue></p>
<p>​           <minresources>512mb,4vcores</minresources></p>
<p>​           <maxresources>20480mb,20vcores</maxresources></p>
<p>​           <maxrunningapps>100</maxrunningapps></p>
<p>​           <schedulingmode>fair</schedulingmode></p>
<p>​           <weight>2.0</weight></p>
<p>​           <aclsubmitapps>hadoop hadoop</aclsubmitapps></p>
<p>​           <acladministerapps>hadoop hadoop</acladministerapps></p>
<p>​      </p>
<p>​      <queue name="develop"></queue></p>
<p>​           <minresources>512mb,4vcores</minresources></p>
<p>​           <maxresources>20480mb,20vcores</maxresources></p>
<p>​           <maxrunningapps>100</maxrunningapps></p>
<p>​           <schedulingmode>fair</schedulingmode></p>
<p>​           <weight>1</weight></p>
<p>​           <aclsubmitapps>develop develop</aclsubmitapps></p>
<p>​           <acladministerapps>develop develop</acladministerapps></p>
<p>​      </p>
<p>​      <queue name="test1"></queue></p>
<p>​           <minresources>512mb,4vcores</minresources></p>
<p>​           <maxresources>20480mb,20vcores</maxresources></p>
<p>​           <maxrunningapps>100</maxrunningapps></p>
<p>​           <schedulingmode>fair</schedulingmode></p>
<p>​           <weight>1.5</weight></p>
<p>​           <aclsubmitapps>test1,hadoop,develop test1</aclsubmitapps></p>
<p>​           <acladministerapps>test1 group_businessC,supergroup</acladministerapps></p>
<p>​      </p>


</allocations>





<h4 id="第三步：将修改后的配置文件拷贝到其他机器上"><a href="#第三步：将修改后的配置文件拷贝到其他机器上" class="headerlink" title="第三步：将修改后的配置文件拷贝到其他机器上"></a>第三步：将修改后的配置文件拷贝到其他机器上</h4><p>将node01修改后的yarn-site.xml和fair-scheduler.xml配置文件分发到其他服务器上面去</p>
<p>cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop</p>
<p>[root@node01 hadoop]# scp yarn-site.xml  fair-scheduler.xml node02:$PWD</p>
<p>[root@node01 hadoop]# scp yarn-site.xml  fair-scheduler.xml node03:$PWD</p>
<h4 id="第四步：重启yarn集群"><a href="#第四步：重启yarn集群" class="headerlink" title="第四步：重启yarn集群"></a>第四步：重启yarn集群</h4><p>修改完yarn-site.xml配置文件之后，重启yarn集群，node01执行以下命令进行重启</p>
<p>[root@node01 hadoop]# cd /kkb/install/hadoop-2.6.0-cdh5.14.2/</p>
<p>[root@node01 hadoop-2.6.0-cdh5.14.2]# sbin/stop-yarn.sh</p>
<h4 id="第五步：修改任务提交的队列"><a href="#第五步：修改任务提交的队列" class="headerlink" title="第五步：修改任务提交的队列"></a>第五步：修改任务提交的队列</h4><p>修改代码，添加我们mapreduce任务需要提交到哪一个队列里面去</p>
<p>Configuration configuration = new Configuration();</p>
<p>configuration.set(“mapred.job.queue.name”, “develop”);</p>
<p>hive任务指定提交队列：</p>
<p>hive-site.xml</p>
<property>

<p>​    <name>mapreduce.job.queuename</name></p>
<p>​    <value>test1</value></p>
</property>



<p>spark任务运行指定提交的队列</p>
<p>1- 脚本方式</p>
<p>–queue hadoop</p>
<p>2- 代码方式</p>
<p>saprkConf.set(“yarn,spark.queue”, “develop”)</p>
<h1 id="11、hadoop的企业级调优"><a href="#11、hadoop的企业级调优" class="headerlink" title="11、hadoop的企业级调优"></a>11、hadoop的企业级调优</h1><h2 id="1、hdfs调优以及yarn参数调优"><a href="#1、hdfs调优以及yarn参数调优" class="headerlink" title="1、hdfs调优以及yarn参数调优"></a>1、hdfs调优以及yarn参数调优</h2><h3 id="1）HDFS参数调优hdfs-site-xml"><a href="#1）HDFS参数调优hdfs-site-xml" class="headerlink" title="1）HDFS参数调优hdfs-site.xml"></a>1）HDFS参数调优hdfs-site.xml</h3><p>（1）dfs.namenode.handler.count=20 * log2(Cluster Size)</p>
<p>调整namenode处理客户端的线程数</p>
<p>比如集群规模为8台时，此参数设置为60</p>
<p>The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.</p>
<p>NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。</p>
<p>（2）编辑日志存储路径dfs.namenode.edits.dir设置与镜像文件存储路径dfs.namenode.name.dir尽量分开，达到最低写入延迟</p>
<p>（3）元数据信息fsimage多目录冗余存储配置</p>
<p>元数据信息尽量做raid1磁盘阵列方式来存储，保证元数据信息不会丢失</p>
<p>元数据信息尽量使用固态盘存储</p>
<h3 id="2）YARN参数调优yarn-site-xml"><a href="#2）YARN参数调优yarn-site-xml" class="headerlink" title="2）YARN参数调优yarn-site.xml"></a>2）YARN参数调优yarn-site.xml</h3><p>1）根据实际调整每个节点和单个任务申请内存值</p>
<p>（1）yarn.nodemanager.resource.memory-mb</p>
<p>表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。</p>
<p>（2）yarn.scheduler.maximum-allocation-mb</p>
<p>单个任务可申请的最多物理内存量，默认是8192（MB）。</p>
<p>2）Hadoop宕机</p>
<p>（1）如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是8192MB）</p>
<p>（2）如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。高峰期的时候用Kafka进行缓存，高峰期过去数据同步会自动跟上。</p>
<h2 id="2、mapreduce运行慢的主要原因可能有哪些"><a href="#2、mapreduce运行慢的主要原因可能有哪些" class="headerlink" title="2、mapreduce运行慢的主要原因可能有哪些"></a>2、mapreduce运行慢的主要原因可能有哪些</h2><h2 id="3、mapreduce的优化方法"><a href="#3、mapreduce的优化方法" class="headerlink" title="3、mapreduce的优化方法"></a>3、mapreduce的优化方法</h2><p>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。</p>
<h3 id="1、数据输入阶段"><a href="#1、数据输入阶段" class="headerlink" title="1、数据输入阶段"></a>1、数据输入阶段</h3><h3 id="2、MapTask运行阶段"><a href="#2、MapTask运行阶段" class="headerlink" title="2、MapTask运行阶段"></a>2、MapTask运行阶段</h3><h3 id="3、ReduceTask运行阶段"><a href="#3、ReduceTask运行阶段" class="headerlink" title="3、ReduceTask运行阶段"></a>3、ReduceTask运行阶段</h3><p>Maptask个数：与文件切片有关</p>
<p>Reducetask个数：手动设置 job.setNumReduceTasks(20)</p>
<p>使用输入数据量/256M 估算reduceTask个数</p>
<h3 id="4、IO传输阶段"><a href="#4、IO传输阶段" class="headerlink" title="4、IO传输阶段"></a>4、IO传输阶段</h3><h3 id="5、数据倾斜问题"><a href="#5、数据倾斜问题" class="headerlink" title="5、数据倾斜问题"></a>5、数据倾斜问题</h3><h3 id="6、常用的调优参数"><a href="#6、常用的调优参数" class="headerlink" title="6、常用的调优参数"></a>6、常用的调优参数</h3><h4 id="1．资源相关参数"><a href="#1．资源相关参数" class="headerlink" title="1．资源相关参数"></a>1．资源相关参数</h4><p>（1）以下参数是在用户自己的MR应用程序中配置就可以生效（mapred-default.xml）</p>
<p>表4-12</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.memory.mb</td>
<td>一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.reduce.memory.mb</td>
<td>一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.map.cpu.vcores</td>
<td>每个MapTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.cpu.vcores</td>
<td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.parallelcopies</td>
<td>每个Reduce去Map中取数据的并行数。默认值是5</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.merge.percent</td>
<td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.input.buffer.percent</td>
<td>Buffer大小占Reduce可用内存的比例。默认值0.7</td>
</tr>
<tr>
<td>mapreduce.reduce.input.buffer.percent</td>
<td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td>
</tr>
</tbody></table>
<p>（2）应该在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml）</p>
<p>表4-13</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>给应用程序Container分配的最小内存，默认值：1024</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>给应用程序Container分配的最大内存，默认值：8192</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>每个Container申请的最小CPU核数，默认值：1</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>每个Container申请的最大CPU核数，默认值：32</td>
</tr>
<tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>给Containers分配的最大物理内存，默认值：8192</td>
</tr>
</tbody></table>
<p>（3）Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml）</p>
<p>表4-14</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.task.io.sort.mb</td>
<td>Shuffle的环形缓冲区大小，默认100m</td>
</tr>
<tr>
<td>mapreduce.map.sort.spill.percent</td>
<td>环形缓冲区溢出的阈值，默认80%</td>
</tr>
</tbody></table>
<p>2．容错相关参数(MapReduce性能优化)</p>
<p>表4-15</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.maxattempts</td>
<td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.reduce.maxattempts</td>
<td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.task.timeout</td>
<td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0   Timed out after 300 secsContainer killed by the ApplicationMaster.”。</td>
</tr>
</tbody></table>
<h2 id="4、hdfs小文件解决方案总结"><a href="#4、hdfs小文件解决方案总结" class="headerlink" title="4、hdfs小文件解决方案总结"></a>4、hdfs小文件解决方案总结</h2><h3 id="1、小文件的问题弊端"><a href="#1、小文件的问题弊端" class="headerlink" title="1、小文件的问题弊端"></a>1、小文件的问题弊端</h3><p>HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p>
<h3 id="2、小文件的解决方案"><a href="#2、小文件的解决方案" class="headerlink" title="2、小文件的解决方案"></a>2、小文件的解决方案</h3><p>小文件的优化无非以下几种方式：</p>
<p>（1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。</p>
<p>（2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。</p>
<p>（3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。</p>
<p>开启JVM重用，避免反复出现资源的申请</p>
<p>节点分配的内存 = map内存*vcore 这样计算最好</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/28/Hadoop之环境搭建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/28/Hadoop之环境搭建/" class="post-title-link" itemprop="url">Hadoop之环境搭建</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-28 16:15:22" itemprop="dateCreated datePublished" datetime="2019-11-28T16:15:22+08:00">2019-11-28</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-29 17:09:28" itemprop="dateModified" datetime="2019-11-29T17:09:28+08:00">2019-11-29</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/2019/11/28/Hadoop之环境搭建/1.png" alt="1574930210032"></p>
<p>编译hadoop的源码的作用是让hadoop支持c程序库以及支持压缩库</p>
<p><img src="/2019/11/28/Hadoop之环境搭建/2.png" alt="1574932055540"></p>
<p>拷贝：</p>
<p>scp</p>
<p>rsync</p>
<p>ssh免密登录出现问题：</p>
<p>node01、node03无法免密登录node02.</p>
<p>方案：将/home下的hadoop目录权限修改为700.</p>
<p>注意：.ssh目录下的au文件的权限为600</p>
<p><img src="/2019/11/28/Hadoop之环境搭建/3.png" alt="1575016476584"></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/20/MySQL之存储过程与函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/20/MySQL之存储过程与函数/" class="post-title-link" itemprop="url">存储过程与函数</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-20 20:50:14" itemprop="dateCreated datePublished" datetime="2019-11-20T20:50:14+08:00">2019-11-20</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-22 21:35:10" itemprop="dateModified" datetime="2019-11-22T21:35:10+08:00">2019-11-22</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h1>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2019/11/20/MySQL之存储过程与函数/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/20/数据结构与算法之回溯算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/20/数据结构与算法之回溯算法/" class="post-title-link" itemprop="url">从《蝴蝶效应》中学习回溯算法</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-20 10:42:35" itemprop="dateCreated datePublished" datetime="2019-11-20T10:42:35+08:00">2019-11-20</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-21 14:03:54" itemprop="dateModified" datetime="2019-11-21T14:03:54+08:00">2019-11-21</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/数据结构与算法/" itemprop="url" rel="index"><span itemprop="name">数据结构与算法</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>深度优先搜索算法利用的是回溯算法思想</p>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2019/11/20/数据结构与算法之回溯算法/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/17/数据结构与算法之深度和广度优先搜索/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/17/数据结构与算法之深度和广度优先搜索/" class="post-title-link" itemprop="url">如何找到社交网络中的三度好友关系</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-17 11:12:48 / 修改时间：17:11:51" itemprop="dateCreated datePublished" datetime="2019-11-17T11:12:48+08:00">2019-11-17</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/数据结构与算法/" itemprop="url" rel="index"><span itemprop="name">数据结构与算法</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="六度分割理论"><a href="#六度分割理论" class="headerlink" title="六度分割理论"></a>六度分割理论</h1><p>具体是说，你与世界上的另一个人间隔的关系不会超过六度，也就是说平均只需要六步就可以联系到任何两个互不认识的人</p>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2019/11/17/数据结构与算法之深度和广度优先搜索/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/13/Redis数据库/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/13/Redis数据库/" class="post-title-link" itemprop="url">Redis数据库</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-13 09:37:40" itemprop="dateCreated datePublished" datetime="2019-11-13T09:37:40+08:00">2019-11-13</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-14 09:32:07" itemprop="dateModified" datetime="2019-11-14T09:32:07+08:00">2019-11-14</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Redis/" itemprop="url" rel="index"><span itemprop="name">Redis</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Redis简介"><a href="#Redis简介" class="headerlink" title="Redis简介"></a>Redis简介</h1><p>Redis是一个开源的key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，</p>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2019/11/13/Redis数据库/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/12/Hadoop之HDFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/12/Hadoop之HDFS/" class="post-title-link" itemprop="url">Hadoop之HDFS</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-12 19:25:28" itemprop="dateCreated datePublished" datetime="2019-11-12T19:25:28+08:00">2019-11-12</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-12-04 13:54:25" itemprop="dateModified" datetime="2019-12-04T13:54:25+08:00">2019-12-04</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p><strong>分布式文件系统</strong>，适合一次写入，多次读出的场景，不支持文件的修改，适合用来做数据分析，不适合做网盘应用。</p>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2019/11/12/Hadoop之HDFS/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/11/MySQL之DML语言/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/11/MySQL之DML语言/" class="post-title-link" itemprop="url">MySQL之DML语言</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-11 09:48:15 / 修改时间：10:37:59" itemprop="dateCreated datePublished" datetime="2019-11-11T09:48:15+08:00">2019-11-11</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>DML:数据操纵语言</p>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2019/11/11/MySQL之DML语言/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/fish.png"
      alt="小鱼儿">
  <p class="site-author-name" itemprop="name">小鱼儿</p>
  <div class="site-description" itemprop="description">肩膀有点痒，可能在长小翅膀</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    
  
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-Hans" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小鱼儿</span>
</div>

        












        
      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  





















  

  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>

