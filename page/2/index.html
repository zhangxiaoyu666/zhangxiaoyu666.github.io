<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="肩膀有点痒，可能在长小翅膀">
<meta property="og:type" content="website">
<meta property="og:title" content="一只鱼的博客">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="一只鱼的博客">
<meta property="og:description" content="肩膀有点痒，可能在长小翅膀">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="一只鱼的博客">
<meta name="twitter:description" content="肩膀有点痒，可能在长小翅膀">
  <link rel="canonical" href="http://yoursite.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>一只鱼的博客</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">一只鱼的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">七秒钟的记忆多一秒</p>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-question-circle"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    
      
      
        
      
        
      
        
          
        
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-question-circle"></i>标签<span class="badge">44</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    
      
      
        
      
        
          
        
      
        
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-question-circle"></i>分类<span class="badge">10</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    
      
      
        
          
        
      
        
      
        
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-question-circle"></i>归档<span class="badge">67</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-schedule">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>日程表</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-sitemap">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-commonweal">
      
    
      
      
        
      
        
      
        
      
    

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/06/Flume日志采集框架/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/03/06/Flume日志采集框架/" class="post-title-link" itemprop="url">Flume辅助框架</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-03-06 11:33:44" itemprop="dateCreated datePublished" datetime="2020-03-06T11:33:44+08:00">2020-03-06</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-09 18:39:41" itemprop="dateModified" datetime="2020-03-09T18:39:41+08:00">2020-03-09</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flume日志采集框架"><a href="#Flume日志采集框架" class="headerlink" title="Flume日志采集框架"></a>Flume日志采集框架</h1><h3 id="1-Flume是什么"><a href="#1-Flume是什么" class="headerlink" title="1. Flume是什么"></a>1. Flume是什么</h3><p><img src="/2020/03/06/Flume日志采集框架/1566778379044.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在一个完整的离线大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架。</span><br></pre></td></tr></table></figure>

<ul>
<li>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的<strong>海量日志采集、聚合和传输的系统</strong></li>
<li>Flume支持在日志系统中定制各类数据发送方，用于收集数据；</li>
<li>Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</li>
</ul>
<h3 id="2-Flume的架构"><a href="#2-Flume的架构" class="headerlink" title="2. Flume的架构"></a>2. Flume的架构</h3><p><img src="/2020/03/06/Flume日志采集框架/flume.png" alt></p>
<ul>
<li>Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。</li>
<li>Flume分布式系统中<strong>最核心的角色是agent</strong>，flume采集系统就是由一个个agent所连接起来形成。</li>
<li><strong>每一个agent相当于一个数据传递员，内部有三个组件</strong><ul>
<li><strong>source</strong><ul>
<li>采集组件，用于跟数据源对接，以获取数据</li>
</ul>
</li>
<li><strong>channel</strong><ul>
<li>传输通道组件，缓存数据，用于从source将数据传递到sink</li>
</ul>
</li>
<li><strong>sink</strong><ul>
<li>下沉组件，数据发送给最终存储系统或者下一级agent中</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-Flume采集系统结构图"><a href="#3-Flume采集系统结构图" class="headerlink" title="3. Flume采集系统结构图"></a>3. Flume采集系统结构图</h3><h4 id="3-1-简单结构"><a href="#3-1-简单结构" class="headerlink" title="3.1 简单结构"></a>3.1 简单结构</h4><ul>
<li>单个agent采集数据</li>
</ul>
<p><img src="/2020/03/06/Flume日志采集框架/flume.png" alt></p>
<h4 id="3-2-复杂结构"><a href="#3-2-复杂结构" class="headerlink" title="3.2 复杂结构"></a>3.2 复杂结构</h4><ul>
<li>2个agent串联</li>
</ul>
<p><img src="/2020/03/06/Flume日志采集框架/UserGuide_image03.png" alt></p>
<ul>
<li>多个agent串联</li>
</ul>
<p><img src="/2020/03/06/Flume日志采集框架/UserGuide_image02.png" alt></p>
<ul>
<li>多个channel</li>
</ul>
<p><img src="/2020/03/06/Flume日志采集框架/UserGuide_image04.png" alt></p>
<h3 id="4-Flume安装部署"><a href="#4-Flume安装部署" class="headerlink" title="4. Flume安装部署"></a>4. Flume安装部署</h3><p>Flume安装很简单，解压好基本上就可以使用</p>
<ul>
<li><p>1、下载安装包</p>
<ul>
<li><a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz</a></li>
<li>flume-ng-1.6.0-cdh5.14.2.tar.gz</li>
</ul>
</li>
<li><p>2、规划安装目录</p>
<ul>
<li>/install</li>
</ul>
</li>
<li><p>3、上传安装包到服务器</p>
</li>
<li><p>4、解压安装包到指定的规划目录</p>
<ul>
<li>tar -zxvf flume-ng-1.6.0-cdh5.14.2.tar.gz -C /install</li>
</ul>
</li>
<li><p>5、重命名解压目录</p>
<ul>
<li>mv apache-flume-1.6.0-cdh5.14.2-bin  flume-1.6.0-cdh5.14.2</li>
</ul>
</li>
<li><p>6、修改配置</p>
<ul>
<li><p>进入到flume安装目录下的conf文件夹中</p>
<ul>
<li><p>先重命名文件</p>
<ul>
<li>mv flume-env.sh.template flume-env.sh</li>
</ul>
</li>
<li><p>修改文件，添加java环境变量</p>
<ul>
<li>vim flume-env.sh</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/kkb/install/jdk1.8.0_141</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-Flume实战"><a href="#5-Flume实战" class="headerlink" title="5. Flume实战"></a>5. Flume实战</h3><h4 id="5-1-采集文件到控制台"><a href="#5-1-采集文件到控制台" class="headerlink" title="5.1 采集文件到控制台"></a>5.1 采集文件到控制台</h4><ul>
<li><p>1、需求描述</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">监控一个文件如果有新增的内容就把数据采集之后打印控制台，通常用于测试/调试目的</span><br></pre></td></tr></table></figure>
</li>
<li><p>2、<strong>flume配置文件开发</strong></p>
<ul>
<li><p>在flume的安装目录下创建一个文件夹myconf， 后期存放flume开发的配置文件</p>
<ul>
<li>mkdir /install/flume-1.6.0-cdh5.14.2/myconf</li>
</ul>
</li>
<li><p>vim tail-memory-logger.conf</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">#定义一个agent，分别指定source、channel、sink别名</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置source</span><br><span class="line">#指定source的类型为exec，通过Unix命令来传输结果数据</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">#监控一个文件，有新的数据产生就不断采集走</span><br><span class="line">a1.sources.r1.command = tail -F /install/flumeData/tail.log</span><br><span class="line">#指定source的数据流入的channel中</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置channel</span><br><span class="line">#指定channel的类型为memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">#指定channel的最多可以存放数据的容量</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">#指定在一个事务中source写数据到channel或者sink从channel取数据最大条数</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">#类型是日志格式，结果会打印在控制台</span><br><span class="line">a1.sinks.k1.type = logger</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>3、启动agent</strong></p>
<ul>
<li>进入到node01上的/install/flume-1.6.0-cdh5.14.2目录下执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/tail-memory-logger.conf -</span><br><span class="line">Dflume.root.logger=info,console</span><br><span class="line"></span><br><span class="line">其中：</span><br><span class="line">-n[name]表示指定该agent名称</span><br><span class="line">-c[myconf]表示配置文件所在的目录</span><br><span class="line">-f表示配置文件的路径名称</span><br><span class="line">-D表示指定key=value键值对---这里指定的是启动的日志输出级别</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="5-2-采集文件到HDFS"><a href="#5-2-采集文件到HDFS" class="headerlink" title="5.2 采集文件到HDFS"></a>5.2 采集文件到HDFS</h4><ul>
<li><p>1、需求描述</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">监控一个文件如果有新增的内容就把数据采集到HDFS上</span><br></pre></td></tr></table></figure>
</li>
<li><p>2、结构示意图</p>
</li>
</ul>
<p><img src="/2020/03/06/Flume日志采集框架/file-Flume-HDFS.png" alt></p>
<ul>
<li><p><strong>3、flume配置文件开发</strong></p>
<ul>
<li>vim file2Hdfs.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /install/flumeData/tail.log</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置channel</span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">#设置检查点目录--该目录是记录下event在数据目录下的位置</span><br><span class="line">a1.channels.c1.checkpointDir=/data/flume_checkpoint</span><br><span class="line">#数据存储所在的目录</span><br><span class="line">a1.channels.c1.dataDirs=/data/flume_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">#指定sink类型为hdfs</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">#指定数据收集到hdfs目录</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://node01:8020/tailFile/%Y-%m-%d/%H%M</span><br><span class="line">#指定生成文件名的前缀</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line"></span><br><span class="line">#是否启用时间上的”舍弃”   --&gt;控制目录 </span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">#时间上进行“舍弃”的值</span><br><span class="line"># 如 12:10 -- 12:19 =&gt; 12:10</span><br><span class="line"># 如 12:20 -- 12:29 =&gt; 12:20</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">#时间上进行“舍弃”的单位</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line"></span><br><span class="line"># 控制文件个数</span><br><span class="line">#60s或者50字节或者10条数据，谁先满足，就开始滚动生成新文件</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 60</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 50</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 10</span><br><span class="line"></span><br><span class="line">#每个批次写入的数据量</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"></span><br><span class="line">#开始本地时间戳--开启后就可以使用%Y-%m-%d去解析时间</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br><span class="line">#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>4、启动agent</strong></p>
<ul>
<li><p>进入到node01上的/install/flume-1.6.0-cdh5.14.2目录下执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/file2Hdfs.conf -Dflume.root.logger=info,consol</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h4 id="5-3-采集目录到HDFS"><a href="#5-3-采集目录到HDFS" class="headerlink" title="5.3 采集目录到HDFS"></a>5.3 采集目录到HDFS</h4><ul>
<li><p>1、需求描述</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一个目录中不断有新的文件产生，需要把目录中的文件不断地进行数据收集保存到HDFS上</span><br></pre></td></tr></table></figure>
</li>
<li><p>2、结构示意图</p>
<p><img src="/2020/03/06/Flume日志采集框架/Dir-Flume-HDFS.png" alt></p>
</li>
<li><p><strong>3、flume配置文件开发</strong></p>
<ul>
<li>在myconf目录中创建配置文件添加内容<ul>
<li>vim  dir2Hdfs.conf</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># 配置source</span><br><span class="line">##注意：不能往监控目中重复丢同名文件</span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.spoolDir = /install/flumeData/files</span><br><span class="line"># 是否将文件的绝对路径添加到header</span><br><span class="line">a1.sources.r1.fileHeader = true</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://node01:9000/spooldir/%Y-%m-%d/%H%M</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 60</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 50</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 10</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>4、启动agent</strong></p>
</li>
<li><p>进入到node01上的/install/flume-1.6.0-cdh5.14.2目录下执行</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/dir2Hdfs.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<h4 id="5-4-两个agent级联"><a href="#5-4-两个agent级联" class="headerlink" title="5.4 两个agent级联"></a>5.4 两个agent级联</h4><ul>
<li>1、需求描述</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一个agent负责监控某个目录中新增的文件进行数据收集，通过网络发送到第二个agent当中去，第二个agent负责接收第一个agent发送的数据，并将数据保存到hdfs上面去。</span><br></pre></td></tr></table></figure>

<ul>
<li>2、结构示意图</li>
</ul>
<p><img src="/2020/03/06/Flume日志采集框架/Dir-Flume-HDFS.png" alt></p>
<ul>
<li><p>3、在node01和node02上分别都安装flume</p>
</li>
<li><p>4、创建node01上的flume配置文件</p>
<ul>
<li>vim dir2avro.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># 配置source</span><br><span class="line">##注意：不能往监控目中重复丢同名文件</span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.spoolDir = /install/flumeData/files</span><br><span class="line">a1.sources.r1.fileHeader = true</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">#AvroSink是用来通过网络来传输数据的,可以将event发送到RPC服务器(比如AvroSource)</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line"></span><br><span class="line">#node02 注意修改为自己的hostname</span><br><span class="line">a1.sinks.k1.hostname = node02</span><br><span class="line">a1.sinks.k1.port = 4141</span><br></pre></td></tr></table></figure>
</li>
<li><p>5、创建node02上的flume配置文件</p>
<ul>
<li>vim avro2Hdfs.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置source</span><br><span class="line">#通过AvroSource接受AvroSink的网络数据</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">#AvroSource服务的ip地址</span><br><span class="line">a1.sources.r1.bind = node02</span><br><span class="line">#AvroSource服务的端口</span><br><span class="line">a1.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line">#配置channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://node01:9000/avro-hdfs/%Y-%m-%d/%H-%M</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 60</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 50</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 10</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>6、启动agent</strong></p>
<ul>
<li><p>先启动node02上的flume。然后在启动node01上的flume</p>
<ul>
<li>在node02上的flume安装目录下执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/avro2Hdfs.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<ul>
<li>在node01上的flume安装目录下执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/dir2avro.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<ul>
<li>最后在node01上的/kkb/install/flumeData/files目录下创建一些数据文件，最后去HDFS上查看数据。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="6-高可用配置案例"><a href="#6-高可用配置案例" class="headerlink" title="6. 高可用配置案例"></a>6. 高可用配置案例</h3><h4 id="6-1-failover故障转移"><a href="#6-1-failover故障转移" class="headerlink" title="6.1 failover故障转移"></a>6.1 failover故障转移</h4><p><img src="/2020/03/06/Flume日志采集框架/flume-failover.png" alt></p>
<ul>
<li>1、节点分配</li>
</ul>
<table>
<thead>
<tr>
<th align="center">名称</th>
<th align="center">服务器主机名</th>
<th align="center">ip地址</th>
<th align="center">角色</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Agent1</td>
<td align="center">node01</td>
<td align="center">192.168.200.200</td>
<td align="center">WebServer</td>
</tr>
<tr>
<td align="center">Collector1</td>
<td align="center">node02</td>
<td align="center">192.168.200.210</td>
<td align="center">AgentMstr1</td>
</tr>
<tr>
<td align="center">Collector2</td>
<td align="center">node03</td>
<td align="center">192.168.200.220</td>
<td align="center">AgentMstr2</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Agent1数据分别流入到Collector1和Collector2，Flume NG本身提供了Failover机制，可以自动切换和恢复。</span><br></pre></td></tr></table></figure>

<ul>
<li><p>2、开发配置文件</p>
<ul>
<li><p>node01、node02、node03分别都要安装flume</p>
</li>
<li><p>创建node01上的flume配置文件</p>
<ul>
<li>vim flume-client-failover.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">#agent name</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sources = r1</span><br><span class="line">#定义了2个sink</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set gruop</span><br><span class="line">#设置一个sink组，一个sink组下可以包含很多个sink</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line"></span><br><span class="line">#set sink group</span><br><span class="line">#指定g1这个sink组下有k1  k2 这2个sink</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set source</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /install/flumeData/tail.log</span><br><span class="line"></span><br><span class="line">#set channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># set sink1    指定sink1的数据会传输给node02</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = node02</span><br><span class="line">a1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"># set sink2    指定sink2的数据会传输给node03</span><br><span class="line">a1.sinks.k2.channel = c1</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = node03</span><br><span class="line">a1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line">#set failover</span><br><span class="line">#指定sink组高可用的策略---failover故障转移</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line">#指定k1这个sink的优先级</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 10</span><br><span class="line">#指定k2这个sink的优先级</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 5</span><br><span class="line">#指定故障转移的最大时间，如果超时会出现异常</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">说明：</span><br><span class="line">#这里首先要申明一个sinkgroups,然后再设置2个sink ,k1与k2,其中2个优先级是10和5。</span><br><span class="line">#而processor的maxpenalty被设置为10秒，默认是30秒.表示故障转移的最大时间</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建node02和node03上的flume配置文件</p>
<ul>
<li>node02和node03上配置信息相同</li>
<li>vim flume-server-failover.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#set Agent name</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">#set channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># set source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 52020</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置拦截器</span><br><span class="line">#指定2个拦截器  i1  i2 </span><br><span class="line">a1.sources.r1.interceptors = i1 i2</span><br><span class="line">#i1的类型为时间戳拦截器  可以解析%Y-%m-%d 时间</span><br><span class="line">a1.sources.r1.interceptors.i1.type = timestamp</span><br><span class="line">#i2的类型为主机拦截器，可以获取当前event中携带的主机名</span><br><span class="line">a1.sources.r1.interceptors.i2.type = host</span><br><span class="line">#指定主机名变量</span><br><span class="line">a1.sources.r1.interceptors.i2.hostHeader=hostname</span><br><span class="line"></span><br><span class="line">#set sink to hdfs</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://node01:8020/failover/logs/%&#123;hostname&#125;</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 60</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 50</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 10</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>3、启动flume配置</p>
<ul>
<li>先分别在node02和node03上启动flume<ul>
<li>分别进入到flume的安装目录下执行命令</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-failover.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后在node01上启动flume</p>
<ul>
<li>进入到flume的安装目录下执行命令</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-failover.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<ul>
<li>最后在hdfs目录上观察数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs://node01:8020/failover/logs</span><br></pre></td></tr></table></figure>

<h4 id="6-2-load-balance负载均衡"><a href="#6-2-load-balance负载均衡" class="headerlink" title="6.2 load balance负载均衡"></a>6.2 load balance负载均衡</h4><ul>
<li><p>实现多个flume采集数据的时候避免单个flume的负载比较高，实现多个flume采集器负载均衡。</p>
</li>
<li><p>1、节点分配</p>
<ul>
<li>与failover故障转移的节点分配</li>
</ul>
</li>
<li><p>2、开发配置文件</p>
<ul>
<li><p><strong>创建node01上的flume配置文件</strong></p>
<ul>
<li>vim  flume-client-loadbalance.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">#agent name</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set gruop</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line"></span><br><span class="line">#set sink group</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set source</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /install/flumeData/tail.log</span><br><span class="line"></span><br><span class="line">#set channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># set sink1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = node02</span><br><span class="line">a1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"># set sink2</span><br><span class="line">a1.sinks.k2.channel = c1</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = node03</span><br><span class="line">a1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line">#set load-balance</span><br><span class="line">#指定sink组高可用的策略---load_balance负载均衡</span><br><span class="line">a1.sinkgroups.g1.processor.type =load_balance</span><br><span class="line"># 默认是round_robin，还可以选择random</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">#如果backoff被开启，则sink processor会屏蔽故障的sink</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>创建node02和node03上的flume配置文件</strong></p>
<ul>
<li>vim  flume-server-loadbalance.conf</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#set Agent name</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">#set channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># set source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 52020</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置拦截器</span><br><span class="line">a1.sources.r1.interceptors = i1 i2</span><br><span class="line">a1.sources.r1.interceptors.i1.type = timestamp</span><br><span class="line">a1.sources.r1.interceptors.i2.type = host</span><br><span class="line">a1.sources.r1.interceptors.i2.hostHeader=hostname</span><br><span class="line">#hostname不使用ip显示，直接就是该服务器对应的主机名</span><br><span class="line">a1.sources.r1.interceptors.i2.useIP=false</span><br><span class="line"></span><br><span class="line">#set sink to hdfs</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://node01:8020/loadbalance/logs/%&#123;hostname&#125;</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 60</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 50</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 10</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br></pre></td></tr></table></figure>

<ul>
<li><p>3、启动flume配置</p>
<ul>
<li>先分别在node02和node03上启动flume<ul>
<li>分别进入到flume的安装目录下执行命令</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-loadbalance.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后在node01上启动flume</p>
<ul>
<li>分别进入到flume的安装目录下执行命令</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-loadbalance.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<ul>
<li>最后在hdfs上目录观察数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs://node01:8020/loadbalance/logs</span><br></pre></td></tr></table></figure>

<h3 id="7-flume企业案例"><a href="#7-flume企业案例" class="headerlink" title="7. flume企业案例"></a>7. flume企业案例</h3><h4 id="7-1-flume案例之静态拦截器使用"><a href="#7-1-flume案例之静态拦截器使用" class="headerlink" title="7.1 flume案例之静态拦截器使用"></a>7.1 flume案例之静态拦截器使用</h4><ul>
<li>1、案例场景</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A、B两台日志服务机器实时生产日志主要类型为access.log、nginx.log、web.log </span><br><span class="line">现在需要把A、B 机器中的access.log、nginx.log、web.log 采集汇总到C机器上然后统一收集到hdfs中。</span><br><span class="line">但是在hdfs中要求的目录为：</span><br><span class="line">/source/logs/access/20180101/**</span><br><span class="line">/source/logs/nginx/20180101/**</span><br><span class="line">/source/logs/web/20180101/**</span><br></pre></td></tr></table></figure>

<ul>
<li>2、场景分析</li>
</ul>
<p><img src="/2020/03/06/Flume日志采集框架/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AC%AC%E5%9B%9B%E6%9C%9F/9_%E8%BE%85%E5%8A%A9%E6%A1%86%E6%9E%B6/1227-%E8%AF%BE%E5%89%8D%E8%B5%84%E6%96%99%E6%9B%B4%E6%96%B0-flume/1227-%E8%AF%BE%E5%89%8D%E8%B5%84%E6%96%99-flume/flume/Flume%E8%AF%BE%E7%A8%8B/Flume%E8%AF%BE%E7%A8%8B/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6.assets/flume%E9%87%87%E9%9B%86%E4%B8%8D%E5%90%8C%E7%9A%84%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE.png" alt="flume采集不同的日志数据"></p>
<ul>
<li>3、数据流程处理分析</li>
</ul>
<p><img src="/2020/03/06/Flume日志采集框架/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AC%AC%E5%9B%9B%E6%9C%9F/9_%E8%BE%85%E5%8A%A9%E6%A1%86%E6%9E%B6/1227-%E8%AF%BE%E5%89%8D%E8%B5%84%E6%96%99%E6%9B%B4%E6%96%B0-flume/1227-%E8%AF%BE%E5%89%8D%E8%B5%84%E6%96%99-flume/flume/Flume%E8%AF%BE%E7%A8%8B/Flume%E8%AF%BE%E7%A8%8B/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6.assets/flume%E9%87%87%E9%9B%86%E4%B8%8D%E5%90%8C%E7%9A%84%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90.png" alt></p>
<ul>
<li><p>4、开发配置文件</p>
<ul>
<li><p>==在node01与node02服务器开发flume的配置文件==</p>
<ul>
<li>vim exec_source_avro_sink.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">#定义三个source</span><br><span class="line">a1.sources = r1 r2 r3</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /home/hadoop/taillogs/access.log</span><br><span class="line">#指定source r1 使用拦截器i1</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">#拦截器类型static静态</span><br><span class="line">a1.sources.r1.interceptors.i1.type = static</span><br><span class="line">## static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对</span><br><span class="line"># 自己进行设置,我们这里的key和value相当于键值对,k=type v=access</span><br><span class="line">a1.sources.r1.interceptors.i1.key = type</span><br><span class="line">a1.sources.r1.interceptors.i1.value = access</span><br><span class="line"></span><br><span class="line">a1.sources.r2.type = exec</span><br><span class="line">a1.sources.r2.command = tail -F /home/hadoop/taillogs/nginx.log</span><br><span class="line">#指定source r2 使用拦截器i2</span><br><span class="line">a1.sources.r2.interceptors = i2</span><br><span class="line">#拦截器类型static静态</span><br><span class="line">a1.sources.r2.interceptors.i2.type = static</span><br><span class="line"># 自己进行设置</span><br><span class="line">a1.sources.r2.interceptors.i2.key = type</span><br><span class="line">a1.sources.r2.interceptors.i2.value = nginx</span><br><span class="line"></span><br><span class="line">a1.sources.r3.type = exec</span><br><span class="line">a1.sources.r3.command = tail -F /home/hadoop/taillogs/web.log</span><br><span class="line">#指定source r3 使用拦截器i3</span><br><span class="line">a1.sources.r3.interceptors = i3</span><br><span class="line">#拦截器类型static静态</span><br><span class="line">a1.sources.r3.interceptors.i3.type = static</span><br><span class="line"># 自己进行设置</span><br><span class="line">a1.sources.r3.interceptors.i3.key = type</span><br><span class="line">a1.sources.r3.interceptors.i3.value = web</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 20000</span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = node03</span><br><span class="line">a1.sinks.k1.port = 41414</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r2.channels = c1</span><br><span class="line">a1.sources.r3.channels = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>==在node03服务器上开发flume配置文件==</p>
<ul>
<li>vim avro_source_hdfs_sink.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">#定义source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = node03</span><br><span class="line">a1.sources.r1.port =41414</span><br><span class="line"></span><br><span class="line">#定义channels</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 20000</span><br><span class="line">a1.channels.c1.transactionCapacity = 1000</span><br><span class="line"></span><br><span class="line">#定义sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line"># 此处的%&#123;type&#125; 这里是取我们在node01和node02定义的type的值,也就是value</span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://node01:9000/source/logs/%&#123;type&#125;/%Y%m%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix =events-</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat = Text</span><br><span class="line">#时间类型</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#生成的文件不按条数生成</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">#生成的文件按时间生成</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 30</span><br><span class="line">#生成的文件按大小生成</span><br><span class="line">a1.sinks.k1.hdfs.rollSize  = 10485760</span><br><span class="line">#批量写入hdfs的个数</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 1000</span><br><span class="line">#flume操作hdfs的线程数（包括新建，写入等）</span><br><span class="line">a1.sinks.k1.hdfs.threadsPoolSize=10</span><br><span class="line">#操作hdfs超时时间</span><br><span class="line">a1.sinks.k1.hdfs.callTimeout=30000</span><br><span class="line"></span><br><span class="line">#组装source、channel、sink</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>5、启动flume配置</p>
<ul>
<li>先在node03上启动flume</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/avro_source_hdfs_sink.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<ul>
<li>然后分别在node01和node02上启动flume</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/exec_source_avro_sink.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<ul>
<li><p>在node01和node02上准备数据文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/home/hadoop/taillogs/access.log</span><br><span class="line">/home/hadoop/taillogs/nginx.log</span><br><span class="line">/home/hadoop/taillogs/web.log</span><br><span class="line"></span><br><span class="line">创建以上文件，内容是什么不重要</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后在hdfs上对应的目录观察</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs://node01:8020/source/logs</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="7-2-flume案例之自定义拦截器"><a href="#7-2-flume案例之自定义拦截器" class="headerlink" title="7.2 flume案例之自定义拦截器"></a>7.2 flume案例之自定义拦截器</h4><ul>
<li>1、案例场景</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在数据采集之后，通过flume的拦截器，实现不需要的数据过滤掉，并将指定的第一个字段进行加密，加密之后再往hdfs上面保存</span><br></pre></td></tr></table></figure>

<ul>
<li>2、数据文件 user.txt</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">13901007610,male,30,sing,beijing</span><br><span class="line">18600000035,male,40,dance,shanghai</span><br><span class="line">13366666659,male,20,Swimming,wuhan</span><br><span class="line">13801179888,female,18,dance,tianjin</span><br><span class="line">18511111114,male,35,sing,beijing</span><br><span class="line">13718428888,female,40,Foodie,shanghai</span><br><span class="line">13901057088,male,50,Basketball,taiwan</span><br><span class="line">13671057777,male,60,Bodybuilding,xianggang</span><br></pre></td></tr></table></figure>

<p><img src="/2020/03/06/Flume日志采集框架/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AC%AC%E5%9B%9B%E6%9C%9F/9_%E8%BE%85%E5%8A%A9%E6%A1%86%E6%9E%B6/1227-%E8%AF%BE%E5%89%8D%E8%B5%84%E6%96%99%E6%9B%B4%E6%96%B0-flume/1227-%E8%AF%BE%E5%89%8D%E8%B5%84%E6%96%99-flume/flume/Flume%E8%AF%BE%E7%A8%8B/Flume%E8%AF%BE%E7%A8%8B/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6.assets/flume-custom-interceptor.png" alt></p>
<ul>
<li>3、创建maven工程添加依赖</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">                    <span class="comment">&lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span><span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>4、代码开发</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.common.base.Charsets;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.math.BigInteger;</span><br><span class="line"><span class="keyword">import</span> java.security.MessageDigest;</span><br><span class="line"><span class="keyword">import</span> java.security.NoSuchAlgorithmException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line">    <span class="comment">/** encrypted_field_index. 指定需要加密的字段下标 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String encrypted_field_index;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The out_index. 指定不需要对应列的下标*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String out_index;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 提供构建方法，后期可以接受配置文件中的参数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> encrypted_field_index</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out_index</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MyInterceptor</span><span class="params">( String encrypted_field_index, String out_index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.encrypted_field_index=encrypted_field_index.trim();</span><br><span class="line">        <span class="keyword">this</span>.out_index=out_index.trim();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 单个event拦截逻辑</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (event == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            String line = <span class="keyword">new</span> String(event.getBody(), Charsets.UTF_8);</span><br><span class="line">            String[] fields = line.split(<span class="string">","</span>);</span><br><span class="line"></span><br><span class="line">            String newLine = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">                <span class="comment">//字符串数字转换成int</span></span><br><span class="line">                <span class="keyword">int</span> encryptedField = Integer.parseInt(encrypted_field_index);</span><br><span class="line">                <span class="keyword">int</span> outIndex = Integer.parseInt(out_index);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (i == encryptedField) &#123;</span><br><span class="line">                     newLine+=md5(fields[i])+<span class="string">","</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span>(i !=outIndex) &#123;</span><br><span class="line">                    newLine+=fields[i]+<span class="string">","</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            newLine=newLine.substring(<span class="number">0</span>,newLine.length()-<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">              event.setBody(newLine.getBytes(Charsets.UTF_8));</span><br><span class="line">            <span class="keyword">return</span> event;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">return</span> event;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 批量event拦截逻辑</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line">        List&lt;Event&gt; out = <span class="keyword">new</span> ArrayList&lt;Event&gt;();</span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            Event outEvent = intercept(event);</span><br><span class="line">            <span class="keyword">if</span> (outEvent != <span class="keyword">null</span>) &#123;</span><br><span class="line">                out.add(outEvent);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> out;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//写一个md5加密的方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">md5</span><span class="params">(String plainText)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//定义一个字节数组</span></span><br><span class="line">        <span class="keyword">byte</span>[] secretBytes = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 生成一个MD5加密计算摘要</span></span><br><span class="line">            MessageDigest md = MessageDigest.getInstance(<span class="string">"MD5"</span>);</span><br><span class="line">            <span class="comment">//对字符串进行加密</span></span><br><span class="line">            md.update(plainText.getBytes());</span><br><span class="line">            <span class="comment">//获得加密后的数据</span></span><br><span class="line">            secretBytes = md.digest();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (NoSuchAlgorithmException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"没有md5这个算法！"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//将加密后的数据转换为16进制数字</span></span><br><span class="line">        String md5code = <span class="keyword">new</span> BigInteger(<span class="number">1</span>, secretBytes).toString(<span class="number">16</span>);<span class="comment">// 16进制数字</span></span><br><span class="line">        <span class="comment">// 如果生成数字未满32位，需要前面补0</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span> - md5code.length(); i++) &#123;</span><br><span class="line">            md5code = <span class="string">"0"</span> + md5code;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> md5code;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 相当于自定义Interceptor的工厂类</span></span><br><span class="line"><span class="comment">     * 在flume采集配置文件中通过制定该Builder来创建Interceptor对象</span></span><br><span class="line"><span class="comment">     * 可以在Builder中获取、解析flume采集配置文件中的拦截器Interceptor的自定义参数：</span></span><br><span class="line"><span class="comment">     * 指定需要加密的字段下标 指定不需要对应列的下标等</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@author</span></span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBuilder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * encrypted_field_index. 指定需要加密的字段下标</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">private</span>  String encrypted_field_index;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * The out_index. 指定不需要对应列的下标</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">private</span>  String out_index;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.encrypted_field_index = context.getString(<span class="string">"encrypted_field_index"</span>, <span class="string">""</span>);</span><br><span class="line">            <span class="keyword">this</span>.out_index = context.getString(<span class="string">"out_index"</span>, <span class="string">""</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * @see org.apache.flume.interceptor.Interceptor.Builder#build()</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> MyInterceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> MyInterceptor(encrypted_field_index, out_index);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>5、打成jar包后放到flume安装目录下的lib中</p>
</li>
<li><p>6、创建配置文件 flume-interceptor-hdfs.conf</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /install/flumeData/user.txt</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.interceptors =i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type =com.kaikeba.interceptor.MyInterceptor$MyBuilder</span><br><span class="line">a1.sources.r1.interceptors.i1.encrypted_field_index=0</span><br><span class="line">a1.sources.r1.interceptors.i1.out_index=3</span><br><span class="line"></span><br><span class="line">#配置channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://node01:8020/interceptor/files/%Y-%m-%d/%H%M</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 5</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 50</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 10</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br></pre></td></tr></table></figure>

<ul>
<li>7、进入到flume安装目录下启动flume</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-interceptor-hdfs.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<h3 id="8-flume自定义Source"><a href="#8-flume自定义Source" class="headerlink" title="8. flume自定义Source"></a>8. flume自定义Source</h3><h4 id="8-1-场景描述"><a href="#8-1-场景描述" class="headerlink" title="8.1 场景描述"></a>8.1 场景描述</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">	官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。如：实时监控MySQL，从MySQL中获取数据传输到HDFS或者其他存储框架，所以此时需要我们自己实现MySQLSource。</span><br><span class="line"></span><br><span class="line">官方也提供了自定义source的接口：</span><br><span class="line">官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#source</span><br></pre></td></tr></table></figure>

<h4 id="8-2-自定义MysqlSource步骤"><a href="#8-2-自定义MysqlSource步骤" class="headerlink" title="8.2 自定义MysqlSource步骤"></a>8.2 自定义MysqlSource步骤</h4><ul>
<li><p>1、根据官方说明自定义mysqlsource需要继承AbstractSource类并实现Configurable和PollableSource接口。</p>
</li>
<li><p>2、实现对应的方法</p>
<ul>
<li>configure(Context context)<ul>
<li>初始化context</li>
</ul>
</li>
<li>process()<ul>
<li>从mysql表中获取数据，然后把数据封装成event对象写入到channel，该方法被一直调用</li>
</ul>
</li>
<li>stop()<ul>
<li>关闭相关资源</li>
</ul>
</li>
</ul>
</li>
<li><p>3、开发流程</p>
<ul>
<li>3.1 创建mysql数据库以及mysql数据库表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建一个数据库</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> mysqlsource <span class="keyword">DEFAULT</span> <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 ;</span><br><span class="line"></span><br><span class="line"><span class="comment">--创建一个表，用户保存拉取目标表位置的信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mysqlsource.flume_meta (</span><br><span class="line">  source_tab <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  currentIndex <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (source_tab)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br><span class="line"></span><br><span class="line"><span class="comment">--插入数据</span></span><br><span class="line"><span class="keyword">insert</span>  <span class="keyword">into</span> mysqlsource.flume_meta(source_tab,currentIndex) <span class="keyword">values</span> (<span class="string">'student'</span>,<span class="string">'4'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--创建要拉取数据的表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mysqlsource.student(</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">5</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br><span class="line"></span><br><span class="line"><span class="comment">--向student表中添加测试数据</span></span><br><span class="line"><span class="keyword">insert</span>  <span class="keyword">into</span> mysqlsource.student(<span class="keyword">id</span>,<span class="keyword">name</span>) <span class="keyword">values</span> (<span class="number">1</span>,<span class="string">'zhangsan'</span>),(<span class="number">2</span>,<span class="string">'lisi'</span>),(<span class="number">3</span>,<span class="string">'wangwu'</span>),(<span class="number">4</span>,<span class="string">'zhaoliu'</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li><p>3.2 代码开发实现</p>
<ul>
<li>构建maven工程，添加依赖</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-lang3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>在resources资源文件夹下添加jdbc.properties</p>
<ul>
<li>jdbc.properties</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dbDriver=com.mysql.jdbc.Driver</span><br><span class="line">dbUrl=jdbc:mysql://node03:3306/mysqlsource?useUnicode=true&amp;characterEncoding=utf-8</span><br><span class="line">dbUser=root</span><br><span class="line">dbPassword=123456</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义QueryMysql工具类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.ConfigurationException;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryMysql</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(QueryMysql.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> runQueryDelay,   <span class="comment">//两次查询的时间间隔</span></span><br><span class="line">            startFrom,            <span class="comment">//开始id</span></span><br><span class="line">            currentIndex,	      <span class="comment">//当前id</span></span><br><span class="line">            recordSixe = <span class="number">0</span>,        <span class="comment">//每次查询返回结果的条数</span></span><br><span class="line">            maxRow;                <span class="comment">//每次查询的最大条数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> String table,          <span class="comment">//要操作的表</span></span><br><span class="line">            columnsToSelect,       <span class="comment">//用户传入的查询的列</span></span><br><span class="line">            customQuery,          <span class="comment">//用户传入的查询语句</span></span><br><span class="line">            query,                 <span class="comment">//构建的查询语句</span></span><br><span class="line">            defaultCharsetResultSet;<span class="comment">//编码集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//上下文，用来获取配置文件</span></span><br><span class="line">    <span class="keyword">private</span> Context context;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//为定义的变量赋值（默认值），可在flume任务的配置文件中修改</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_QUERY_DELAY = <span class="number">10000</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_START_VALUE = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_MAX_ROWS = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_COLUMNS_SELECT = <span class="string">"*"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_CHARSET_RESULTSET = <span class="string">"UTF-8"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection conn = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> PreparedStatement ps = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String connectionURL, connectionUserName, connectionPassword;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//加载静态资源</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        Properties p = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            p.load(QueryMysql.class.getClassLoader().getResourceAsStream(<span class="string">"jdbc.properties"</span>));</span><br><span class="line">            connectionURL = p.getProperty(<span class="string">"dbUrl"</span>);</span><br><span class="line">            connectionUserName = p.getProperty(<span class="string">"dbUser"</span>);</span><br><span class="line">            connectionPassword = p.getProperty(<span class="string">"dbPassword"</span>);</span><br><span class="line">            Class.forName(p.getProperty(<span class="string">"dbDriver"</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.error(e.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取JDBC连接</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">InitConnection</span><span class="params">(String url, String user, String pw)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Connection conn = DriverManager.getConnection(url, user, pw);</span><br><span class="line">            <span class="keyword">if</span> (conn == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> SQLException();</span><br><span class="line">            <span class="keyword">return</span> conn;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    QueryMysql(Context context) <span class="keyword">throws</span> ParseException &#123;</span><br><span class="line">        <span class="comment">//初始化上下文</span></span><br><span class="line">        <span class="keyword">this</span>.context = context;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//有默认值参数：获取flume任务配置文件中的参数，读不到的采用默认值</span></span><br><span class="line">        <span class="keyword">this</span>.columnsToSelect = context.getString(<span class="string">"columns.to.select"</span>, DEFAULT_COLUMNS_SELECT);</span><br><span class="line">        <span class="keyword">this</span>.runQueryDelay = context.getInteger(<span class="string">"run.query.delay"</span>, DEFAULT_QUERY_DELAY);</span><br><span class="line">        <span class="keyword">this</span>.startFrom = context.getInteger(<span class="string">"start.from"</span>, DEFAULT_START_VALUE);</span><br><span class="line">        <span class="keyword">this</span>.defaultCharsetResultSet = context.getString(<span class="string">"default.charset.resultset"</span>, DEFAULT_CHARSET_RESULTSET);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//无默认值参数：获取flume任务配置文件中的参数</span></span><br><span class="line">        <span class="keyword">this</span>.table = context.getString(<span class="string">"table"</span>);</span><br><span class="line">        <span class="keyword">this</span>.customQuery = context.getString(<span class="string">"custom.query"</span>);</span><br><span class="line"></span><br><span class="line">        connectionURL = context.getString(<span class="string">"connection.url"</span>);</span><br><span class="line">        connectionUserName = context.getString(<span class="string">"connection.user"</span>);</span><br><span class="line">        connectionPassword = context.getString(<span class="string">"connection.password"</span>);</span><br><span class="line">        conn = InitConnection(connectionURL, connectionUserName, connectionPassword);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//校验相应的配置信息，如果没有默认值的参数也没赋值，抛出异常</span></span><br><span class="line">        checkMandatoryProperties();</span><br><span class="line">        <span class="comment">//获取当前的id</span></span><br><span class="line">        currentIndex = getStatusDBIndex(startFrom);</span><br><span class="line">        <span class="comment">//构建查询语句</span></span><br><span class="line">        query = buildQuery();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//校验相应的配置信息（表，查询语句以及数据库连接的参数）</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkMandatoryProperties</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (table == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConfigurationException(<span class="string">"property table not set"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (connectionURL == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConfigurationException(<span class="string">"connection.url property not set"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (connectionUserName == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConfigurationException(<span class="string">"connection.user property not set"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (connectionPassword == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConfigurationException(<span class="string">"connection.password property not set"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构建sql语句</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">buildQuery</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String sql = <span class="string">""</span>;</span><br><span class="line">        <span class="comment">//获取当前id</span></span><br><span class="line">        currentIndex = getStatusDBIndex(startFrom);</span><br><span class="line">        LOG.info(currentIndex + <span class="string">""</span>);</span><br><span class="line">        <span class="keyword">if</span> (customQuery == <span class="keyword">null</span>) &#123;</span><br><span class="line">            sql = <span class="string">"SELECT "</span> + columnsToSelect + <span class="string">" FROM "</span> + table;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            sql = customQuery;</span><br><span class="line">        &#125;</span><br><span class="line">        StringBuilder execSql = <span class="keyword">new</span> StringBuilder(sql);</span><br><span class="line">        <span class="comment">//以id作为offset</span></span><br><span class="line">        <span class="keyword">if</span> (!sql.contains(<span class="string">"where"</span>)) &#123;</span><br><span class="line">            execSql.append(<span class="string">" where "</span>);</span><br><span class="line">            execSql.append(<span class="string">"id"</span>).append(<span class="string">"&gt;"</span>).append(currentIndex);</span><br><span class="line">            <span class="keyword">return</span> execSql.toString();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> length = execSql.toString().length();</span><br><span class="line">            <span class="keyword">return</span> execSql.toString().substring(<span class="number">0</span>, length - String.valueOf(currentIndex).length()) + currentIndex;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行查询</span></span><br><span class="line">    List&lt;List&lt;Object&gt;&gt; executeQuery() &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//每次执行查询时都要重新生成sql，因为id不同</span></span><br><span class="line">            customQuery = buildQuery();</span><br><span class="line">            <span class="comment">//存放结果的集合</span></span><br><span class="line">            List&lt;List&lt;Object&gt;&gt; results = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">if</span> (ps == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//初始化PrepareStatement对象</span></span><br><span class="line">                ps = conn.prepareStatement(customQuery);</span><br><span class="line">            &#125;</span><br><span class="line">            ResultSet result = ps.executeQuery(customQuery);</span><br><span class="line">            <span class="keyword">while</span> (result.next()) &#123;</span><br><span class="line">                <span class="comment">//存放一条数据的集合（多个列）</span></span><br><span class="line">                List&lt;Object&gt; row = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                <span class="comment">//将返回结果放入集合</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= result.getMetaData().getColumnCount(); i++) &#123;</span><br><span class="line">                    row.add(result.getObject(i));</span><br><span class="line">                &#125;</span><br><span class="line">                results.add(row);</span><br><span class="line">            &#125;</span><br><span class="line">            LOG.info(<span class="string">"execSql:"</span> + customQuery + <span class="string">"\nresultSize:"</span> + results.size());</span><br><span class="line">            <span class="keyword">return</span> results;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            LOG.error(e.toString());</span><br><span class="line">            <span class="comment">// 重新连接</span></span><br><span class="line">            conn = InitConnection(connectionURL, connectionUserName, connectionPassword);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将结果集转化为字符串，每一条数据是一个list集合，将每一个小的list集合转化为字符串</span></span><br><span class="line">    <span class="function">List&lt;String&gt; <span class="title">getAllRows</span><span class="params">(List&lt;List&lt;Object&gt;&gt; queryResult)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; allRows = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span> (queryResult == <span class="keyword">null</span> || queryResult.isEmpty())</span><br><span class="line">            <span class="keyword">return</span> allRows;</span><br><span class="line">        StringBuilder row = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">for</span> (List&lt;Object&gt; rawRow : queryResult) &#123;</span><br><span class="line">            Object value = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">for</span> (Object aRawRow : rawRow) &#123;</span><br><span class="line">                value = aRawRow;</span><br><span class="line">                <span class="keyword">if</span> (value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    row.append(<span class="string">","</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    row.append(aRawRow.toString()).append(<span class="string">","</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            allRows.add(row.toString());</span><br><span class="line">            row = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> allRows;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//更新offset元数据状态，每次返回结果集后调用。必须记录每次查询的offset值，为程序中断续跑数据时使用，以id为offset</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">updateOffset2DB</span><span class="params">(<span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//以source_tab做为KEY，如果不存在则插入，存在则更新（每个源表对应一条记录）</span></span><br><span class="line">        String sql = <span class="string">"insert into flume_meta(source_tab,currentIndex) VALUES('"</span></span><br><span class="line">                + <span class="keyword">this</span>.table</span><br><span class="line">                + <span class="string">"','"</span> + (recordSixe += size)</span><br><span class="line">                + <span class="string">"') on DUPLICATE key update source_tab=values(source_tab),currentIndex=values(currentIndex)"</span>;</span><br><span class="line">        LOG.info(<span class="string">"updateStatus Sql:"</span> + sql);</span><br><span class="line">        execSql(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行sql语句</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">execSql</span><span class="params">(String sql)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ps = conn.prepareStatement(sql);</span><br><span class="line">            LOG.info(<span class="string">"exec::"</span> + sql);</span><br><span class="line">            ps.execute();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取当前id的offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Integer <span class="title">getStatusDBIndex</span><span class="params">(<span class="keyword">int</span> startFrom)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//从flume_meta表中查询出当前的id是多少</span></span><br><span class="line">        String dbIndex = queryOne(<span class="string">"select currentIndex from flume_meta where source_tab='"</span> + table + <span class="string">"'"</span>);</span><br><span class="line">        <span class="keyword">if</span> (dbIndex != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> Integer.parseInt(dbIndex);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//如果没有数据，则说明是第一次查询或者数据表中还没有存入数据，返回最初传入的值</span></span><br><span class="line">        <span class="keyword">return</span> startFrom;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//查询一条数据的执行语句(当前id)</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">queryOne</span><span class="params">(String sql)</span> </span>&#123;</span><br><span class="line">        ResultSet result = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ps = conn.prepareStatement(sql);</span><br><span class="line">            result = ps.executeQuery();</span><br><span class="line">            <span class="keyword">while</span> (result.next()) &#123;</span><br><span class="line">                <span class="keyword">return</span> result.getString(<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭相关资源</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">            conn.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getCurrentIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> currentIndex;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setCurrentIndex</span><span class="params">(<span class="keyword">int</span> newValue)</span> </span>&#123;</span><br><span class="line">        currentIndex = newValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getRunQueryDelay</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> runQueryDelay;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">getQuery</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> query;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">getConnectionURL</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> connectionURL;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isCustomQuerySet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (customQuery != <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">Context <span class="title">getContext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> context;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getConnectionUserName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> connectionUserName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getConnectionPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> connectionPassword;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">getDefaultCharsetResultSet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> defaultCharsetResultSet;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义MySqlSource类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.source;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySqlSource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打印日志</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(MySqlSource.class);</span><br><span class="line">    <span class="comment">//定义sqlHelper</span></span><br><span class="line">    <span class="keyword">private</span> QueryMysql sqlSourceHelper;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getBackOffSleepIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxBackOffSleepInterval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            sqlSourceHelper = <span class="keyword">new</span> QueryMysql(context);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ParseException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 接受mysql表中的数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> EventDeliveryException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> PollableSource.<span class="function">Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//查询数据表</span></span><br><span class="line">            List&lt;List&lt;Object&gt;&gt; result = sqlSourceHelper.executeQuery();</span><br><span class="line">            <span class="comment">//存放event的集合</span></span><br><span class="line">            List&lt;Event&gt; events = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="comment">//存放event头集合</span></span><br><span class="line">            HashMap&lt;String, String&gt; header = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="comment">//如果有返回数据，则将数据封装为event</span></span><br><span class="line">            <span class="keyword">if</span> (!result.isEmpty()) &#123;</span><br><span class="line">                List&lt;String&gt; allRows = sqlSourceHelper.getAllRows(result);</span><br><span class="line">                Event event = <span class="keyword">null</span>;</span><br><span class="line">                <span class="keyword">for</span> (String row : allRows) &#123;</span><br><span class="line">                    event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line">                    event.setBody(row.getBytes());</span><br><span class="line">                    event.setHeaders(header);</span><br><span class="line">                    events.add(event);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//将event写入channel</span></span><br><span class="line">                <span class="keyword">this</span>.getChannelProcessor().processEventBatch(events);</span><br><span class="line">                <span class="comment">//更新数据表中的offset信息</span></span><br><span class="line">                sqlSourceHelper.updateOffset2DB(result.size());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//等待时长</span></span><br><span class="line">            Thread.sleep(sqlSourceHelper.getRunQueryDelay());</span><br><span class="line">            <span class="keyword">return</span> Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            LOG.error(<span class="string">"Error procesing row"</span>, e);</span><br><span class="line">            <span class="keyword">return</span> Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        LOG.info(<span class="string">"Stopping sql source &#123;&#125; ..."</span>, getName());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//关闭资源</span></span><br><span class="line">            sqlSourceHelper.close();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">super</span>.stop();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>4、测试</p>
<ul>
<li><p>4.1 程序打成jar包，上传jar包到flume的lib目录下</p>
</li>
<li><p>4.2 配置文件准备</p>
<ul>
<li>vim mysqlsource.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = com.kaikeba.source.MySqlSource</span><br><span class="line"># 老师的是node01,同学们改成自己的节点 一定要注意</span><br><span class="line">a1.sources.r1.connection.url = jdbc:mysql://node01:3306/mysqlsource</span><br><span class="line">a1.sources.r1.connection.user = root</span><br><span class="line">a1.sources.r1.connection.password = 123456</span><br><span class="line">a1.sources.r1.table = student</span><br><span class="line">a1.sources.r1.columns.to.select = *</span><br><span class="line">a1.sources.r1.start.from=0</span><br><span class="line">a1.sources.r1.run.query.delay=3000</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>4.3 启动flume配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/mysqlsource.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>4.4 最后向表添加数据，观察控制台信息</li>
</ul>
<h3 id="9-flume自定义Sink"><a href="#9-flume自定义Sink" class="headerlink" title="9. flume自定义Sink"></a>9. flume自定义Sink</h3><h4 id="9-1-场景描述"><a href="#9-1-场景描述" class="headerlink" title="9.1 场景描述"></a>9.1 场景描述</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">	官方提供的sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些sink。如：需要把接受到的数据按照规则进行过滤之后写入到某张mysql表中，所以此时需要我们自己实现MySQLSink。</span><br><span class="line"></span><br><span class="line">官方也提供了自定义sink的接口：</span><br><span class="line">官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#sink</span><br></pre></td></tr></table></figure>

<h4 id="9-2-自定义MysqlSink步骤"><a href="#9-2-自定义MysqlSink步骤" class="headerlink" title="9.2 自定义MysqlSink步骤"></a>9.2 自定义MysqlSink步骤</h4><ul>
<li><p>1、根据官方说明自定义MysqlSink需要继承AbstractSink类并实现Configurable</p>
</li>
<li><p>2、实现对应的方法</p>
<ul>
<li><p>configure(Context context)</p>
<ul>
<li>初始化context</li>
</ul>
</li>
<li><p>start()</p>
<ul>
<li>启动准备操作</li>
</ul>
</li>
<li><p>process()</p>
<ul>
<li>从channel获取数据，然后解析之后，保存在mysql表中</li>
</ul>
</li>
<li><p>stop()</p>
<ul>
<li>关闭相关资源</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>3、开发流程</p>
<ul>
<li>3.1 创建mysql数据库以及mysql数据库表</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--创建一个数据库</span><br><span class="line">CREATE DATABASE IF NOT EXISTS mysqlsource DEFAULT CHARACTER SET utf8 ;</span><br><span class="line"></span><br><span class="line">--创建一个表，用户保存拉取目标表位置的信息</span><br><span class="line">CREATE TABLE mysqlsource.flume2mysql (</span><br><span class="line">  id int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  createTime varchar(64) NOT NULL,</span><br><span class="line">  content varchar(255) NOT NULL,</span><br><span class="line">  PRIMARY KEY (id)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>3.2  代码开发实现</p>
<p>定义MysqlSink类</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.sink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义MysqlSink</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MysqlSink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String mysqlurl = <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">private</span> String username = <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">private</span> String password = <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">private</span> String tableName = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">    Connection con = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span></span>&#123;</span><br><span class="line">        Status status = <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// Start transaction</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line">        txn.begin();</span><br><span class="line">        <span class="keyword">try</span></span><br><span class="line">        &#123;</span><br><span class="line">            Event event = ch.take();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (event != <span class="keyword">null</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                    <span class="comment">//获取body中的数据</span></span><br><span class="line">                    String body = <span class="keyword">new</span> String(event.getBody(), <span class="string">"UTF-8"</span>);</span><br><span class="line"></span><br><span class="line">                    <span class="comment">//如果日志中有以下关键字的不需要保存，过滤掉</span></span><br><span class="line">                <span class="keyword">if</span>(body.contains(<span class="string">"delete"</span>) || body.contains(<span class="string">"drop"</span>) || body.contains(<span class="string">"alert"</span>))&#123;</span><br><span class="line">                    status = Status.BACKOFF;</span><br><span class="line">                &#125;<span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">//存入Mysql</span></span><br><span class="line">                    SimpleDateFormat df = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>);</span><br><span class="line">                    String createtime = df.format(<span class="keyword">new</span> Date());</span><br><span class="line"></span><br><span class="line">                    PreparedStatement stmt = con.prepareStatement(<span class="string">"insert into "</span> + tableName + <span class="string">" (createtime, content) values (?, ?)"</span>);</span><br><span class="line">                    stmt.setString(<span class="number">1</span>, createtime);</span><br><span class="line">                    stmt.setString(<span class="number">2</span>, body);</span><br><span class="line">                    stmt.execute();</span><br><span class="line">                    stmt.close();</span><br><span class="line">                    status = Status.READY;</span><br><span class="line">                &#125;</span><br><span class="line">           &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                    status = Status.BACKOFF;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            txn.commit();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t)&#123;</span><br><span class="line">            txn.rollback();</span><br><span class="line">            t.getCause().printStackTrace();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125; <span class="keyword">finally</span>&#123;</span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取配置文件中指定的参数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        mysqlurl = context.getString(<span class="string">"mysqlurl"</span>);</span><br><span class="line">        username = context.getString(<span class="string">"username"</span>);</span><br><span class="line">        password = context.getString(<span class="string">"password"</span>);</span><br><span class="line">        tableName = context.getString(<span class="string">"tablename"</span>);</span><br><span class="line">    &#125;    </span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">              <span class="comment">//初始化数据库连接</span></span><br><span class="line">            con = DriverManager.getConnection(mysqlurl, username, password);</span><br><span class="line">            <span class="keyword">super</span>.start();</span><br><span class="line">            System.out.println(<span class="string">"finish start"</span>);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception ex)&#123;</span><br><span class="line">            ex.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            con.close();</span><br><span class="line">        &#125;<span class="keyword">catch</span>(SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">super</span>.stop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>4、测试</p>
<ul>
<li><p>4.1 程序打成jar包，上传jar包到flume的lib目录下</p>
</li>
<li><p>4.2 配置文件准备</p>
<ul>
<li>vim mysqlsink.conf</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /kkb/install/flumeData/data.log</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.type = com.kaikeba.sink.MysqlSink</span><br><span class="line">a1.sinks.k1.mysqlurl=jdbc:mysql://node01:3306/mysqlsource?useSSL=false</span><br><span class="line">a1.sinks.k1.username=root</span><br><span class="line">a1.sinks.k1.password=123456</span><br><span class="line">a1.sinks.k1.tablename=flume2mysql</span><br></pre></td></tr></table></figure>
</li>
<li><p>4.3 启动flume配置</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/mysqlsink.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<ul>
<li>4.4 最后向文件中添加数据，观察mysql表中的数据</li>
</ul>
</li>
</ul>
<h3 id="10-Flume实际使用注意事项"><a href="#10-Flume实际使用注意事项" class="headerlink" title="10. Flume实际使用注意事项"></a>10. Flume实际使用注意事项</h3><ul>
<li>1、注意启动脚本命名的书写</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agent 的名称别写错了，后台执行加上 nohup ... &amp;</span><br></pre></td></tr></table></figure>

<ul>
<li>2、channel参数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">capacity：默认该通道中最大的可以存储的event数量</span><br><span class="line">trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量</span><br><span class="line">注意：capacity &gt; trasactionCapacity</span><br></pre></td></tr></table></figure>

<ul>
<li>3、日志采集到HDFS配置说明1（sink端）</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>定义sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%&#123;type&#125;/%Y%m%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix =events</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat = Text</span><br><span class="line"><span class="meta">#</span>时间类型</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span>生成的文件不按条数生成</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"><span class="meta">#</span>生成的文件按时间生成</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line"><span class="meta">#</span>生成的文件按大小生成</span><br><span class="line">a1.sinks.k1.hdfs.rollSize  = 10485760</span><br><span class="line"><span class="meta">#</span>批量写入hdfs的个数</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 10000</span><br><span class="line"><span class="meta">#</span>flume操作hdfs的线程数（包括新建，写入等）</span><br><span class="line">a1.sinks.k1.hdfs.threadsPoolSize=10</span><br><span class="line"><span class="meta">#</span>操作hdfs超时时间</span><br><span class="line">a1.sinks.k1.hdfs.callTimeout=30000</span><br></pre></td></tr></table></figure>

<ul>
<li>4、日志采集到HDFS配置说明2（sink端）</li>
</ul>
<table>
<thead>
<tr>
<th>hdfs.round</th>
<th>false</th>
<th>Should the timestamp be rounded down (if true, affects all time based escape sequences except %t)</th>
</tr>
</thead>
<tbody><tr>
<td>hdfs.roundValue</td>
<td>1</td>
<td>Rounded down to the highest multiple of this (in the unit configured usinghdfs.roundUnit), less than current time.</td>
</tr>
<tr>
<td>hdfs.roundUnit</td>
<td>second</td>
<td>The unit of the round down value - second, minute or hour.</td>
</tr>
</tbody></table>
<p>Ø round： 默认值：false 是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”</p>
<p>Ø roundValue：默认值：1  时间上进行“舍弃”的值；</p>
<p>Ø roundUnit： 默认值：seconds时间上进行”舍弃”的单位，包含：second,minute,hour</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">y案例一：</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为：</span><br><span class="line">/flume/events/2015-10-16/17:30/00</span><br><span class="line">/flume/events/2015-10-16/17:40/00</span><br><span class="line">/flume/events/2015-10-16/17:50/00</span><br><span class="line">因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。</span><br><span class="line"></span><br><span class="line">案例二：</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = second</span><br><span class="line">现象：10秒为时间梯度生成对应的目录，目录下面包括很多小文件！！！</span><br><span class="line">格式如下：</span><br><span class="line">/flume/events/2016-07-28/18:45/10</span><br><span class="line">/flume/events/2016-07-28/18:45/20</span><br><span class="line">/flume/events/2016-07-28/18:45/30</span><br><span class="line">/flume/events/2016-07-28/18:45/40</span><br><span class="line">/flume/events/2016-07-28/18:45/50</span><br><span class="line">/flume/events/2016-07-28/18:46/10</span><br><span class="line">/flume/events/2016-07-28/18:46/20</span><br><span class="line">/flume/events/2016-07-28/18:46/30</span><br><span class="line">/flume/events/2016-07-28/18:46/40</span><br><span class="line">/flume/events/2016-07-28/18:46/50</span><br></pre></td></tr></table></figure>

<ul>
<li><p>5、实现数据的断点续传</p>
<ul>
<li>当一个flume挂掉之后重启的时候还是可以接着上一次的数据继续收集<ul>
<li>flume在1.7版本之前使用的监控一个文件（source exec）、监控一个目录（source spooldir）都无法直接实现</li>
</ul>
</li>
<li>flume在1.7版本之后已经集成了该功能<ul>
<li>其本质就是记录下每一次消费的位置，把消费信息的位置保存到文件中，后续程序挂掉了再重启的时候，可以接着上一次消费的数据位置继续拉取。</li>
</ul>
</li>
<li>配置文件<ul>
<li>vim taildir.conf<ul>
<li>source 类型—-&gt;taildir</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = s1</span><br><span class="line">a1.channels = ch1</span><br><span class="line">a1.sinks = hdfs-sink1</span><br><span class="line"></span><br><span class="line">#channel</span><br><span class="line">a1.channels.ch1.type = memory</span><br><span class="line">a1.channels.ch1.capacity=10000</span><br><span class="line">a1.channels.ch1.transactionCapacity=500</span><br><span class="line"></span><br><span class="line">#source</span><br><span class="line">a1.sources.s1.channels = ch1</span><br><span class="line">#监控一个目录下的多个文件新增的内容</span><br><span class="line">a1.sources.s1.type = taildir</span><br><span class="line">#通过 json 格式存下每个文件消费的偏移量，避免从头消费</span><br><span class="line">a1.sources.s1.positionFile = /install/flumeData/index/taildir_position.json</span><br><span class="line">a1.sources.s1.filegroups = f1 f2 f3 </span><br><span class="line">a1.sources.s1.filegroups.f1 = /home/hadoop/taillogs/access.log</span><br><span class="line">a1.sources.s1.filegroups.f2 = /home/hadoop/taillogs/nginx.log</span><br><span class="line">a1.sources.s1.filegroups.f3 = /home/hadoop/taillogs/web.log</span><br><span class="line">a1.sources.s1.headers.f1.headerKey = access</span><br><span class="line">a1.sources.s1.headers.f2.headerKey = nginx</span><br><span class="line">a1.sources.s1.headers.f3.headerKey = web</span><br><span class="line">a1.sources.s1.fileHeader  = true</span><br><span class="line"></span><br><span class="line">##sink</span><br><span class="line">a1.sinks.hdfs-sink1.channel = ch1</span><br><span class="line">a1.sinks.hdfs-sink1.type = hdfs</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.path =hdfs://node01:8020/demo/data/%&#123;headerKey&#125;</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.filePrefix = event_data</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.fileSuffix = .log</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.rollSize = 1048576</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.rollInterval =20</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.rollCount = 10</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.batchSize = 1500</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.round = true</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.threadsPoolSize = 25</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.fileType =DataStream</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.writeFormat = Text</span><br><span class="line">a1.sinks.hdfs-sink1.hdfs.callTimeout = 60000</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">运行后生成的 taildir_position.json文件信息如下：</span><br><span class="line">[</span><br><span class="line">&#123;&quot;inode&quot;:102626782,&quot;pos&quot;:123,&quot;file&quot;:&quot;/home/hadoop/taillogs/access.log&quot;&#125;,&#123;&quot;inode&quot;:102626785,&quot;pos&quot;:123,&quot;file&quot;:&quot;/home/hadoop/taillogs/web.log&quot;&#125;,&#123;&quot;inode&quot;:102626786,&quot;pos&quot;:123,&quot;file&quot;:&quot;/home/hadoop/taillogs/nginx.log&quot;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">这里inode就是标记文件的，文件名称改变，这个iNode不会变，pos记录偏移量，file就是绝对路径</span><br></pre></td></tr></table></figure>

<ul>
<li>6、flume的header参数配置讲解<ul>
<li>vim test-header.conf    </li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#配置信息test-header.conf</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sources=r1</span><br><span class="line">a1.sinks=k1</span><br><span class="line"></span><br><span class="line">#source</span><br><span class="line">a1.sources.r1.channels=c1</span><br><span class="line">a1.sources.r1.type= spooldir</span><br><span class="line">a1.sources.r1.spoolDir= /home/hadoop/test</span><br><span class="line">a1.sources.r1.batchSize= 100</span><br><span class="line">a1.sources.r1.inputCharset= UTF-8</span><br><span class="line">#是否添加一个key存储目录下文件的绝对路径</span><br><span class="line">a1.sources.r1.fileHeader= true</span><br><span class="line">#指定存储目录下文件的绝对路径的key</span><br><span class="line">a1.sources.r1.fileHeaderKey= mm</span><br><span class="line">#是否添加一个key存储目录下的文件名称</span><br><span class="line">a1.sources.r1.basenameHeader= true</span><br><span class="line">#指定存储目录下文件的名称的key</span><br><span class="line">a1.sources.r1.basenameHeaderKey= nn</span><br><span class="line"></span><br><span class="line">#channel</span><br><span class="line">a1.channels.c1.type= memory</span><br><span class="line">a1.channels.c1.capacity=10000</span><br><span class="line">a1.channels.c1.transactionCapacity=500</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#sink</span><br><span class="line">a1.sinks.k1.type=logger</span><br><span class="line">a1.sinks.k1.channel=c1</span><br></pre></td></tr></table></figure>

<ul>
<li>准备数据文件，添加内容</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/home/hadoop/test/abc.txt</span><br><span class="line">/home/hadoop/test/def.txt</span><br></pre></td></tr></table></figure>

<ul>
<li>启动flume配置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c myconf -f myconf/test-header.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<ul>
<li>查看控制台</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Event: &#123; headers:&#123;mm=/home/hadoop/test/abc.txt, nn=abc.txt&#125; body: 68 65 6C 6C 6F 20 73 70 61 72 6B                hello spark &#125;</span><br><span class="line">19/08/30 19:23:15 INFO sink.LoggerSink: Event: &#123; headers:&#123;mm=/home/hadoop/test/abc.txt, nn=abc.txt&#125; body: 68 65 6C 6C 6F 20 68 61 64 6F 6F 70             hello hadoop &#125;</span><br></pre></td></tr></table></figure>

<h3 id="11-总结"><a href="#11-总结" class="headerlink" title="11. 总结"></a>11. 总结</h3><p><img src="/2020/03/06/Flume日志采集框架/Flume%E6%80%BB%E7%BB%93.png" alt></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/04/HBase与MR与hive集成/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/03/04/HBase与MR与hive集成/" class="post-title-link" itemprop="url">HBase与MR与hive集成</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-03-04 12:13:45" itemprop="dateCreated datePublished" datetime="2020-03-04T12:13:45+08:00">2020-03-04</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-08 14:04:10" itemprop="dateModified" datetime="2020-03-08T14:04:10+08:00">2020-03-08</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/03/HBase底层原理与实战/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/03/03/HBase底层原理与实战/" class="post-title-link" itemprop="url">HBase底层原理与实战</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-03-03 14:47:21" itemprop="dateCreated datePublished" datetime="2020-03-03T14:47:21+08:00">2020-03-03</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-04 11:35:34" itemprop="dateModified" datetime="2020-03-04T11:35:34+08:00">2020-03-04</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="1-HBase的数据存储原理"><a href="#1-HBase的数据存储原理" class="headerlink" title="1. HBase的数据存储原理"></a>1. HBase的数据存储原理</h2><p><img src="/2020/03/03/HBase底层原理与实战/hbase%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84.png" alt></p>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2020/03/03/HBase底层原理与实战/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/02/HBase基本原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/03/02/HBase基本原理/" class="post-title-link" itemprop="url">HBase基本原理</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-03-02 15:38:06" itemprop="dateCreated datePublished" datetime="2020-03-02T15:38:06+08:00">2020-03-02</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-08 14:43:22" itemprop="dateModified" datetime="2020-03-08T14:43:22+08:00">2020-03-08</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="1-HBase是什么"><a href="#1-HBase是什么" class="headerlink" title="1. HBase是什么"></a>1. HBase是什么</h2><ul>
<li><a href="http://developer.51cto.com/art/201904/595698.htm" target="_blank" rel="noopener">漫画学习HBase—-最易懂的Hbase架构原理解析</a></li>
</ul>
<h3 id="1-1-HBase的概念"><a href="#1-1-HBase的概念" class="headerlink" title="1.1 HBase的概念"></a>1.1 HBase的概念</h3><ul>
<li>HBase基于Google的BigTable论文，是建立的<strong>HDFS</strong>之上，提供<strong>高可靠性</strong>、<strong>高性能</strong>、<strong>列存储</strong>、<strong>可伸缩</strong>、<strong>实时读写</strong>的分布式数据库系统。</li>
<li>在需要<strong>实时读写随机访问</strong>超大规模数据集时，可以使用HBase。</li>
</ul>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2020/03/02/HBase基本原理/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/25/hive调优/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/02/25/hive调优/" class="post-title-link" itemprop="url">hive调优</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-02-25 11:50:15" itemprop="dateCreated datePublished" datetime="2020-02-25T11:50:15+08:00">2020-02-25</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-20 21:10:25" itemprop="dateModified" datetime="2020-03-20T21:10:25+08:00">2020-03-20</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="hive的企业级调优"><a href="#hive的企业级调优" class="headerlink" title="hive的企业级调优"></a>hive的企业级调优</h3><h4 id="1、Fetch抓取"><a href="#1、Fetch抓取" class="headerlink" title="1、Fetch抓取"></a>1、Fetch抓取</h4><ul>
<li><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算</p>
<ul>
<li>例如：select * from score;</li>
<li>在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台</li>
</ul>
</li>
<li><p>在hive-default.xml.template文件中 <strong>hive.fetch.task.conversion默认是more</strong>，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p>
</li>
<li><p>案例实操</p>
<ul>
<li>把 hive.fetch.task.conversion设置成<strong>none</strong>，然后执行查询语句，都会执行mapreduce程序</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion=<span class="keyword">none</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> s_id <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> s_id <span class="keyword">from</span> score <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>把hive.fetch.task.conversion设置成<strong>more</strong>，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion=more;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> s_id <span class="keyword">from</span> score;</span><br><span class="line"><span class="keyword">select</span> s_id <span class="keyword">from</span> score <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="2、本地模式"><a href="#2、本地模式" class="headerlink" title="2、本地模式"></a>2、本地模式</h4><ul>
<li><p>在Hive客户端测试时，默认情况下是启用hadoop的job模式,把任务提交到集群中运行，这样会导致计算非常缓慢；</p>
</li>
<li><p>Hive可以通过本地模式在单台机器上处理任务。对于小数据集，执行时间可以明显被缩短。</p>
</li>
<li><p>案例实操</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启本地模式，并执行查询语句</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;  //开启本地mr</span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，</span></span><br><span class="line"><span class="comment">--默认为134217728，即128M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，</span></span><br><span class="line"><span class="comment">--默认为4</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max=<span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--执行查询的sql语句</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student cluster <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--关闭本地运行模式</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">false</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student cluster <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure>

<h4 id="3、表的优化"><a href="#3、表的优化" class="headerlink" title="3、表的优化"></a>3、表的优化</h4><h5 id="1-小表、大表-join"><a href="#1-小表、大表-join" class="headerlink" title="1 小表、大表 join"></a>1 小表、大表 join</h5><ul>
<li><p>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select  count(distinct s_id)  from score;</span><br><span class="line"></span><br><span class="line">select count(s_id) from score group by s_id; 在map端进行聚合，效率更高</span><br><span class="line"></span><br><span class="line">select  count(distinct s_id)  from score;</span><br><span class="line"></span><br><span class="line">select count(1) from (</span><br><span class="line">select count(1) from score group  by  s_id</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li><p>实际测试发现：新版的hive已经对小表 join 大表和大表 join 小表进行了优化。小表放在左边和右边已经没有明显区别。</p>
</li>
<li><p>表数据存储在HDFS的datanode里面，使用小表join大表是将小表加载到分布式缓存中，和大表进行join</p>
</li>
<li><p>多个表关联时，最好分拆成小段，避免大sql（无法控制中间Job）</p>
</li>
</ul>
<h5 id="2-大表-join-大表"><a href="#2-大表-join-大表" class="headerlink" title="2 大表 join 大表"></a>2 大表 join 大表</h5><p>join的原理：相同的key去往同一个reduce task，某一个key数据比较多时，会发生数据倾斜，某一个reduce task的计算量太大，整体拖慢job的进度。</p>
<ul>
<li><p>1．空 key 过滤</p>
<ul>
<li><p>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。</p>
</li>
<li><p>此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。</p>
</li>
<li><p>测试环境准备：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">use myhive;</span><br><span class="line">create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">create table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/kkb/install/hivedatas/hive_big_table/*&apos; into table ori; </span><br><span class="line">load data local inpath &apos;/kkb/install/hivedatas/hive_have_null_id/*&apos; into table nullidtable;</span><br></pre></td></tr></table></figure>

<p>过滤空key与不过滤空key的结果比较</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">不过滤：</span><br><span class="line">INSERT OVERWRITE TABLE jointable</span><br><span class="line">SELECT a.* FROM nullidtable a JOIN ori b ON a.id = b.id;</span><br><span class="line">结果：</span><br><span class="line">No rows affected (152.135 seconds)</span><br><span class="line"></span><br><span class="line">过滤：</span><br><span class="line">INSERT OVERWRITE TABLE jointable</span><br><span class="line">SELECT a.* FROM (SELECT * FROM nullidtable WHERE id IS NOT NULL ) a JOIN ori b ON a.id = b.id;</span><br><span class="line">结果：</span><br><span class="line">No rows affected (141.585 seconds)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>2、空 key 转换</p>
<ul>
<li><p>有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在 join 的结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。</p>
<p>不随机分布：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=<span class="number">32123456</span>;</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">7</span>;</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> jointable</span><br><span class="line"><span class="keyword">SELECT</span> a.*</span><br><span class="line"><span class="keyword">FROM</span> nullidtable a</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> ori b <span class="keyword">ON</span> <span class="keyword">CASE</span> <span class="keyword">WHEN</span> a.id <span class="keyword">IS</span> <span class="literal">NULL</span> <span class="keyword">THEN</span> <span class="string">'hive'</span> <span class="keyword">ELSE</span> a.id <span class="keyword">END</span> = b.id;</span><br><span class="line">No rows affected (41.668 seconds)</span><br></pre></td></tr></table></figure>

<p><strong>结果：这样的后果就是所有为null值的id全部都变成了相同的字符串，及其容易造成数据的倾斜（所有的key相同，相同key的数据会到同一个reduce当中去）</strong></p>
<p><strong>为了解决这种情况，我们可以通过hive的rand函数，随记的给每一个为空的id赋上一个随机值，这样就不会造成数据倾斜</strong>        </p>
</li>
</ul>
<p>​        随机分布：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=<span class="number">32123456</span>;</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">7</span>;</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> jointable</span><br><span class="line"><span class="keyword">SELECT</span> a.*</span><br><span class="line"><span class="keyword">FROM</span> nullidtable a</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> ori b <span class="keyword">ON</span> <span class="keyword">CASE</span> <span class="keyword">WHEN</span> a.id <span class="keyword">IS</span> <span class="literal">NULL</span> <span class="keyword">THEN</span> <span class="keyword">concat</span>(<span class="string">'hive'</span>, <span class="keyword">rand</span>()) <span class="keyword">ELSE</span> a.id <span class="keyword">END</span> = b.id;</span><br><span class="line"></span><br><span class="line">No rows affected (42.594 seconds)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="3、大表join小表与小表join大表实测"><a href="#3、大表join小表与小表join大表实测" class="headerlink" title="3、大表join小表与小表join大表实测"></a>3、大表join小表与小表join大表实测</h5><p>需求：测试大表JOIN小表和小表JOIN大表的效率 （新的版本当中已经没有区别了，旧的版本当中需要使用小表）</p>
<p>（1）建大表、小表和JOIN后表的语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable2(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>

<p>（2）分别向大表和小表中导入数据</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/kkb/install/hivedatas/big_data' into table bigtable;</span><br><span class="line"></span><br><span class="line">hive (default)&gt;load data local inpath '/kkb/install/hivedatas/small_data' into table smalltable;</span><br></pre></td></tr></table></figure>

<h5 id="3-map-join"><a href="#3-map-join" class="headerlink" title="3 map  join"></a>3 map  join</h5><ul>
<li><p>如果不指定MapJoin 或者不符合 MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存，在map端进行join，避免reducer处理。</p>
</li>
<li><p>1、开启MapJoin参数设置</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">--默认为true</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>2、大表小表的阈值设置（默认25M一下认为是小表）</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">26214400</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>3、MapJoin工作机制</li>
</ul>
<p><img src="/2020/02/25/hive调优/xxx-1570506631515.jpg" alt></p>
<p>首先是Task A，它是一个Local Task（在客户端本地执行的Task），负责扫描小表b的数据，将其转换成一个HashTable的数据结构，并写入本地的文件中，之后将该文件加载到DistributeCache中。</p>
<p>接下来是Task B，该任务是一个没有Reduce的MR，启动MapTasks扫描大表a,在Map阶段，根据a的每一条记录去和DistributeCache中b表对应的HashTable关联，并直接输出结果。</p>
<p>由于MapJoin没有Reduce，所以由Map直接输出结果文件，有多少个Map Task，就有多少个结果文件。</p>
<p><strong>案例实操：</strong></p>
<p>（1）开启Mapjoin功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.auto.convert.join = true; 默认为true</span><br></pre></td></tr></table></figure>

<p>（2）执行小表JOIN大表语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE jointable2</span><br><span class="line">SELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line">FROM smalltable s</span><br><span class="line">JOIN bigtable  b</span><br><span class="line">ON s.id = b.id;</span><br><span class="line"></span><br><span class="line">Time taken: 31.814 seconds</span><br></pre></td></tr></table></figure>

<p>（3）执行大表JOIN小表语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE jointable2</span><br><span class="line">SELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line">FROM bigtable  b</span><br><span class="line">JOIN smalltable  s</span><br><span class="line">ON s.id = b.id;</span><br><span class="line"></span><br><span class="line">Time taken: 28.46 seconds</span><br></pre></td></tr></table></figure>

<h5 id="4-group-By"><a href="#4-group-By" class="headerlink" title="4 group By"></a>4 group By</h5><ul>
<li><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。</p>
</li>
<li><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p>
</li>
<li><p>开启Map端聚合参数设置</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--是否在Map端进行聚合，默认为True</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--在Map端进行聚合操作的条目数目</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval = <span class="number">100000</span>;</span><br><span class="line"><span class="comment">--有数据倾斜的时候进行负载均衡（默认是false）</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="5-count-distinct"><a href="#5-count-distinct" class="headerlink" title="5 count(distinct)"></a>5 count(distinct)</h5><ul>
<li><p>数据量小的时候无所谓，数据量大的情况下，由于count distinct 操作需要用一个reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般count distinct使用先group by 再count的方式替换</p>
<p>环境准备：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/kkb/install/hivedatas/data/100万条大表数据（id除以10取整）/bigtable'</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--每个reduce任务处理的数据量 默认256000000（256M）</span></span><br><span class="line"> <span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=<span class="number">32123456</span>;</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">select</span>  <span class="keyword">count</span>(<span class="keyword">distinct</span> ip )  <span class="keyword">from</span> log_text;</span><br><span class="line"> </span><br><span class="line"> 转换成</span><br><span class="line"> <span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=<span class="number">32123456</span>;</span><br><span class="line"> <span class="keyword">select</span> <span class="keyword">count</span>(ip) <span class="keyword">from</span> (<span class="keyword">select</span> ip <span class="keyword">from</span> log_text <span class="keyword">group</span> <span class="keyword">by</span> ip) t;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> 虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="6-笛卡尔积"><a href="#6-笛卡尔积" class="headerlink" title="6 笛卡尔积"></a>6 笛卡尔积</h5><ul>
<li>尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件</li>
<li>Hive只能使用1个reducer来完成笛卡尔积。</li>
</ul>
<h4 id="4、使用分区剪裁、列剪裁"><a href="#4、使用分区剪裁、列剪裁" class="headerlink" title="4、使用分区剪裁、列剪裁"></a>4、使用分区剪裁、列剪裁</h4><ul>
<li>尽可能早地过滤掉尽可能多的数据量，避免大量数据流入外层SQL。</li>
<li><strong>列剪裁</strong><ul>
<li>只获取需要的列的数据，减少数据输入。</li>
</ul>
</li>
<li><strong>分区裁剪</strong><ul>
<li>分区在hive实质上是目录，分区裁剪可以方便直接地过滤掉大部分数据。</li>
<li>尽量使用<strong>分区过滤</strong>，少用select  *</li>
</ul>
</li>
</ul>
<p>​    环境准备：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">create table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/home/admin/softwares/data/加递增id的原始数据/ori&apos; into table ori;</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/home/admin/softwares/data/100万条大表数据（id除以10取整）/bigtable&apos; into table bigtable;</span><br></pre></td></tr></table></figure>

<p>先关联再Where：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.id</span><br><span class="line">FROM bigtable a</span><br><span class="line">LEFT JOIN ori b ON a.id = b.id</span><br><span class="line">WHERE b.id &lt;= 10;</span><br></pre></td></tr></table></figure>

<p>正确的写法是写在ON后面：先Where再关联</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.id</span><br><span class="line">FROM ori a</span><br><span class="line">LEFT JOIN bigtable b ON (a.id &lt;= 10 AND a.id = b.id);</span><br></pre></td></tr></table></figure>

<p>或者直接写成子查询：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.id</span><br><span class="line">FROM bigtable a</span><br><span class="line">RIGHT JOIN (SELECT id</span><br><span class="line">FROM ori</span><br><span class="line">WHERE id &lt;= 10</span><br><span class="line">) b ON a.id = b.id;</span><br></pre></td></tr></table></figure>

<h4 id="5、并行执行"><a href="#5、并行执行" class="headerlink" title="5、并行执行"></a>5、并行执行</h4><ul>
<li>把一个sql语句中没有相互依赖的阶段并行去运行。提高集群资源利用率</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启并行执行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--同一个sql允许最大并行度，默认为8。</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">16</span>;</span><br></pre></td></tr></table></figure>

<h4 id="6、严格模式"><a href="#6、严格模式" class="headerlink" title="6、严格模式"></a>6、严格模式</h4><ul>
<li><p>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。</p>
</li>
<li><p>通过设置属性hive.mapred.mode值为默认是非严格模式<strong>nonstrict</strong> 。开启严格模式需要修改hive.mapred.mode值为<strong>strict</strong>，开启严格模式可以禁止3种类型的查询。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--设置非严格模式（默认）</span></span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode=nonstrict;</span><br><span class="line"></span><br><span class="line"><span class="comment">--设置严格模式</span></span><br><span class="line"><span class="keyword">set</span> hive.mapred.mode=<span class="keyword">strict</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>（1）对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--设置严格模式下 执行sql语句报错； 非严格模式下是可以的</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> order_partition；</span><br><span class="line"></span><br><span class="line">异常信息：<span class="keyword">Error</span>: <span class="keyword">Error</span> <span class="keyword">while</span> compiling <span class="keyword">statement</span>: <span class="keyword">FAILED</span>: SemanticException [<span class="keyword">Error</span> <span class="number">10041</span>]: <span class="keyword">No</span> <span class="keyword">partition</span> predicate <span class="keyword">found</span> <span class="keyword">for</span> <span class="keyword">Alias</span> <span class="string">"order_partition"</span> <span class="keyword">Table</span> <span class="string">"order_partition"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>（2）对于使用了order by语句的查询，要求必须使用limit语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--设置严格模式下 执行sql语句报错； 非严格模式下是可以的</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> order_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'2019-03'</span> <span class="keyword">order</span> <span class="keyword">by</span> order_price; </span><br><span class="line"></span><br><span class="line">异常信息：Error: Error while compiling statement: FAILED: SemanticException 1:61 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'order_price'</span><br></pre></td></tr></table></figure>
</li>
<li><p>（3）限制笛卡尔积的查询</p>
<ul>
<li>严格模式下，避免出现笛卡尔积的查询</li>
</ul>
</li>
</ul>
<h4 id="7、JVM重用"><a href="#7、JVM重用" class="headerlink" title="7、JVM重用"></a>7、JVM重用</h4><ul>
<li><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p>
<p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;How many tasks to run per jvm. If set to -1, there is</span><br><span class="line">  no limit. </span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>我们也可以在hive当中通过</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set  mapred.job.reuse.jvm.num.tasks=10;</span><br></pre></td></tr></table></figure>

<p>这个设置来设置我们的jvm重用</p>
<p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p>
</li>
</ul>
<h4 id="8、推测执行"><a href="#8、推测执行" class="headerlink" title="8、推测执行"></a>8、推测执行</h4><ul>
<li><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p>
<p>设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置</p>
</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.speculative&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some map tasks </span><br><span class="line">               may be executed in parallel.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some reduce tasks </span><br><span class="line">               may be executed in parallel.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>不过hive本身也提供了配置项来控制reduce-side的推测执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p>
<h4 id="9、压缩"><a href="#9、压缩" class="headerlink" title="9、压缩"></a>9、压缩</h4><p>​    参见数据的压缩</p>
<ul>
<li><p>Hive表中间数据压缩</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>设置为true为激活中间数据压缩功能，默认是false，没有开启</span><br><span class="line">set hive.exec.compress.intermediate=true;</span><br><span class="line"><span class="meta">#</span>设置中间数据的压缩算法</span><br><span class="line">set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Hive表最终输出结果压缩</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.compress.output=true;</span><br><span class="line">set mapred.output.compression.codec= </span><br><span class="line">org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="10、使用EXPLAIN（执行计划）"><a href="#10、使用EXPLAIN（执行计划）" class="headerlink" title="10、使用EXPLAIN（执行计划）"></a>10、使用EXPLAIN（执行计划）</h4><p>查看hql执行计划</p>
<h4 id="11、数据倾斜"><a href="#11、数据倾斜" class="headerlink" title="11、数据倾斜"></a>11、数据倾斜</h4><h5 id="1-合理设置Map数"><a href="#1-合理设置Map数" class="headerlink" title="1 合理设置Map数"></a>1 合理设置Map数</h5><ul>
<li><p>1)  通常情况下，作业会通过input的目录产生一个或者多个map任务。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</span><br><span class="line"></span><br><span class="line">举例：</span><br><span class="line">a)  假设input目录下有1个文件a，大小为780M，那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数。</span><br><span class="line">b) 假设input目录下有3个文件a，b，c大小分别为10m，20m，150m，那么hadoop会分隔成4个块（10m，20m，128m，22m），从而产生4个map数。即，如果文件大于块大小(128m)，那么会拆分，如果小于块大小，则把该文件当成一个块。</span><br></pre></td></tr></table></figure>
</li>
<li><p>2） 是不是map数越多越好？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</span><br></pre></td></tr></table></figure>
</li>
<li><p>3） 是不是保证每个map处理接近128m的文件块，就高枕无忧了？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</span><br><span class="line"></span><br><span class="line">针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="2-小文件合并"><a href="#2-小文件合并" class="headerlink" title="2 小文件合并"></a>2 小文件合并</h5><ul>
<li><p>在map执行前合并小文件，减少map数：</p>
</li>
<li><p>CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">112345600</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node=<span class="number">112345600</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack=<span class="number">112345600</span>;</span><br><span class="line"><span class="keyword">set</span> hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>

<p>这个参数表示执行前进行小文件合并，前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m，大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并。</p>
</li>
</ul>
<h5 id="3-复杂文件增加Map数"><a href="#3-复杂文件增加Map数" class="headerlink" title="3 复杂文件增加Map数"></a>3 复杂文件增加Map数</h5><ul>
<li><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p>
</li>
<li><p>增加map的方法为</p>
<ul>
<li>根据 ==computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))==公式</li>
<li>==调整maxSize最大值==。让maxSize最大值低于blocksize就可以增加map的个数。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mapreduce.input.fileinputformat.split.minsize=1 默认值为1</span><br><span class="line"></span><br><span class="line">mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值Long.MAXValue因此，默认情况下，切片大小=blocksize </span><br><span class="line"></span><br><span class="line">maxsize（切片最大值): 参数如果调到比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。</span><br><span class="line"></span><br><span class="line">minsize(切片最小值): 参数调的比blockSize大，则可以让切片变得比blocksize还大。</span><br></pre></td></tr></table></figure>

<ul>
<li>例如</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--设置maxsize大小为10M，也就是说一个fileSplit的大小为10M</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize=<span class="number">10485760</span>;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>牺牲了计算的本地性</li>
</ul>
<h5 id="4-合理设置Reduce数"><a href="#4-合理设置Reduce数" class="headerlink" title="4 合理设置Reduce数"></a>4 合理设置Reduce数</h5><ul>
<li><p>1、调整reduce个数方法一</p>
<ul>
<li><p>1）每个Reduce处理的数据量默认是256MB</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=<span class="number">256000000</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>2) 每个任务最大的reduce数，默认为1009</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max=<span class="number">1009</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>3) 计算reducer数的公式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N=min(参数2，总输入数据量/参数1)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>2、调整reduce个数方法二</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--设置每一个job中reduce个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>3、reduce个数并不是越多越好</p>
<ul>
<li>过多的启动和初始化reduce也会消耗时间和资源；</li>
<li>同时过多的reduce会生成很多个文件，也有可能出现小文件问题</li>
</ul>
</li>
</ul>
<p><img src="/2020/02/25/hive调优/xxx-1570506631515.jpg" alt></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/18/Spark内存计算框架原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/02/18/Spark内存计算框架原理/" class="post-title-link" itemprop="url">Spark内存计算框架原理</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-02-18 15:17:03" itemprop="dateCreated datePublished" datetime="2020-02-18T15:17:03+08:00">2020-02-18</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-20 13:05:06" itemprop="dateModified" datetime="2020-02-20T13:05:06+08:00">2020-02-20</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="spark内存计算框架"><a href="#spark内存计算框架" class="headerlink" title="spark内存计算框架"></a>spark内存计算框架</h1><h3 id="1-RDD的依赖关系"><a href="#1-RDD的依赖关系" class="headerlink" title="1. RDD的依赖关系"></a>1. RDD的依赖关系</h3><p><img src="/2020/02/18/Spark内存计算框架原理/rdd-dependencies.png" alt></p>
<ul>
<li>RDD和它依赖的父RDD的关系有两种不同的类型</li>
</ul>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2020/02/18/Spark内存计算框架原理/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/17/Hive实战/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/02/17/Hive实战/" class="post-title-link" itemprop="url">Hive实战</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-02-17 15:15:11 / 修改时间：19:42:48" itemprop="dateCreated datePublished" datetime="2020-02-17T15:15:11+08:00">2020-02-17</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="大数据分析利器之Hive"><a href="#大数据分析利器之Hive" class="headerlink" title="大数据分析利器之Hive"></a>大数据分析利器之Hive</h1><h3 id="1-Hive的分桶表"><a href="#1-Hive的分桶表" class="headerlink" title="1. Hive的分桶表"></a>1. Hive的分桶表</h3>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2020/02/17/Hive实战/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/17/Hive原理与优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/02/17/Hive原理与优化/" class="post-title-link" itemprop="url">Hive原理与优化</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-02-17 11:52:09 / 修改时间：15:12:24" itemprop="dateCreated datePublished" datetime="2020-02-17T11:52:09+08:00">2020-02-17</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="0-准备"><a href="#0-准备" class="headerlink" title="0.准备"></a>0.准备</h3><p>安装好对应版本的hadoop集群，并启动hadoop的HDFS以及YARN服务</p>
<h3 id="1-数据仓库"><a href="#1-数据仓库" class="headerlink" title="1. 数据仓库"></a>1. 数据仓库</h3><h4 id="1-1-数据仓库的基本概念"><a href="#1-1-数据仓库的基本概念" class="headerlink" title="1.1 数据仓库的基本概念"></a>1.1 数据仓库的基本概念</h4><ul>
<li>数据仓库的英文名称为Data Warehouse，可简写为DW或DWH。</li>
</ul>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2020/02/17/Hive原理与优化/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/16/Spark底层核心之RDD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/02/16/Spark底层核心之RDD/" class="post-title-link" itemprop="url">Spark底层核心之RDD</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-02-16 19:00:43" itemprop="dateCreated datePublished" datetime="2020-02-16T19:00:43+08:00">2020-02-16</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-18 15:15:18" itemprop="dateModified" datetime="2020-02-18T15:15:18+08:00">2020-02-18</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="spark内存计算框架"><a href="#spark内存计算框架" class="headerlink" title="spark内存计算框架"></a>spark内存计算框架</h1><h3 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1. RDD是什么"></a>1. RDD是什么</h3><ul>
<li>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合.<ul>
<li><strong>Dataset</strong>:          就是一个集合，存储很多数据.</li>
<li><strong>Distributed</strong>：它内部的元素进行了分布式存储，方便于后期进行分布式计算.</li>
<li><strong>Resilient</strong>：     表示弹性，rdd的数据是可以保存在内存或者是磁盘中.</li>
</ul>
</li>
</ul>
          <!--noindex-->
          
            <div class="post-button">
              <a class="btn" href="/2020/02/16/Spark底层核心之RDD/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/12/kafka内部原理和机制二/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小鱼儿">
      <meta itemprop="description" content="肩膀有点痒，可能在长小翅膀">
      <meta itemprop="image" content="/images/fish.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一只鱼的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/02/12/kafka内部原理和机制二/" class="post-title-link" itemprop="url">kafka内部原理和机制二</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-02-12 19:07:24" itemprop="dateCreated datePublished" datetime="2020-02-12T19:07:24+08:00">2020-02-12</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-16 16:59:54" itemprop="dateModified" datetime="2020-02-16T16:59:54+08:00">2020-02-16</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据开发/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/fish.png"
      alt="小鱼儿">
  <p class="site-author-name" itemprop="name">小鱼儿</p>
  <div class="site-description" itemprop="description">肩膀有点痒，可能在长小翅膀</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">67</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    
  
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-Hans" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小鱼儿</span>
</div>

        












        
      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  





















  

  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>

